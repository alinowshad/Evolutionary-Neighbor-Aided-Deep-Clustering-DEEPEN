{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e068de48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:21:04.697196Z",
     "iopub.status.busy": "2024-09-26T19:21:04.696926Z",
     "iopub.status.idle": "2024-09-26T19:21:17.426431Z",
     "shell.execute_reply": "2024-09-26T19:21:17.425051Z"
    },
    "papermill": {
     "duration": 12.750668,
     "end_time": "2024-09-26T19:21:17.429066",
     "exception": false,
     "start_time": "2024-09-26T19:21:04.678398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tslearn\r\n",
      "  Downloading tslearn-0.6.3-py3-none-any.whl (374 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tslearn) (1.23.5)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from tslearn) (1.11.2)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from tslearn) (1.2.2)\r\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from tslearn) (0.57.1)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from tslearn) (1.3.2)\r\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->tslearn) (0.40.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->tslearn) (3.1.0)\r\n",
      "Installing collected packages: tslearn\r\n",
      "Successfully installed tslearn-0.6.3\r\n"
     ]
    }
   ],
   "source": [
    "! pip install tslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f47560d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:21:17.466369Z",
     "iopub.status.busy": "2024-09-26T19:21:17.466046Z",
     "iopub.status.idle": "2024-09-26T19:21:29.115904Z",
     "shell.execute_reply": "2024-09-26T19:21:29.114887Z"
    },
    "papermill": {
     "duration": 11.671081,
     "end_time": "2024-09-26T19:21:29.118312",
     "exception": false,
     "start_time": "2024-09-26T19:21:17.447231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn-extra\r\n",
      "  Downloading scikit_learn_extra-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn-extra) (1.23.5)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn-extra) (1.11.2)\r\n",
      "Requirement already satisfied: scikit-learn>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn-extra) (1.2.2)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.1.0)\r\n",
      "Installing collected packages: scikit-learn-extra\r\n",
      "Successfully installed scikit-learn-extra-0.3.0\r\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-learn-extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c436dae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:21:29.157795Z",
     "iopub.status.busy": "2024-09-26T19:21:29.157472Z",
     "iopub.status.idle": "2024-09-26T19:21:40.426602Z",
     "shell.execute_reply": "2024-09-26T19:21:40.425466Z"
    },
    "papermill": {
     "duration": 11.291202,
     "end_time": "2024-09-26T19:21:40.428999",
     "exception": false,
     "start_time": "2024-09-26T19:21:29.137797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rstl\r\n",
      "  Downloading rstl-0.1.3-py3-none-any.whl (5.2 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rstl) (1.23.5)\r\n",
      "Installing collected packages: rstl\r\n",
      "Successfully installed rstl-0.1.3\r\n"
     ]
    }
   ],
   "source": [
    "! pip install rstl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76790150",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:21:40.467537Z",
     "iopub.status.busy": "2024-09-26T19:21:40.467205Z",
     "iopub.status.idle": "2024-09-26T19:21:52.326198Z",
     "shell.execute_reply": "2024-09-26T19:21:52.325143Z"
    },
    "papermill": {
     "duration": 11.881238,
     "end_time": "2024-09-26T19:21:52.328921",
     "exception": false,
     "start_time": "2024-09-26T19:21:40.447683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pmdarima\r\n",
      "  Downloading pmdarima-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.10/site-packages (from pmdarima) (1.3.2)\r\n",
      "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /opt/conda/lib/python3.10/site-packages (from pmdarima) (0.29.35)\r\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from pmdarima) (1.23.5)\r\n",
      "Requirement already satisfied: pandas>=0.19 in /opt/conda/lib/python3.10/site-packages (from pmdarima) (2.0.2)\r\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /opt/conda/lib/python3.10/site-packages (from pmdarima) (1.2.2)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from pmdarima) (1.11.2)\r\n",
      "Requirement already satisfied: statsmodels>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from pmdarima) (0.14.0)\r\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from pmdarima) (1.26.15)\r\n",
      "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /opt/conda/lib/python3.10/site-packages (from pmdarima) (68.0.0)\r\n",
      "Requirement already satisfied: packaging>=17.1 in /opt/conda/lib/python3.10/site-packages (from pmdarima) (21.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.1->pmdarima) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.19->pmdarima) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.19->pmdarima) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.19->pmdarima) (2023.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22->pmdarima) (3.1.0)\r\n",
      "Requirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.10/site-packages (from statsmodels>=0.13.2->pmdarima) (0.5.3)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.2->statsmodels>=0.13.2->pmdarima) (1.16.0)\r\n",
      "Installing collected packages: pmdarima\r\n",
      "Successfully installed pmdarima-2.0.4\r\n"
     ]
    }
   ],
   "source": [
    "! pip install pmdarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c666a45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:21:52.370393Z",
     "iopub.status.busy": "2024-09-26T19:21:52.369791Z",
     "iopub.status.idle": "2024-09-26T19:22:04.085093Z",
     "shell.execute_reply": "2024-09-26T19:22:04.084120Z"
    },
    "papermill": {
     "duration": 11.738568,
     "end_time": "2024-09-26T19:22:04.087495",
     "exception": false,
     "start_time": "2024-09-26T19:21:52.348927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-tcn\r\n",
      "  Downloading keras_tcn-3.5.0-py3-none-any.whl (13 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras-tcn) (1.23.5)\r\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (from keras-tcn) (2.12.0)\r\n",
      "Requirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.10/site-packages (from keras-tcn) (0.21.0)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (1.4.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (23.5.26)\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (0.4.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (0.2.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (1.51.1)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (3.9.0)\r\n",
      "Requirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (0.4.13)\r\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (2.12.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (16.0.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (3.3.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (21.3)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (3.20.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (68.0.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (1.16.0)\r\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (2.12.3)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (2.12.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (2.3.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (4.6.3)\r\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (1.14.1)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (0.32.0)\r\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /opt/conda/lib/python3.10/site-packages (from tensorflow-addons->keras-tcn) (2.13.3)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow->keras-tcn) (0.40.0)\r\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow->keras-tcn) (0.2.0)\r\n",
      "Requirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow->keras-tcn) (1.11.2)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (2.20.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (1.0.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (3.4.3)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (2.31.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (0.7.1)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (2.3.7)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow->keras-tcn) (3.0.9)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (4.2.4)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (0.2.7)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (4.9)\r\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (1.26.15)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (1.3.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (2023.7.22)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (2.1.3)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (3.2.2)\r\n",
      "Installing collected packages: keras-tcn\r\n",
      "Successfully installed keras-tcn-3.5.0\r\n"
     ]
    }
   ],
   "source": [
    "! pip install keras-tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49789aa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:04.130284Z",
     "iopub.status.busy": "2024-09-26T19:22:04.129964Z",
     "iopub.status.idle": "2024-09-26T19:22:15.575934Z",
     "shell.execute_reply": "2024-09-26T19:22:15.574718Z"
    },
    "papermill": {
     "duration": 11.469706,
     "end_time": "2024-09-26T19:22:15.578254",
     "exception": false,
     "start_time": "2024-09-26T19:22:04.108548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in /opt/conda/lib/python3.10/site-packages (0.14.0)\r\n",
      "Requirement already satisfied: numpy>=1.18 in /opt/conda/lib/python3.10/site-packages (from statsmodels) (1.23.5)\r\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.4 in /opt/conda/lib/python3.10/site-packages (from statsmodels) (1.11.2)\r\n",
      "Requirement already satisfied: pandas>=1.0 in /opt/conda/lib/python3.10/site-packages (from statsmodels) (2.0.2)\r\n",
      "Requirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.10/site-packages (from statsmodels) (0.5.3)\r\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from statsmodels) (21.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21.3->statsmodels) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0->statsmodels) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0->statsmodels) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0->statsmodels) (2023.3)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.2->statsmodels) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb14ffa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:15.621309Z",
     "iopub.status.busy": "2024-09-26T19:22:15.620751Z",
     "iopub.status.idle": "2024-09-26T19:22:29.110998Z",
     "shell.execute_reply": "2024-09-26T19:22:29.110032Z"
    },
    "papermill": {
     "duration": 13.514229,
     "end_time": "2024-09-26T19:22:29.113437",
     "exception": false,
     "start_time": "2024-09-26T19:22:15.599208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting statsforecast\r\n",
      "  Downloading statsforecast-1.7.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (314 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from statsforecast) (2.2.1)\r\n",
      "Collecting coreforecast>=0.0.12 (from statsforecast)\r\n",
      "  Downloading coreforecast-0.0.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (196 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.7/196.7 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numba>=0.55.0 in /opt/conda/lib/python3.10/site-packages (from statsforecast) (0.57.1)\r\n",
      "Requirement already satisfied: numpy>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from statsforecast) (1.23.5)\r\n",
      "Requirement already satisfied: pandas>=1.3.5 in /opt/conda/lib/python3.10/site-packages (from statsforecast) (2.0.2)\r\n",
      "Requirement already satisfied: scipy>=1.7.3 in /opt/conda/lib/python3.10/site-packages (from statsforecast) (1.11.2)\r\n",
      "Requirement already satisfied: statsmodels>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from statsforecast) (0.14.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from statsforecast) (4.66.1)\r\n",
      "Collecting fugue>=0.8.1 (from statsforecast)\r\n",
      "  Downloading fugue-0.9.1-py3-none-any.whl (278 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting utilsforecast>=0.1.4 (from statsforecast)\r\n",
      "  Downloading utilsforecast-0.2.5-py3-none-any.whl (41 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=3 in /opt/conda/lib/python3.10/site-packages (from statsforecast) (3.1.0)\r\n",
      "Collecting triad>=0.9.7 (from fugue>=0.8.1->statsforecast)\r\n",
      "  Downloading triad-0.9.8-py3-none-any.whl (62 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting adagio>=0.2.4 (from fugue>=0.8.1->statsforecast)\r\n",
      "  Downloading adagio-0.2.6-py3-none-any.whl (19 kB)\r\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.55.0->statsforecast) (0.40.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3.5->statsforecast) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3.5->statsforecast) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3.5->statsforecast) (2023.3)\r\n",
      "Requirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.10/site-packages (from statsmodels>=0.13.2->statsforecast) (0.5.3)\r\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from statsmodels>=0.13.2->statsforecast) (21.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21.3->statsmodels>=0.13.2->statsforecast) (3.0.9)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.2->statsmodels>=0.13.2->statsforecast) (1.16.0)\r\n",
      "Requirement already satisfied: pyarrow>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from triad>=0.9.7->fugue>=0.8.1->statsforecast) (11.0.0)\r\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from triad>=0.9.7->fugue>=0.8.1->statsforecast) (2023.9.0)\r\n",
      "Collecting fs (from triad>=0.9.7->fugue>=0.8.1->statsforecast)\r\n",
      "  Downloading fs-2.4.16-py2.py3-none-any.whl (135 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: appdirs~=1.4.3 in /opt/conda/lib/python3.10/site-packages (from fs->triad>=0.9.7->fugue>=0.8.1->statsforecast) (1.4.4)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from fs->triad>=0.9.7->fugue>=0.8.1->statsforecast) (68.0.0)\r\n",
      "Installing collected packages: fs, coreforecast, utilsforecast, triad, adagio, fugue, statsforecast\r\n",
      "Successfully installed adagio-0.2.6 coreforecast-0.0.12 fs-2.4.16 fugue-0.9.1 statsforecast-1.7.8 triad-0.9.8 utilsforecast-0.2.5\r\n"
     ]
    }
   ],
   "source": [
    "! pip install statsforecast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80ae8854",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:29.160077Z",
     "iopub.status.busy": "2024-09-26T19:22:29.159766Z",
     "iopub.status.idle": "2024-09-26T19:22:31.422299Z",
     "shell.execute_reply": "2024-09-26T19:22:31.421334Z"
    },
    "papermill": {
     "duration": 2.288444,
     "end_time": "2024-09-26T19:22:31.424642",
     "exception": false,
     "start_time": "2024-09-26T19:22:29.136198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93392e08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:31.470982Z",
     "iopub.status.busy": "2024-09-26T19:22:31.470473Z",
     "iopub.status.idle": "2024-09-26T19:22:32.318881Z",
     "shell.execute_reply": "2024-09-26T19:22:32.317873Z"
    },
    "papermill": {
     "duration": 0.873899,
     "end_time": "2024-09-26T19:22:32.321217",
     "exception": false,
     "start_time": "2024-09-26T19:22:31.447318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pmdarima as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7e68bc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:32.367036Z",
     "iopub.status.busy": "2024-09-26T19:22:32.366595Z",
     "iopub.status.idle": "2024-09-26T19:22:32.512131Z",
     "shell.execute_reply": "2024-09-26T19:22:32.511242Z"
    },
    "papermill": {
     "duration": 0.170639,
     "end_time": "2024-09-26T19:22:32.514160",
     "exception": false,
     "start_time": "2024-09-26T19:22:32.343521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af40316a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:32.601068Z",
     "iopub.status.busy": "2024-09-26T19:22:32.600442Z",
     "iopub.status.idle": "2024-09-26T19:22:44.487438Z",
     "shell.execute_reply": "2024-09-26T19:22:44.486630Z"
    },
    "papermill": {
     "duration": 11.9536,
     "end_time": "2024-09-26T19:22:44.489821",
     "exception": false,
     "start_time": "2024-09-26T19:22:32.536221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import os\n",
    "import csv\n",
    "import argparse\n",
    "from time import time\n",
    "# Keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Reshape, UpSampling2D, Conv2DTranspose, GlobalAveragePooling1D, Softmax\n",
    "from keras.losses import kullback_leibler_divergence\n",
    "import keras.backend as K\n",
    "# scikit-learn\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "# Dataset helper function\n",
    "# DTC components\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, LeakyReLU, MaxPool1D, LSTM, Bidirectional, TimeDistributed, Dense, Reshape\n",
    "from keras.layers import UpSampling2D, Conv2DTranspose\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from statsmodels.tsa import stattools\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, Attention\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e7064aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:44.536949Z",
     "iopub.status.busy": "2024-09-26T19:22:44.536121Z",
     "iopub.status.idle": "2024-09-26T19:22:44.670673Z",
     "shell.execute_reply": "2024-09-26T19:22:44.669793Z"
    },
    "papermill": {
     "duration": 0.159916,
     "end_time": "2024-09-26T19:22:44.672849",
     "exception": false,
     "start_time": "2024-09-26T19:22:44.512933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "import tensorflow_addons as tfa\n",
    "from math import pi, ceil\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from keras.losses import CategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a077d139",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:44.719316Z",
     "iopub.status.busy": "2024-09-26T19:22:44.718688Z",
     "iopub.status.idle": "2024-09-26T19:22:44.736994Z",
     "shell.execute_reply": "2024-09-26T19:22:44.736103Z"
    },
    "papermill": {
     "duration": 0.043448,
     "end_time": "2024-09-26T19:22:44.738825",
     "exception": false,
     "start_time": "2024-09-26T19:22:44.695377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import SpectralClustering\n",
    "# %tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import layers\n",
    "from keras.models import Sequential,Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import time\n",
    "print(tf.__version__)\n",
    "from keras.layers import MultiHeadAttention\n",
    "from keras.layers import Dense\n",
    "import gc\n",
    "from keras.layers import concatenate\n",
    "import csv\n",
    "import math\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# import xgboost as xgb\n",
    "warnings.filterwarnings('ignore')\n",
    "# import GPy, GPyOpt\n",
    "tfkl = tf.keras.layers\n",
    "tfk = tf.keras\n",
    "from tcn import TCN\n",
    "\n",
    "from rstl import STL\n",
    "from texttable import Texttable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1a5b2ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:44.785798Z",
     "iopub.status.busy": "2024-09-26T19:22:44.785496Z",
     "iopub.status.idle": "2024-09-26T19:22:47.904519Z",
     "shell.execute_reply": "2024-09-26T19:22:47.903736Z"
    },
    "papermill": {
     "duration": 3.145648,
     "end_time": "2024-09-26T19:22:47.906815",
     "exception": false,
     "start_time": "2024-09-26T19:22:44.761167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsforecast.models import AutoARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5770e1a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:47.954321Z",
     "iopub.status.busy": "2024-09-26T19:22:47.953711Z",
     "iopub.status.idle": "2024-09-26T19:22:47.958076Z",
     "shell.execute_reply": "2024-09-26T19:22:47.957165Z"
    },
    "papermill": {
     "duration": 0.029797,
     "end_time": "2024-09-26T19:22:47.960101",
     "exception": false,
     "start_time": "2024-09-26T19:22:47.930304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "678e367b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:48.005448Z",
     "iopub.status.busy": "2024-09-26T19:22:48.005157Z",
     "iopub.status.idle": "2024-09-26T19:22:48.010774Z",
     "shell.execute_reply": "2024-09-26T19:22:48.010117Z"
    },
    "papermill": {
     "duration": 0.030502,
     "end_time": "2024-09-26T19:22:48.012660",
     "exception": false,
     "start_time": "2024-09-26T19:22:47.982158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import KLDivergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3a06e88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:48.059203Z",
     "iopub.status.busy": "2024-09-26T19:22:48.058931Z",
     "iopub.status.idle": "2024-09-26T19:22:48.079595Z",
     "shell.execute_reply": "2024-09-26T19:22:48.078944Z"
    },
    "papermill": {
     "duration": 0.046487,
     "end_time": "2024-09-26T19:22:48.081414",
     "exception": false,
     "start_time": "2024-09-26T19:22:48.034927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset_params(dataset_name = 'cif-12'):\n",
    "    suilin_smape = False\n",
    "\n",
    "    #-------------------------------------------------- CIF 2016 ------------------------------------------------#\n",
    "    if dataset_name == 'cif-6':\n",
    "        dataset_path = '/kaggle/working'+'/'+'CIF2016'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/cifnewdataset/6.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "        features = None #pd.read_csv(\"/kaggle/input/ae-cif012-unsupervised-ae/Features_LSTM_cif012_32.csv\",sep=',', header=None) # CIF 2016 - Horizon 6\n",
    "#         features = pd.read_csv('/kaggle/input/cif-data-and-hynd-khs-feature/fs_hyndman_freqfind_cif12.csv',sep=',', header=0) # CIF 2016 - Horizon 6\n",
    "\n",
    "        lag = 15\n",
    "        look_forward = 6\n",
    "        batch_size = 3\n",
    "        epochs = 30\n",
    "        learning_rate = 0.0001\n",
    "        features=raw_data\n",
    "\n",
    "        suilin_smape = False\n",
    "        frequency = None\n",
    "\n",
    "    elif dataset_name == 'cif-12':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'CIF2016'\n",
    "\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/cifnewdataset/12.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "#         features = pd.read_csv(\"/kaggle/input/ae-cif012-unsupervised-ae/Features_LSTM_cif012_32.csv\",sep=',', header=None) # CIF 2016 - Horizon 6\n",
    "        #features = pd.read_csv('/kaggle/input/cif-data-and-hynd-khs-feature/fs_hyndman_freqfind_cif12.csv',sep=',', header=0) # CIF 2016 - Horizon 6\n",
    "#         raw_data = raw_data.iloc[:, :-6]\n",
    "        lag = 36\n",
    "        look_forward = 12\n",
    "        batch_size = 6\n",
    "        epochs = 25\n",
    "        learning_rate = 0.001\n",
    "\n",
    "        suilin_smape = False\n",
    "        frequency = 12\n",
    "#         frequency = None\n",
    "    \n",
    "    elif dataset_name == 'tourism':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'tourism'\n",
    "\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/newtsdatasets/Tourism-new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "\n",
    "        #         features = pd.read_csv('/kaggle/input/tourism/fs_hyndman_f-4_tourism.csv',sep=',', header=0) # CIF 2016 - Horizon 6\n",
    "        #     features = pd.read_csv('/kaggle/input/ae-tourism-unsupervised-cnn/Features_tourism_32.csv',sep=',', header=None) # CIF 2016 - Horizon 6\n",
    "\n",
    "\n",
    "        lag = 12\n",
    "        look_forward = 8\n",
    "        batch_size = 100\n",
    "        epochs =50\n",
    "        learning_rate = 0.001\n",
    "        suilin_smape = False\n",
    "        frequency = 4\n",
    "        # frequency = None   \n",
    "    elif dataset_name == 'hospital':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'hospital'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/newtsdatasets/Hospital_new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "\n",
    "#         raw_data = pd.read_csv(\"/kaggle/input/hospital-dataset/hospital_dataset.txt\",sep='delimeter', header=None) # Hospital - Horizon 12\n",
    "#         features = pd.read_csv(\"/kaggle/input/hospital-dataset/hospital_dataset-hyndman-features.csv\",sep=',', header=0) # CIF 2016 - Horizon 6\n",
    "#         features = pd.read_csv(\"/kaggle/input/ae-hospital-unsupervised-ae/Features_hospital_LSTM_8.csv\",sep=',', header=None) # CIF 2016 - Horizon 6\n",
    "\n",
    "        lag = 30\n",
    "        look_forward = 12\n",
    "        batch_size = 20\n",
    "        epochs = 50\n",
    "        learning_rate = 0.0001\n",
    "#         frequency = 12\n",
    "        frequency = None\n",
    "        \n",
    "    elif dataset_name == 'm3-demo':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'M3'\n",
    "\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/newtsdatasets/m3-demo-new2.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "        #features = pd.read_csv(\"/kaggle/input/ae-m3-unsupervised-attenlstm/Features_demo_32.csv\",sep=',', header=None)\n",
    "        \n",
    "        lag = 28#29\n",
    "        look_forward = 18\n",
    "        batch_size = 7\n",
    "        epochs = 20\n",
    "        learning_rate = 0.0001\n",
    "        frequency = 12\n",
    "        # frequency = None\n",
    "\n",
    "    elif dataset_name == 'm3-finance':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'M3'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/newtsdatasets/m3-finance-new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "        #features = pd.read_csv(\"/kaggle/input/ae-m3-unsupervised-attenlstm/Features_finance_32.csv\",sep=',', header=None)\n",
    "        lag = 28\n",
    "        look_forward = 18\n",
    "        batch_size = 9\n",
    "        epochs = 25\n",
    "        learning_rate = 0.0001\n",
    "        frequency = 12\n",
    "        # frequency = None\n",
    "\n",
    "    elif dataset_name == 'm3-industry':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'M3'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/newtsdatasets/m3-industry-new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "        #features = pd.read_csv(\"/kaggle/input/ae-m3-unsupervised-attenlstm/Features_industry_32.csv\",sep=',', header=None)\n",
    "        lag = 28\n",
    "        look_forward = 18\n",
    "        batch_size = 10\n",
    "        epochs = 25\n",
    "        learning_rate = 0.0001\n",
    "        frequency = 12\n",
    "        # frequency = None\n",
    "\n",
    "    elif dataset_name == 'm3-macro':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'M3'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/newtsdatasets/m3-macro-new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "        #features = pd.read_csv(\"/kaggle/input/ae-m3-unsupervised-attenlstm/Features_macro_32.csv\",sep=',', header=None) # CIF 2016 - Horizon 6\n",
    "\n",
    "        lag = 28\n",
    "        look_forward = 18\n",
    "        batch_size = 15\n",
    "        epochs = 25\n",
    "        learning_rate = 0.0001\n",
    "        frequency = 12\n",
    "        # frequency = None\n",
    "\n",
    "    elif dataset_name == 'm3-micro':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'M3'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/newtsdatasets/M3-micro-new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "        #features = pd.read_csv(\"/kaggle/input/ae-m3-unsupervised-attenlstm/Features_micro_32.csv\",sep=',', header=None) # CIF 2016 - Horizon 6\n",
    "\n",
    "        lag = 28\n",
    "        look_forward = 18\n",
    "        batch_size = 18\n",
    "        epochs = 25\n",
    "        learning_rate = 0.0001\n",
    "#         frequency = 12\n",
    "        frequency = None\n",
    "\n",
    "    elif dataset_name == 'm3-other':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'M3'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/newtsdatasets/m3-other-new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "        #features = pd.read_csv(\"/kaggle/input/ae-m3-unsupervised-attenlstm/Features_other_32.csv\",sep=',', header=None) # CIF 2016 - Horizon 6\n",
    "\n",
    "        lag = 28\n",
    "        look_forward = 18\n",
    "        batch_size = 2\n",
    "        epochs = 25\n",
    "        learning_rate = 0.0001\n",
    "        frequency = 12\n",
    "        # frequency = None\n",
    "\n",
    "    #-------------------------------------------------- Hospital ------------------------------------------------#\n",
    "    #-------------------------------------------------- Hospital ------------------------------------------------#\n",
    "\n",
    "    \n",
    "    #------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "    sample_overlap = look_forward - 1\n",
    "\n",
    "    raw_data = raw_data.to_numpy().astype('float64')\n",
    "    features = None # features.to_numpy().astype('float64')\n",
    "    dataset = []\n",
    "    seri_len=[]\n",
    "    for i in range(len(raw_data)):\n",
    "        dataset.append(raw_data[i][~np.isnan(raw_data[i])])\n",
    "        seri_len.append(len(raw_data[i][~np.isnan(raw_data[i])]))\n",
    "\n",
    "\n",
    "    print(dataset_name,np.min(seri_len),np.max(seri_len))\n",
    "            \n",
    "\n",
    "\n",
    "    return dataset, features, lag, look_forward, sample_overlap, learning_rate, dataset_path, suilin_smape, frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09398f8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:48.127134Z",
     "iopub.status.busy": "2024-09-26T19:22:48.126870Z",
     "iopub.status.idle": "2024-09-26T19:22:48.147931Z",
     "shell.execute_reply": "2024-09-26T19:22:48.147229Z"
    },
    "papermill": {
     "duration": 0.04602,
     "end_time": "2024-09-26T19:22:48.149794",
     "exception": false,
     "start_time": "2024-09-26T19:22:48.103774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_dataset(dataset, look_forward ):\n",
    "    data_means = [];\n",
    "    for index in range(len(dataset)):\n",
    "    # Mean Noramlization\n",
    "        series_mean = np.mean(dataset[index][:len(dataset[index]) - look_forward]) # Train Mean: look_forward || Full Mean: Mean: look_forward = 0\n",
    "\n",
    "        if series_mean == 0:\n",
    "            series_mean = 0.001\n",
    "\n",
    "        data_means.append(series_mean)\n",
    "        dataset[index] = np.divide(dataset[index], series_mean)\n",
    "        if np.min(dataset[index][:len(dataset[index])])<=0:\n",
    "                    dataset[index] = np.log(dataset[index] + 1)\n",
    "        else:\n",
    "             dataset[index] = np.log(dataset[index])\n",
    "        # Log Transformation\n",
    "#         dataset[index] = np.log(dataset[index] + 1)\n",
    "       \n",
    "\n",
    "\n",
    "    return dataset, np.array(data_means)\n",
    "\n",
    "def rescale_data_to_main_value(data, means, dataset_seasonal = []):\n",
    "    for index in range(len(data)):\n",
    "        if len(dataset_seasonal) != 0:\n",
    "            data[index] = data[index] + dataset_seasonal[index]\n",
    "        # Revert Log Transformation\n",
    "        data[index] = np.e ** data[index]\n",
    "#         data[index] = data[index]\n",
    "\n",
    "        # Revert Mean Normalization\n",
    "        data[index] = means[index] * data[index]\n",
    "    \n",
    "\n",
    "    return data\n",
    "\n",
    "def normalize_feature_vectors(features):\n",
    "    #------------------- Z-score ----------------------#\n",
    "#     means = features.mean(0)\n",
    "#     stds = features.std(0)\n",
    "\n",
    "#     for i in range(len(features)):\n",
    "#         features[i] = (features[i] - means) / stds\n",
    "\n",
    "    #--------------------Min - Max---------------------#\n",
    "    minimum = features.min(0)\n",
    "    maximum = features.max(0)\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        features[i] = (features[i] - minimum) / (maximum - minimum)\n",
    "\n",
    "\n",
    "    return features\n",
    "\n",
    "\"\"\"![root_mean_square_deviation.svg](attachment:root_mean_square_deviation.svg)\"\"\"\n",
    "\n",
    "#RMSE\n",
    "def root_mean_squared_error(actual, forecast, method = 'single_value'):\n",
    "    # Methods = single_value | per_series\n",
    "    if method == 'single_value':\n",
    "        #Flatten To One Vector\n",
    "        actual = actual.flatten()\n",
    "        forecast = forecast.flatten()\n",
    "\n",
    "        return np.sqrt(np.mean(np.square(actual - forecast)))\n",
    "    elif method == 'per_series':\n",
    "        rmses = []\n",
    "        for i in range(len(actual)):\n",
    "            rmses.append(np.sqrt(np.mean(np.square(actual[i] - forecast[i]))))\n",
    "\n",
    "        return rmses\n",
    "\n",
    "\"\"\"![YIy33.png](attachment:YIy33.png)\"\"\"\n",
    "\n",
    "#SMAPE\n",
    "def single_point_smape(actual, forecast, suilin_smape = False):\n",
    "    if suilin_smape == True:\n",
    "        epsilon = 0.1\n",
    "\n",
    "        return (np.sum(2 * np.abs(forecast - actual) / max((np.abs(actual) + np.abs(forecast))+ epsilon, 0.5 + epsilon)))\n",
    "    else:\n",
    "        return (np.sum(2 * np.abs(forecast - actual) / (np.abs(actual) + np.abs(forecast))))\n",
    "\n",
    "def smape(actual, forecast, method = 'single_value', suilin_smape = False):\n",
    "    # Methods = single_value | per_series\n",
    "    if method == 'single_value':\n",
    "        #Flatten To One Vector\n",
    "        actual = actual.flatten()\n",
    "        forecast = forecast.flatten()\n",
    "        sum_smape = 0\n",
    "        for i in range(len(actual)):\n",
    "            sum_smape += single_point_smape(actual[i], forecast[i], suilin_smape)\n",
    "        return 100 * sum_smape / len(actual)\n",
    "\n",
    "    elif method == 'per_series':\n",
    "        smapes = []\n",
    "        for i in range(len(actual)):\n",
    "            sum_smape = 0\n",
    "            for j in range(len(actual[i])):\n",
    "                sum_smape += single_point_smape(actual[i,j], forecast[i,j], suilin_smape)\n",
    "            smapes.append(100 * sum_smape / len(actual[i]))\n",
    "        return np.array(smapes)\n",
    "\n",
    "# Create Samples from DataSet\n",
    "def create_dataset(sample, look_back, look_forward, sample_overlap, dataset_seasonal):\n",
    "    if(sample_overlap >= look_forward or sample_overlap < 0): sample_overlap = look_forward - 1\n",
    "    if(look_forward == 1): sample_overlap = 0\n",
    "\n",
    "    dataX, dataY, dataY_seasonal = [], [], []\n",
    "    dataX_means, dataY_means = [], []\n",
    "    for i in range(0, len(sample) - look_back - look_forward+1, look_forward - sample_overlap):\n",
    "        dataX.append(sample[i:(i+look_back), 0])\n",
    "        dataY.append(sample[(i + look_back):(i + look_back + look_forward), 0])\n",
    "\n",
    "        dataY_seasonal.append(dataset_seasonal[(i + look_back):(i + look_back + look_forward)])\n",
    "\n",
    "\n",
    "    return np.array(dataX), np.array(dataY), np.array(dataY_seasonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "966547ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:48.195852Z",
     "iopub.status.busy": "2024-09-26T19:22:48.195598Z",
     "iopub.status.idle": "2024-09-26T19:22:48.207389Z",
     "shell.execute_reply": "2024-09-26T19:22:48.206541Z"
    },
    "papermill": {
     "duration": 0.037059,
     "end_time": "2024-09-26T19:22:48.209300",
     "exception": false,
     "start_time": "2024-09-26T19:22:48.172241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset3_arima(sample, look_back, look_forward, sample_overlap, dataset_seasonal,dataset_name):\n",
    "    if(sample_overlap >= look_forward or sample_overlap < 0): sample_overlap = look_forward - 1\n",
    "    print('dddd')\n",
    "    if(look_forward == 1): sample_overlap = 0\n",
    "    # print(\"sample.shape\",sample.shape)\n",
    "    sample_trn=sample[0:len(sample)-look_forward]\n",
    "    frequency=12\n",
    "    if dataset_name=='tourism':\n",
    "        frequency=4\n",
    "    if dataset_name=='cif-6':\n",
    "        frequency=12\n",
    "#     print(\"freq\",frequency)\n",
    "    #         fit1 = pm.auto_arima(sample_trn,trace=True,error_action=\"ignore\",stepwise=False,seasonal=True,m=frequency,information_criterion='aic')#,stepwise=True,information_criterion='aic')\n",
    "\n",
    "    if len(sample_trn) > frequency*2:\n",
    "        \n",
    "        fit1 =pm.auto_arima(sample_trn, seasonal=True, m=12,maxiter=5,n_jobs=-1)\n",
    "        aug_seri, conf_int = fit1.predict(n_periods=look_forward, return_conf_int=True)\n",
    "        aug_seri = aug_seri.reshape(1,-1)\n",
    "           \n",
    "           \n",
    "    elif len(sample_trn)<frequency*2 and len(sample_trn)>frequency :\n",
    "        frequency = int(frequency/2)\n",
    "        fit1 = pm.auto_arima(sample_trn, seasonal=True, m=12,maxiter=5)\n",
    "\n",
    "        aug_seri, conf_int = fit1.predict(n_periods=look_forward, return_conf_int=True)\n",
    "        aug_seri = aug_seri.reshape(1,-1)\n",
    "    else:\n",
    "        fit1 = pm.auto_arima(sample_trn, seasonal=True, m=12,maxiter=5)\n",
    "        aug_seri, conf_int = fit1.predict(n_periods=look_forward, return_conf_int=True)\n",
    "        aug_seri = aug_seri.reshape(1,-1)\n",
    "\n",
    "\n",
    "#     fit1 = ExponentialSmoothing(endog=pd.Series(sample_trn.flatten()), seasonal_periods=12, trend='add',\n",
    "#                                 seasonal='add').fit()\n",
    "#     aug_seri = fit1.forecast(steps=look_forward).values.reshape(1,-1)\n",
    "    # print(aug_seri)\n",
    "    from_train = sample_trn[-(look_back+look_forward-1):]\n",
    "    frm_train_aug=np.concatenate([from_train.reshape(-1,1),aug_seri.reshape(-1,1)])\n",
    "    frm_train_aug=frm_train_aug.flatten()\n",
    "    # print(frm_train_aug)\n",
    "    # print(from_train[:,0])\n",
    "\n",
    "\n",
    "    aug_trainX,aug_trainY=[],[]\n",
    "    for i in range(0, len(frm_train_aug) - look_back - look_forward+1, look_forward - sample_overlap):\n",
    "        aug_trainX.append(frm_train_aug[i:(i+look_back)])\n",
    "        aug_trainY.append(frm_train_aug[(i + look_back):(i + look_back + look_forward)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # dataY_seasonal.append(dataset_seasonal[(i + look_back):(i + look_back + look_forward)])\n",
    "\n",
    "    return np.array(aug_trainX), np.array(aug_trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ac6d484",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:48.255025Z",
     "iopub.status.busy": "2024-09-26T19:22:48.254738Z",
     "iopub.status.idle": "2024-09-26T19:22:48.265950Z",
     "shell.execute_reply": "2024-09-26T19:22:48.265158Z"
    },
    "papermill": {
     "duration": 0.036297,
     "end_time": "2024-09-26T19:22:48.267838",
     "exception": false,
     "start_time": "2024-09-26T19:22:48.231541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset2(sample, look_back, look_forward, sample_overlap, dataset_seasonal,dataset_name):\n",
    "    if(sample_overlap >= look_forward or sample_overlap < 0): sample_overlap = look_forward - 1\n",
    "    if(look_forward == 1): sample_overlap = 0\n",
    "    # print(\"sample.shape\",sample.shape)\n",
    "    sample_trn=sample[0:len(sample)-look_forward]\n",
    "    frequency=12\n",
    "    if dataset_name=='tourism':\n",
    "        frequency=4\n",
    "    if dataset_name=='cif-6':\n",
    "        frequency=12\n",
    "#     print(\"freq\",frequency)\n",
    "    \n",
    "    if len(sample_trn) > frequency*2:\n",
    "        fit1 = ExponentialSmoothing(endog=pd.Series(sample_trn.flatten()), seasonal_periods=frequency, trend='add',\n",
    "                            seasonal='add').fit()\n",
    "        aug_seri = fit1.forecast(steps=look_forward).values.reshape(1,-1)\n",
    "           \n",
    "           \n",
    "    elif len(sample_trn)<frequency*2 and len(sample_trn)>frequency :\n",
    "        frequency = int(frequency/2)\n",
    "        fit1 = ExponentialSmoothing(endog=pd.Series(sample_trn.flatten()), seasonal_periods=frequency, trend='add',\n",
    "                            seasonal='add').fit()\n",
    "        aug_seri = fit1.forecast(steps=look_forward).values.reshape(1,-1)\n",
    "    else:\n",
    "        fit1 = ExponentialSmoothing(endog=pd.Series(sample_trn.flatten())).fit()\n",
    "\n",
    "        aug_seri = fit1.forecast(steps=look_forward).values.reshape(1,-1)\n",
    "\n",
    "\n",
    "#     fit1 = ExponentialSmoothing(endog=pd.Series(sample_trn.flatten()), seasonal_periods=12, trend='add',\n",
    "#                                 seasonal='add').fit()\n",
    "#     aug_seri = fit1.forecast(steps=look_forward).values.reshape(1,-1)\n",
    "    # print(aug_seri)\n",
    "    from_train = sample_trn[-(look_back+look_forward-1):]\n",
    "    frm_train_aug=np.concatenate([from_train.reshape(-1,1),aug_seri.reshape(-1,1)])\n",
    "    frm_train_aug=frm_train_aug.flatten()\n",
    "    # print(frm_train_aug)\n",
    "    # print(from_train[:,0])\n",
    "\n",
    "\n",
    "    aug_trainX,aug_trainY=[],[]\n",
    "    for i in range(0, len(frm_train_aug) - look_back - look_forward+1, look_forward - sample_overlap):\n",
    "        aug_trainX.append(frm_train_aug[i:(i+look_back)])\n",
    "        aug_trainY.append(frm_train_aug[(i + look_back):(i + look_back + look_forward)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # dataY_seasonal.append(dataset_seasonal[(i + look_back):(i + look_back + look_forward)])\n",
    "\n",
    "    return np.array(aug_trainX), np.array(aug_trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71cbf04e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:48.313119Z",
     "iopub.status.busy": "2024-09-26T19:22:48.312853Z",
     "iopub.status.idle": "2024-09-26T19:22:48.320446Z",
     "shell.execute_reply": "2024-09-26T19:22:48.319623Z"
    },
    "papermill": {
     "duration": 0.032234,
     "end_time": "2024-09-26T19:22:48.322297",
     "exception": false,
     "start_time": "2024-09-26T19:22:48.290063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stl_decomposition2(dataset, frequency,look):\n",
    "    seasonal = []\n",
    "    trend = []\n",
    "    for index in range(len(dataset)):\n",
    "        if frequency != None:\n",
    "            stl = STL(dataset[index][:len(dataset[index]) - look], frequency, \"periodic\")\n",
    "            stl_tot = STL(dataset[index], frequency, \"periodic\")\n",
    "\n",
    "            seasonal.append(np.concatenate([stl.seasonal,stl_tot.seasonal[-look:]]))\n",
    "            trend.append(np.concatenate([stl.trend,stl_tot.trend[-look:]]))\n",
    "            # deseason_part=dataset[index][:len(dataset[index]) - look] - stl.seasonal\n",
    "            # d=np.concatenate([deseason_part,dataset[index][-look:]])\n",
    "            dataset[index]=dataset[index]-np.concatenate([stl.seasonal,stl_tot.seasonal[-look:]])\n",
    "        else:\n",
    "            seasonal.append(np.zeros((dataset[index].shape)))\n",
    "            trend.append(np.zeros((dataset[index].shape)))\n",
    "\n",
    "    return dataset, np.array(seasonal), np.array(trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d78546bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:48.368172Z",
     "iopub.status.busy": "2024-09-26T19:22:48.367388Z",
     "iopub.status.idle": "2024-09-26T19:22:48.374948Z",
     "shell.execute_reply": "2024-09-26T19:22:48.374066Z"
    },
    "papermill": {
     "duration": 0.032551,
     "end_time": "2024-09-26T19:22:48.376888",
     "exception": false,
     "start_time": "2024-09-26T19:22:48.344337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def single_point_mase(actual, forecast, insample, frequency) :\n",
    "    if frequency==None:\n",
    "        frequency=12\n",
    "    # print(\"HHHHHHHHHHHHHHHHHHHHHHHHHHHH\")\n",
    "    return np.mean(np.abs(forecast - actual)) / np.mean(np.abs(insample[:-frequency] - insample[frequency:]))\n",
    "\n",
    "\n",
    "def mase(actual, forecast, insample,frequency):\n",
    "    if frequency==None:\n",
    "        frequency=12\n",
    "    print(\"shapes\",actual.shape,forecast.shape,insample.shape)\n",
    "    # print(insample)\n",
    "    MASE = []\n",
    "    for i in range(len(actual)):\n",
    "        sum_MASE= 0\n",
    "        for j in range(len(actual[i])):\n",
    "            sum_MASE += single_point_mase(actual[i, j], forecast[i, j],insample[i][:-len(actual[i])], frequency)\n",
    "        MASE.append(sum_MASE / len(actual[i]))\n",
    "    return np.array(MASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5309253",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:48.425060Z",
     "iopub.status.busy": "2024-09-26T19:22:48.424534Z",
     "iopub.status.idle": "2024-09-26T19:22:48.430855Z",
     "shell.execute_reply": "2024-09-26T19:22:48.430000Z"
    },
    "papermill": {
     "duration": 0.033893,
     "end_time": "2024-09-26T19:22:48.432882",
     "exception": false,
     "start_time": "2024-09-26T19:22:48.398989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mase_val(actual, forecast, insample,frequency):\n",
    "    if frequency==None:\n",
    "        frequency=12\n",
    "    print(\"shapes\",actual.shape,forecast.shape,insample.shape)\n",
    "    # print(insample)\n",
    "    MASE = []\n",
    "    for i in range(len(actual)):\n",
    "        sum_MASE= 0\n",
    "        for j in range(len(actual[i])):\n",
    "            sum_MASE += single_point_mase(actual[i, j], forecast[i, j],insample[i][:-2*len(actual[i])], frequency)\n",
    "        MASE.append(sum_MASE / len(actual[i]))\n",
    "    return np.array(MASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5594ac8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:48.479146Z",
     "iopub.status.busy": "2024-09-26T19:22:48.478842Z",
     "iopub.status.idle": "2024-09-26T19:22:48.501434Z",
     "shell.execute_reply": "2024-09-26T19:22:48.500725Z"
    },
    "papermill": {
     "duration": 0.048418,
     "end_time": "2024-09-26T19:22:48.503423",
     "exception": false,
     "start_time": "2024-09-26T19:22:48.455005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_sample(look_forward,sample_seasonal,dataX, dataY, data_mean, dataY_seasonal,frequency):\n",
    "    test_size=1\n",
    "    val_size=1\n",
    "    \n",
    "\n",
    "    train_size0=(len(dataX)-test_size)-look_forward+1\n",
    "    train_size=(len(dataX)-test_size)\n",
    "\n",
    "\n",
    "    trainX, testX = dataX[0:train_size0-val_size,:], dataX[train_size:,:]  #note -valsize is added now\n",
    "    trainY, testY = dataY[0:train_size0-val_size,:], dataY[train_size:,:]\n",
    "\n",
    "    valX, valY = dataX[train_size0-val_size:train_size0,:],dataY[train_size0-val_size:train_size0, :]\n",
    "    # print(\"valX shape\",valX.shape)\n",
    "    # print(\"valY shape\",valY.shape)\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0],1, trainX.shape[1]))\n",
    "    valX = np.reshape(valX, (valX.shape[0],1, valX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0],1, testX.shape[1]))\n",
    "\n",
    "    val_means = np.full(len(valY), data_mean)\n",
    "    test_means = np.full(len(testY), data_mean)\n",
    "\n",
    "    val_seasonal = dataY_seasonal[train_size0-val_size:train_size0, :]\n",
    "    # print(\"len val\",len(val_seasonal))\n",
    "    # print(\"val\",val_seasonal)\n",
    "\n",
    "    train=dataY_seasonal[:train_size0, :]\n",
    "    train=train.reshape(-1,1)\n",
    "#     train2=train[:len(train):len(valY[0])]\n",
    "    # print(train2.reshape(1,-1))\n",
    "    # print(len(train2))\n",
    "\n",
    "    test2 = []\n",
    "\n",
    "\n",
    "    # modl = pm.auto_arima(train2, trace=False,seasonal=True, stepwise=False, information_criterion='aic',)\n",
    "    # preds = modl.predict(n_periods=len(valY[0]))\n",
    "    # print(\"xx\",train2.shape)\n",
    "    sample_size = len(sample_seasonal.flatten()) - look_forward\n",
    "\n",
    "    train3=sample_seasonal[:sample_size].flatten()\n",
    "#     print(\"len,frequency\",len(train3.flatten()),frequency)\n",
    "    if frequency!=None:\n",
    "        if len(train3.flatten()) > frequency*2:\n",
    "            sp = frequency\n",
    "           \n",
    "            fit1 = ExponentialSmoothing(endog=pd.Series(train3.flatten()), seasonal_periods=sp,trend='add', seasonal='add').fit()\n",
    "\n",
    "        elif len(train3.flatten())<frequency*2 and len(train3.flatten())>frequency :\n",
    "            sp = int(frequency/2)\n",
    "            fit1 = ExponentialSmoothing(endog=pd.Series(train3.flatten()), seasonal_periods=sp, trend='add',\n",
    "                                        seasonal='add').fit()\n",
    "        else:\n",
    "            fit1 = ExponentialSmoothing(endog=pd.Series(train3.flatten())).fit()\n",
    "\n",
    "        preds2 = fit1.forecast(steps=look_forward).values.reshape(1,-1)\n",
    "\n",
    "        # print(\"preds\",preds2,type(preds2))\n",
    "    else:\n",
    "        preds2=np.zeros(look_forward)\n",
    "#     for i in range(0,len(val_seasonal[0])):\n",
    "#         test2.append(val_seasonal[0][len(val_seasonal[0])-1])\n",
    "\n",
    "    # print(\"datay_seasonal\",dataY_seasonal)\n",
    "\n",
    "    test_seasonal_y = dataY_seasonal[train_size:,:]\n",
    "    # print(\"test\",test_seasonal_y)\n",
    "    # print(\"train2\",train2.flatten())\n",
    "    # np.savetxt('train.csv', train2.flatten(), delimiter=', ')\n",
    "\n",
    "    return np.array(trainX),np.array(valX),np.array(testX),np.array(trainY),np.array(valY),np.array(testY), test_means, val_means, val_seasonal,test_seasonal_y, preds2\n",
    "\n",
    "\n",
    "# Preprocess Data For Sampling\n",
    "def all_pre_process(all_dataset, lag, look_forward, sample_overlap, data_means, dataset_seasonal,frequency):\n",
    "    look_back = lag\n",
    "\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "\n",
    "\n",
    "    valX = []\n",
    "    valY = []\n",
    "\n",
    "    testX = []\n",
    "    testY = []\n",
    "\n",
    "    all_test_means = []\n",
    "    all_val_means = []\n",
    "\n",
    "    all_test_seasonals = []\n",
    "    all_test_seasonals2 = []\n",
    "    all_val_seasonals = []\n",
    "\n",
    "    for index in range(len(all_dataset)):\n",
    "        sample = np.array(all_dataset[index])\n",
    "        sample = sample.reshape(sample.shape[0], 1)\n",
    "        # sample_seasonal=np.array(dataset_seasonal[index])\n",
    "\n",
    "        dataX_s, dataY_s, dataY_seasonal = create_dataset(sample, look_back, look_forward, sample_overlap, dataset_seasonal[index])\n",
    "\n",
    "    \n",
    "        temp_trainX, temp_valX, temp_testX, temp_trainY, temp_valY, temp_testY, test_means, val_means, val_seasonal, test_seasonal,test2 = create_sample(look_forward,dataset_seasonal[index],dataX_s,dataY_s,data_means[index], dataY_seasonal,frequency)\n",
    "\n",
    "        trainX = trainX + temp_trainX.tolist()\n",
    "        trainY = trainY + temp_trainY.tolist()\n",
    "        #for adding augmented data\n",
    "#         trainX = trainX + aug_trainX.tolist()\n",
    "#         trainY = trainY + aug_trainY.tolist()\n",
    "\n",
    "        valX = valX + temp_valX.tolist()\n",
    "        valY = valY + temp_valY.tolist()\n",
    "\n",
    "        testX = testX + temp_testX.tolist()\n",
    "        testY = testY + temp_testY.tolist()\n",
    "\n",
    "        all_test_means = all_test_means + test_means.tolist()\n",
    "        all_val_means = all_val_means + val_means.tolist()\n",
    "        all_test_seasonals = all_test_seasonals +  test_seasonal.tolist()\n",
    "        all_test_seasonals2 = all_test_seasonals2 +test2.tolist()  # test_seasonal.tolist() #\"NOTE\"\n",
    "        all_val_seasonals = all_val_seasonals + val_seasonal.tolist()\n",
    "\n",
    "\n",
    "    return np.array(trainX), np.array(valX), np.array(testX), np.array(trainY), np.array(valY), np.array(testY), np.array(all_test_means), np.array(all_val_means), np.array(all_val_seasonals), np.array(all_test_seasonals),np.array(all_test_seasonals2)\n",
    "\n",
    "def save_prediction_result(data, dataset_name = 'cif-6', dataset_path = ''):\n",
    "    if dataset_name == '':\n",
    "        filename = dataset_name + '-results.csv'\n",
    "    else:\n",
    "        filename = dataset_path + '/' + dataset_name + '-results.csv'\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, sep=',',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d37decb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:48.549134Z",
     "iopub.status.busy": "2024-09-26T19:22:48.548800Z",
     "iopub.status.idle": "2024-09-26T19:22:48.564583Z",
     "shell.execute_reply": "2024-09-26T19:22:48.563496Z"
    },
    "papermill": {
     "duration": 0.041046,
     "end_time": "2024-09-26T19:22:48.566719",
     "exception": false,
     "start_time": "2024-09-26T19:22:48.525673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def all_pre_process_aug(all_dataset, lag, look_forward, sample_overlap, data_means, dataset_seasonal,frequency,dataset_name):\n",
    "    look_back = lag\n",
    "\n",
    "    trainX = []\n",
    "    trainY = []\n",
    " \n",
    "\n",
    "    valX = []\n",
    "    valY = []\n",
    "\n",
    "    testX = []\n",
    "    testY = []\n",
    "\n",
    "    all_test_means = []\n",
    "    all_val_means = []\n",
    "\n",
    "    all_test_seasonals = []\n",
    "    all_test_seasonals2 = []\n",
    "    all_val_seasonals = []\n",
    "\n",
    "    for index in range(len(all_dataset)):\n",
    "        sample = np.array(all_dataset[index])\n",
    "        sample = sample.reshape(sample.shape[0], 1)\n",
    "        # sample_seasonal=np.array(dataset_seasonal[index])\n",
    "\n",
    "        dataX_s, dataY_s, dataY_seasonal = create_dataset(sample, look_back, look_forward, sample_overlap, dataset_seasonal[index])\n",
    "        aug_trainX,aug_trainY = create_dataset2(sample, look_back, look_forward, sample_overlap, dataset_seasonal[index],dataset_name)\n",
    "#         aug_trainX,aug_trainY = create_dataset3_arima(sample, look_back, look_forward, sample_overlap, dataset_seasonal[index],dataset_name)\n",
    "\n",
    "        aug_trainX= np.reshape(aug_trainX, (aug_trainX.shape[0],1, aug_trainX.shape[1]))\n",
    "    \n",
    "        temp_trainX, temp_valX, temp_testX, temp_trainY, temp_valY, temp_testY, test_means, val_means, val_seasonal, test_seasonal,test2 = create_sample(look_forward,dataset_seasonal[index],dataX_s,dataY_s,data_means[index], dataY_seasonal,frequency)\n",
    "        # print(\"temp_trainX.shape\",temp_trainX.shape)\n",
    "        trainX = trainX + temp_trainX.tolist()\n",
    "        trainY = trainY + temp_trainY.tolist()\n",
    "        #for adding augmented data\n",
    "        trainX = trainX + aug_trainX.tolist()\n",
    "        trainY = trainY + aug_trainY.tolist()\n",
    "\n",
    "        valX = valX + temp_valX.tolist()\n",
    "        valY = valY + temp_valY.tolist()\n",
    "\n",
    "        testX = testX + temp_testX.tolist()\n",
    "        testY = testY + temp_testY.tolist()\n",
    "\n",
    "        all_test_means = all_test_means + test_means.tolist()\n",
    "        all_val_means = all_val_means + val_means.tolist()\n",
    "        all_test_seasonals = all_test_seasonals +  test_seasonal.tolist()\n",
    "        all_test_seasonals2 = all_test_seasonals2 +test2.tolist()  # test_seasonal.tolist() #\"NOTE\"\n",
    "        all_val_seasonals = all_val_seasonals + val_seasonal.tolist()\n",
    "\n",
    "\n",
    "    return np.array(trainX), np.array(valX), np.array(testX), np.array(trainY), np.array(valY), np.array(testY), np.array(all_test_means), np.array(all_val_means), np.array(all_val_seasonals), np.array(all_test_seasonals),np.array(all_test_seasonals2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "edf41ae7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:48.625175Z",
     "iopub.status.busy": "2024-09-26T19:22:48.624812Z",
     "iopub.status.idle": "2024-09-26T19:22:48.638707Z",
     "shell.execute_reply": "2024-09-26T19:22:48.637833Z"
    },
    "papermill": {
     "duration": 0.044379,
     "end_time": "2024-09-26T19:22:48.640734",
     "exception": false,
     "start_time": "2024-09-26T19:22:48.596355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_sample_sep_val(look_forward,sample_seasonal,dataX, dataY, data_mean, dataY_seasonal,frequency):\n",
    "    test_size=1\n",
    "    val_size=1\n",
    "    # print(frequency)\n",
    "    # print(sample_seasonal)\n",
    "    # test_size=int(len(dataX) * testsize)\n",
    "    # val_size=int((len(dataX) - test_size) * valsize)\n",
    "#     print(\"LENNNN\",len(dataX))\n",
    "    train_size=(len(dataX)-test_size-val_size)\n",
    "#     print(\"trainsize, train+val\",train_size,train_size +val_size)\n",
    "\n",
    "    trainX, testX = dataX[0:train_size,:], dataX[train_size+val_size:,:]\n",
    "    trainY, testY = dataY[0:train_size,:], dataY[train_size+val_size:,:]\n",
    "\n",
    "    valX, valY = dataX[train_size:train_size +val_size,:],dataY[train_size:train_size+val_size, :]\n",
    "#     print(\"LENNNN\",len(valX),len(valY),valX.shape,valY.shape)\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0],1, trainX.shape[1]))\n",
    "    valX = np.reshape(valX, (valX.shape[0],1, valX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0],1, testX.shape[1]))\n",
    "\n",
    "    val_means = np.full(len(valY), data_mean)\n",
    "    test_means = np.full(len(testY), data_mean)\n",
    "\n",
    "    val_seasonal = dataY_seasonal[train_size:train_size+val_size, :]\n",
    "    # print(\"len val\",len(val_seasonal))\n",
    "#     print(\"valseasonal \",val_seasonal.shape)\n",
    "\n",
    "#     train=dataY_seasonal[:train_size, :]\n",
    "#     train=train.reshape(-1,1)\n",
    "   \n",
    "    # print(train2.reshape(1,-1))\n",
    "    # print(len(train2))\n",
    "\n",
    "#     test2 = []\n",
    "\n",
    "\n",
    "    # modl = pm.auto_arima(train2, trace=False,seasonal=True, stepwise=False, information_criterion='aic',)\n",
    "    # preds = modl.predict(n_periods=len(valY[0]))\n",
    "    # print(\"xx\",train2.shape)\n",
    "    sample_size = len(sample_seasonal.flatten()) - look_forward\n",
    "\n",
    "    train3=sample_seasonal[:sample_size].flatten()\n",
    "\n",
    "    if frequency!=None:\n",
    "        if len(train3.flatten()) > frequency*2:\n",
    "            sp = frequency\n",
    "            fit1 = ExponentialSmoothing(endog=pd.Series(train3.flatten()), seasonal_periods=sp,trend='add', seasonal='add').fit()\n",
    "\n",
    "        elif len(train3.flatten())<frequency*2 and len(train3.flatten())>frequency :\n",
    "            sp = int(frequency/2)\n",
    "            fit1 = ExponentialSmoothing(endog=pd.Series(train3.flatten()), seasonal_periods=sp, trend='add',\n",
    "                                        seasonal='add').fit()\n",
    "        else:\n",
    "            fit1 = ExponentialSmoothing(endog=pd.Series(train3.flatten())).fit()\n",
    "\n",
    "        preds2 = fit1.forecast(steps=look_forward).values.reshape(1,-1)\n",
    "\n",
    "        # print(\"preds\",preds2,type(preds2))\n",
    "    else:\n",
    "        preds2=np.zeros(look_forward)\n",
    "#     for i in range(0,len(val_seasonal[0])):\n",
    "#         test2.append(val_seasonal[0][len(val_seasonal[0])-1])\n",
    "\n",
    "    # print(\"datay_seasonal\",dataY_seasonal)\n",
    "\n",
    "    test_seasonal_y = dataY_seasonal[train_size+val_size:,:]\n",
    "    \n",
    "#     print(\"test_seasonal\",test_seasonal_y.shape)\n",
    "    # print(\"train2\",train2.flatten())\n",
    "    # np.savetxt('train.csv', train2.flatten(), delimiter=', ')\n",
    "\n",
    "    return np.array(trainX),np.array(valX),np.array(testX),np.array(trainY),np.array(valY),np.array(testY), test_means, val_means, val_seasonal,test_seasonal_y, preds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0bde792",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:48.686197Z",
     "iopub.status.busy": "2024-09-26T19:22:48.685932Z",
     "iopub.status.idle": "2024-09-26T19:22:48.696548Z",
     "shell.execute_reply": "2024-09-26T19:22:48.695722Z"
    },
    "papermill": {
     "duration": 0.035635,
     "end_time": "2024-09-26T19:22:48.698482",
     "exception": false,
     "start_time": "2024-09-26T19:22:48.662847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess Data For Sampling\n",
    "def all_pre_process(all_dataset, lag, look_forward, sample_overlap, data_means, dataset_seasonal,frequency):\n",
    "    look_back = lag\n",
    "\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "\n",
    "    valX = []\n",
    "    valY = []\n",
    "\n",
    "    testX = []\n",
    "    testY = []\n",
    "\n",
    "    all_test_means = []\n",
    "    all_val_means = []\n",
    "\n",
    "    all_test_seasonals = []\n",
    "    all_test_seasonals2 = []\n",
    "    all_val_seasonals = []\n",
    "\n",
    "    for index in range(len(all_dataset)):\n",
    "        sample = np.array(all_dataset[index])\n",
    "        sample = sample.reshape(sample.shape[0], 1)\n",
    "        # sample_seasonal=np.array(dataset_seasonal[index])\n",
    "\n",
    "        dataX_s, dataY_s, dataY_seasonal = create_dataset(sample, look_back, look_forward, sample_overlap, dataset_seasonal[index])\n",
    "        temp_trainX, temp_valX, temp_testX, temp_trainY, temp_valY, temp_testY, test_means, val_means, val_seasonal, test_seasonal,test2 = create_sample(look_forward,dataset_seasonal[index],dataX_s,dataY_s,data_means[index], dataY_seasonal,frequency)\n",
    "\n",
    "        trainX = trainX + temp_trainX.tolist()\n",
    "        trainY = trainY + temp_trainY.tolist()\n",
    "\n",
    "        valX = valX + temp_valX.tolist()\n",
    "        valY = valY + temp_valY.tolist()\n",
    "\n",
    "        testX = testX + temp_testX.tolist()\n",
    "        testY = testY + temp_testY.tolist()\n",
    "\n",
    "        all_test_means = all_test_means + test_means.tolist()\n",
    "        all_val_means = all_val_means + val_means.tolist()\n",
    "        all_test_seasonals = all_test_seasonals +  test_seasonal.tolist()\n",
    "        all_test_seasonals2 = all_test_seasonals2 +test2.tolist()  # test_seasonal.tolist() #\"NOTE\"\n",
    "        all_val_seasonals = all_val_seasonals + val_seasonal.tolist()\n",
    "\n",
    "\n",
    "    return np.array(trainX), np.array(valX), np.array(testX), np.array(trainY), np.array(valY), np.array(testY), np.array(all_test_means), np.array(all_val_means), np.array(all_val_seasonals), np.array(all_test_seasonals),np.array(all_test_seasonals2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8cc65ca5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:48.744360Z",
     "iopub.status.busy": "2024-09-26T19:22:48.744070Z",
     "iopub.status.idle": "2024-09-26T19:22:48.748094Z",
     "shell.execute_reply": "2024-09-26T19:22:48.747334Z"
    },
    "papermill": {
     "duration": 0.028959,
     "end_time": "2024-09-26T19:22:48.749972",
     "exception": false,
     "start_time": "2024-09-26T19:22:48.721013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tcn import TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f34ab1d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:48.795585Z",
     "iopub.status.busy": "2024-09-26T19:22:48.795228Z",
     "iopub.status.idle": "2024-09-26T19:22:48.804386Z",
     "shell.execute_reply": "2024-09-26T19:22:48.803590Z"
    },
    "papermill": {
     "duration": 0.03421,
     "end_time": "2024-09-26T19:22:48.806169",
     "exception": false,
     "start_time": "2024-09-26T19:22:48.771959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Prediction_Model(lag, look_forward, learning_rate, dense_neuron):\n",
    "    input_layer = layers.Input(shape = (1, lag,), name = \"Input-Layer\")\n",
    "    multi_head_attention_layer = TCN(return_sequences=True,dilations=[1, 2, 4, 8])(input_layer)\n",
    "    # multi_head_attention_layer=TCN(return_sequences=True)(input_layer)\n",
    "    # dot=keras.layers.Dot(axes=1)([multi_head_attention_layer, multi_head_attention_layer])\n",
    "    # flatten_layer1 = keras.layers.Flatten(name=\"Flatten-Layer\")(multi_head_attention_layer)\n",
    "    conv = keras.layers.Conv1D(64,\n",
    "                              strides=2,\n",
    "                              kernel_size=4,\n",
    "                              activation=None,\n",
    "                              padding=\"same\",)(multi_head_attention_layer)#multi_head_attention_layer\n",
    "    conv2 = keras.layers.Conv1D(16,\n",
    "                              strides=2,\n",
    "                              kernel_size=4,\n",
    "                              activation=None,\n",
    "                              padding=\"same\",)(conv)\n",
    "    flatten_layer2=keras.layers.Flatten(name=\"Flatten-Layer2\")(conv2)\n",
    "    # concat=keras.layers.concatenate([flatten_layer1,flatten_layer2])\n",
    "\n",
    "    dense_layer1 = Dense(\n",
    "                dense_neuron,\n",
    "                activation = 'linear',\n",
    "                name = \"Fully-Connected-Layer\")(flatten_layer2)\n",
    "\n",
    "    dense_layer2 = Dense(\n",
    "                look_forward,\n",
    "                activation = None,\n",
    "                name = \"Output-Layer\")(dense_layer1)\n",
    "\n",
    "\n",
    "\n",
    "                # Create Model\n",
    "    model = Model(inputs = [input_layer], outputs = dense_layer2)\n",
    "\n",
    "            # Optimizer\n",
    "    opt=tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "            # Compile Model\n",
    "    model.compile(loss=\"mse\",optimizer=opt,metrics=[\"mse\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a69f32b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:48.851317Z",
     "iopub.status.busy": "2024-09-26T19:22:48.851044Z",
     "iopub.status.idle": "2024-09-26T19:22:48.862088Z",
     "shell.execute_reply": "2024-09-26T19:22:48.861242Z"
    },
    "papermill": {
     "duration": 0.035807,
     "end_time": "2024-09-26T19:22:48.864001",
     "exception": false,
     "start_time": "2024-09-26T19:22:48.828194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def autoencoder(latent_dim=16, series_len=None):\n",
    "\n",
    "    # Input\n",
    "    x = Input((None, series_len),name='input')\n",
    "    #masked_seq = layers.Masking(mask_value=0.0)(encoder_inputs)\n",
    "    # Encoder\n",
    "    encoded = LSTM(256, return_sequences=True, name=\"encoded_lstm1\")(x)\n",
    "    encoded = LSTM(128, return_sequences=True, name=\"encoded_lstm2\")(encoded)\n",
    "    attention = Attention(name=\"encoded_atten\")([encoded, encoded, encoded])  # Apply self-attention to the encoder outputs\n",
    "    merged = layers.Concatenate(axis=-1)([encoded, attention])        \n",
    "    encoded = LSTM(64, name=\"encoded_lstm3\")(merged)\n",
    "    encoded = Dense(latent_dim, name=\"encoded_dense\")(encoded)\n",
    "\n",
    "    # Decoder\n",
    "    decoded = RepeatVector(1)(encoded)\n",
    "    decoded = LSTM(64, return_sequences=True, name=\"decoded_lstm1\")(decoded)\n",
    "    decoded = LSTM(128, return_sequences=True, name=\"decoded_lstm2\")(decoded)\n",
    "    decoded = LSTM(256, return_sequences=True, name=\"decoded_lstm3\")(decoded)\n",
    "    attention = Attention(name=\"decoded_atten\")([decoded, decoded, decoded])  # Apply self-attention to the decoder outputs\n",
    "    decoded = layers.Concatenate(axis=-1, name=\"decoded_concat\")([decoded, attention])\n",
    "    decoder_outputs = TimeDistributed(Dense(series_len), name=\"decoded_timeD\")(decoded)\n",
    "    \n",
    "    # AE model\n",
    "    autoencoder = Model(inputs=x, outputs=decoder_outputs, name='AE')\n",
    "\n",
    "    # Encoder model\n",
    "    encoder = Model(inputs=x, outputs=encoded, name='encoder')\n",
    "\n",
    "    # Create input for decoder model\n",
    "    encoded_input =Input(shape=(latent_dim,))\n",
    "    # Internal layers in decoder\n",
    "    decoded = RepeatVector(1)(encoded_input)\n",
    "    decoded = autoencoder.get_layer('decoded_lstm1')(decoded)\n",
    "    decoded = autoencoder.get_layer('decoded_lstm2')(decoded)\n",
    "    decoded = autoencoder.get_layer('decoded_lstm3')(decoded)\n",
    "    attention = autoencoder.get_layer('decoded_atten')([decoded, decoded, decoded])\n",
    "    decoded = autoencoder.get_layer('decoded_concat')([decoded, attention])\n",
    "    decoder_output = autoencoder.get_layer('decoded_timeD')(decoded)\n",
    "    # Decoder model\n",
    "    decoder = Model(inputs=encoded_input, outputs=decoder_output, name='decoder')\n",
    "\n",
    "    return autoencoder, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9de0276",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:48.909151Z",
     "iopub.status.busy": "2024-09-26T19:22:48.908853Z",
     "iopub.status.idle": "2024-09-26T19:22:48.925194Z",
     "shell.execute_reply": "2024-09-26T19:22:48.924361Z"
    },
    "papermill": {
     "duration": 0.041211,
     "end_time": "2024-09-26T19:22:48.927124",
     "exception": false,
     "start_time": "2024-09-26T19:22:48.885913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def autoencoderNeighbour(latent_dim=16, series_len=None):\n",
    "\n",
    "    # Input\n",
    "    x = Input((None, series_len), name='input')\n",
    "    #masked_seq = layers.Masking(mask_value=0.0)(encoder_inputs)\n",
    "    # Encoder\n",
    "    encoded = LSTM(256, return_sequences=True, name='lstm_input_256')(x)\n",
    "    encoded = LSTM(128, return_sequences=True, name='lstm_input_128')(encoded)\n",
    "    attention = Attention(name='attention_input')([encoded, encoded, encoded])  # Apply self-attention to the encoder outputs\n",
    "    merged = layers.Concatenate(axis=-1)([encoded, attention])        \n",
    "    encoded = LSTM(64, name='lstm_input_64')(merged)\n",
    "    encoded = Dense(latent_dim, name='dense_input')(encoded)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    x_neighbour1 = Input((None, series_len), name='Neighbour1_input')\n",
    "    x_neighbour2 = Input((None, series_len), name='Neighbour2_input')\n",
    "    \n",
    "    encoded_neighbour1 = LSTM(256, return_sequences=True, name='lstm_Neighbour1_256')(x_neighbour1)\n",
    "    encoded_neighbour2 = LSTM(256, return_sequences=True, name='lstm_Neighbour2_256')(x_neighbour2)\n",
    "    encoded_neighbour1 = LSTM(128, return_sequences=True, name='lstm_Neighbour1_128')(encoded_neighbour1)\n",
    "    encoded_neighbour2 = LSTM(128, return_sequences=True, name='lstm_Neighbour2_128')(encoded_neighbour2)\n",
    "    attention_neighbour1 = Attention(name='attention_Neighbour1')([encoded_neighbour1, encoded_neighbour2, encoded_neighbour2])  # Apply self-attention to the encoder outputs\n",
    "    attention_neighbour2 = Attention(name='attention_Neighbour2')([encoded_neighbour2, encoded_neighbour1, encoded_neighbour1])  # Apply self-attention to the encoder outputs\n",
    "    merged_neighbour1 = layers.Concatenate(axis=-1)([encoded_neighbour1, attention_neighbour1])        \n",
    "    merged_neighbour2 = layers.Concatenate(axis=-1)([encoded_neighbour2, attention_neighbour2])        \n",
    "    encoded_neighbour1 = LSTM(64, name='lstm_Neighbour1_64')(merged_neighbour1)\n",
    "    encoded_neighbour2 = LSTM(64, name='lstm_Neighbour2_64')(merged_neighbour2)\n",
    "    encoded_neighbour1 = Dense(latent_dim, name='dense_Neighbour1')(encoded_neighbour1)\n",
    "    encoded_neighbour2 = Dense(latent_dim, name='dense_Neighbour2')(encoded_neighbour2)\n",
    "\n",
    "    out_encoder_Neighbor = layers.Maximum()([encoded_neighbour1, encoded_neighbour2]) \n",
    "    # add Final encoder\n",
    "    encoded = layers.add([encoded ,out_encoder_Neighbor],name='add_inputs')\n",
    "    \n",
    "    \n",
    "    # Decoder\n",
    "    decoded = RepeatVector(1)(encoded)\n",
    "    decoded = LSTM(64, return_sequences=True, name=\"decoded_lstm1\")(decoded)\n",
    "    decoded = LSTM(128, return_sequences=True, name=\"decoded_lstm2\")(decoded)\n",
    "    decoded = LSTM(256, return_sequences=True, name=\"decoded_lstm3\")(decoded)\n",
    "    attention = Attention(name=\"decoded_atten\")([decoded, decoded, decoded])  # Apply self-attention to the decoder outputs\n",
    "    decoded = layers.Concatenate(axis=-1, name=\"decoded_concat\")([decoded, attention])\n",
    "    decoder_outputs = TimeDistributed(Dense(series_len), name=\"decoded_timeD\")(decoded)\n",
    "    \n",
    "    # AE model\n",
    "    autoencoder = Model(inputs=[x,x_neighbour1,x_neighbour2], outputs=decoder_outputs, name='AE')\n",
    "\n",
    "    # Encoder model\n",
    "    encoder = Model(inputs=[x,x_neighbour1,x_neighbour2], outputs=encoded, name='encoder')\n",
    "\n",
    "    # Create input for decoder model\n",
    "    encoded_input =Input(shape=(latent_dim,))\n",
    "    # Internal layers in decoder\n",
    "    decoded = RepeatVector(1)(encoded_input)\n",
    "    decoded = autoencoder.get_layer('decoded_lstm1')(decoded)\n",
    "    decoded = autoencoder.get_layer('decoded_lstm2')(decoded)\n",
    "    decoded = autoencoder.get_layer('decoded_lstm3')(decoded)\n",
    "    attention = autoencoder.get_layer('decoded_atten')([decoded, decoded, decoded])\n",
    "    decoded = autoencoder.get_layer('decoded_concat')([decoded, attention])\n",
    "    decoder_output = autoencoder.get_layer('decoded_timeD')(decoded)\n",
    "    # Decoder model\n",
    "    decoder = Model(inputs=encoded_input, outputs=decoder_output, name='decoder')\n",
    "\n",
    "    return autoencoder, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42f53f83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:48.972336Z",
     "iopub.status.busy": "2024-09-26T19:22:48.972089Z",
     "iopub.status.idle": "2024-09-26T19:22:48.983641Z",
     "shell.execute_reply": "2024-09-26T19:22:48.982855Z"
    },
    "papermill": {
     "duration": 0.036269,
     "end_time": "2024-09-26T19:22:48.985457",
     "exception": false,
     "start_time": "2024-09-26T19:22:48.949188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eucl(x, y):\n",
    "    \"\"\"\n",
    "    Euclidean distance between two multivariate time series given as arrays of shape (timesteps, dim)\n",
    "    \"\"\"\n",
    "    d = np.sqrt(np.sum(np.square(x - y), axis=0))\n",
    "    return np.sum(d)\n",
    "\n",
    "\n",
    "def cid(x, y):\n",
    "    \"\"\"\n",
    "    Complexity-Invariant Distance (CID) between two multivariate time series given as arrays of shape (timesteps, dim)\n",
    "    Reference: Batista, Wang & Keogh (2011). A Complexity-Invariant Distance Measure for Time Series. https://doi.org/10.1137/1.9781611972818.60\n",
    "    \"\"\"\n",
    "    assert(len(x.shape) == 2 and x.shape == y.shape)  # time series must have same length and dimensionality\n",
    "    ce_x = np.sqrt(np.sum(np.square(np.diff(x, axis=0)), axis=0) + 1e-9)\n",
    "    ce_y = np.sqrt(np.sum(np.square(np.diff(y, axis=0)), axis=0) + 1e-9)\n",
    "    d = np.sqrt(np.sum(np.square(x - y), axis=0)) * np.divide(np.maximum(ce_x, ce_y), np.minimum(ce_x, ce_y))\n",
    "    return np.sum(d)\n",
    "\n",
    "\n",
    "def cor(x, y):\n",
    "    \"\"\"\n",
    "    Correlation-based distance (COR) between two multivariate time series given as arrays of shape (timesteps, dim)\n",
    "    \"\"\"\n",
    "    scaler = TimeSeriesScalerMeanVariance()\n",
    "    x_norm = scaler.fit_transform(x)\n",
    "    y_norm = scaler.fit_transform(y)\n",
    "    pcc = np.mean(x_norm * y_norm)  # Pearson correlation coefficients\n",
    "    d = np.sqrt(2.0 * (1.0 - pcc + 1e-9))  # correlation-based similarities\n",
    "    return np.sum(d)\n",
    "\n",
    "\n",
    "def acf(x, y):\n",
    "    \"\"\"\n",
    "    Autocorrelation-based distance (ACF) between two multivariate time series given as arrays of shape (timesteps, dim)\n",
    "    Computes a linearly weighted euclidean distance between the autocorrelation coefficients of the input time series.\n",
    "    Reference: Galeano & Pena (2000). Multivariate Analysis in Vector Time Series.\n",
    "    \"\"\"\n",
    "    assert (len(x.shape) == 2 and x.shape == y.shape)  # time series must have same length and dimensionality\n",
    "    x_acf = np.apply_along_axis(lambda z: stattools.acf(z, nlags=z.shape[0]), 0, x)\n",
    "    y_acf = np.apply_along_axis(lambda z: stattools.acf(z, nlags=z.shape[0]), 0, y)\n",
    "    weights = np.linspace(1.0, 0.0, x.shape[0])\n",
    "    d = np.sqrt(np.sum(np.expand_dims(weights, axis=1) * np.square(x_acf - y_acf), axis=0))\n",
    "    return np.sum(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "369eb429",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:49.030975Z",
     "iopub.status.busy": "2024-09-26T19:22:49.030689Z",
     "iopub.status.idle": "2024-09-26T19:22:49.041424Z",
     "shell.execute_reply": "2024-09-26T19:22:49.040659Z"
    },
    "papermill": {
     "duration": 0.035969,
     "end_time": "2024-09-26T19:22:49.043364",
     "exception": false,
     "start_time": "2024-09-26T19:22:49.007395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cluster_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate unsupervised clustering accuracy. Requires scikit-learn installed\n",
    "\n",
    "    # Arguments\n",
    "        y_true: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
    "    return w[row_ind, col_ind].sum() * 1.0 / y_pred.size\n",
    "\n",
    "\n",
    "def cluster_purity(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering purity\n",
    "\n",
    "    # Arguments\n",
    "        y_true: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "\n",
    "    # Return\n",
    "        purity, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    label_mapping = w.argmax(axis=1)\n",
    "    y_pred_voted = y_pred.copy()\n",
    "    for i in range(y_pred.size):\n",
    "        y_pred_voted[i] = label_mapping[y_pred[i]]\n",
    "    return metrics.accuracy_score(y_pred_voted, y_true)\n",
    "\n",
    "\n",
    "def roc_auc(y_true, q_pred, n_classes):\n",
    "    \"\"\"\n",
    "    Calculate area under ROC curve (ROC AUC)\n",
    "    WARNING: DO NOT USE, MAY CONTAIN ERRORS\n",
    "    TODO: CHECK IT!\n",
    "\n",
    "    # Arguments\n",
    "        y_true: true labels, numpy.array with shape `(n_samples,)`\n",
    "        q_pred: predicted probabilities, numpy.array with shape `(n_samples,)`\n",
    "\n",
    "    # Return\n",
    "        ROC AUC score, in [0,1]\n",
    "    \"\"\"\n",
    "    if n_classes == 2:  # binary ROC AUC\n",
    "        auc = max(metrics.roc_auc_score(y_true, q_pred[:, 1]), metrics.roc_auc_score(y_true, q_pred[:, 0]))\n",
    "    else:  # micro-averaged ROC AUC (multiclass)\n",
    "        fpr, tpr, _ = metrics.roc_curve(label_binarize(y_true, classes=np.unique(y_true)).ravel(), q_pred.ravel())\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d9e175e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:49.088708Z",
     "iopub.status.busy": "2024-09-26T19:22:49.088439Z",
     "iopub.status.idle": "2024-09-26T19:22:49.109557Z",
     "shell.execute_reply": "2024-09-26T19:22:49.108704Z"
    },
    "papermill": {
     "duration": 0.046137,
     "end_time": "2024-09-26T19:22:49.111434",
     "exception": false,
     "start_time": "2024-09-26T19:22:49.065297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TSClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, timesteps, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "        dist_metric: distance metric between sequences used in similarity kernel ('eucl', 'cir', 'cor' or 'acf').\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(n_samples, timesteps, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, dist_metric='eucl', **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(TSClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.dist_metric = dist_metric\n",
    "        self.initial_weights = weights\n",
    "        self.clusters = None\n",
    "        self.built = False\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_shape[1]))\n",
    "        self.clusters = self.add_weight(shape=(self.n_clusters, input_shape[1]), initializer='glorot_uniform', name='cluster_centers')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    '''def call(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Student t-distribution kernel, probability of assigning encoded sequence i to cluster k.\n",
    "            q_{ik} = (1 + dist(z_i, m_k)^2)^{-1} / normalization.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: encoded input sequences, shape=(n_samples, timesteps, n_features)\n",
    "        Return:\n",
    "            q: soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        if self.dist_metric == 'eucl':\n",
    "            distance = K.sum(K.sqrt(K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=1)), axis=-1)\n",
    "        elif self.dist_metric == 'cid':\n",
    "            ce_x = K.sqrt(K.sum(K.square(inputs[:, 1:, :] - inputs[:, :-1, :]), axis=1))  # shape (n_samples, n_features)\n",
    "            ce_w = K.sqrt(K.sum(K.square(self.clusters[:, 1:, :] - self.clusters[:, :-1, :]), axis=1))  # shape (n_clusters, n_features)\n",
    "            ce = K.maximum(K.expand_dims(ce_x, axis=1), ce_w) / K.minimum(K.expand_dims(ce_x, axis=1), ce_w)  # shape (n_samples, n_clusters, n_features)\n",
    "            ed = K.sqrt(K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2))  # shape (n_samples, n_clusters, n_features)\n",
    "            distance = K.sum(ed * ce, axis=-1)  # shape (n_samples, n_clusters)\n",
    "        elif self.dist_metric == 'cor':\n",
    "            inputs_norm = (inputs - K.expand_dims(K.mean(inputs, axis=1), axis=1)) / K.expand_dims(K.std(inputs, axis=1), axis=1)  # shape (n_samples, timesteps, n_features)\n",
    "            clusters_norm = (self.clusters - K.expand_dims(K.mean(self.clusters, axis=1), axis=1)) / K.expand_dims(K.std(self.clusters, axis=1), axis=1)  # shape (n_clusters, timesteps, n_features)\n",
    "            pcc = K.mean(K.expand_dims(inputs_norm, axis=1) * clusters_norm, axis=2)  # Pearson correlation coefficients\n",
    "            distance = K.sum(K.sqrt(2.0 * (1.0 - pcc)), axis=-1)  # correlation-based similarities, shape (n_samples, n_clusters)\n",
    "        elif self.dist_metric == 'acf':\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise ValueError('Available distances are eucl, cid, cor and acf!')\n",
    "        q = 1.0 / (1.0 + K.square(distance) / self.alpha)\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q'''\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Student t-distribution kernel, probability of assigning encoded sequence i to cluster k.\n",
    "            q_{ik} = (1 + dist(z_i, m_k)^2)^{-1} / normalization.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: encoded input sequences, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        inputs_expanded = K.expand_dims(inputs, axis=1)  # shape=(n_samples, 1, n_features)\n",
    "        clusters_expanded = K.expand_dims(self.clusters, axis=0)  # shape=(1, n_clusters, n_features)\n",
    "\n",
    "        if self.dist_metric == 'eucl':\n",
    "            distance = K.sqrt(K.sum(K.square(inputs_expanded - clusters_expanded), axis=-1))\n",
    "        elif self.dist_metric == 'cid':\n",
    "            ce_x = K.sqrt(K.sum(K.square(inputs_expanded - K.expand_dims(inputs[:, :-1], axis=2)), axis=-1))  # shape (n_samples, timesteps)\n",
    "            ce_w = K.sqrt(K.sum(K.square(clusters_expanded - K.expand_dims(self.clusters[:, :-1], axis=1)), axis=-1))  # shape (n_clusters, timesteps)\n",
    "            ce = K.maximum(K.expand_dims(ce_x, axis=1), ce_w) / K.minimum(K.expand_dims(ce_x, axis=1), ce_w)  # shape (n_samples, n_clusters, timesteps)\n",
    "            ed = K.sqrt(K.sum(K.square(inputs_expanded - clusters_expanded), axis=-1))  # shape (n_samples, n_clusters)\n",
    "            distance = K.sum(ed * ce, axis=-1)  # shape (n_samples, n_clusters)\n",
    "        elif self.dist_metric == 'cor':\n",
    "            inputs_norm = (inputs - K.mean(inputs, axis=1, keepdims=True)) / K.std(inputs, axis=1, keepdims=True)  # shape (n_samples, n_features)\n",
    "            clusters_norm = (self.clusters - K.mean(self.clusters, axis=1, keepdims=True)) / K.std(self.clusters, axis=1, keepdims=True)  # shape (n_clusters, n_features)\n",
    "            pcc = K.mean(K.expand_dims(inputs_norm, axis=1) * K.expand_dims(clusters_norm, axis=0), axis=-1)  # Pearson correlation coefficients\n",
    "            distance = K.sqrt(2.0 * (1.0 - pcc))  # correlation-based similarities, shape (n_samples, n_clusters)\n",
    "        elif self.dist_metric == 'acf':\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise ValueError('Available distances are eucl, cid, cor and acf!')\n",
    "\n",
    "        q = 1.0 / (1.0 + K.square(distance) / self.alpha)\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 3\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters, 'dist_metric': self.dist_metric}\n",
    "        base_config = super(TSClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe183f20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:49.157004Z",
     "iopub.status.busy": "2024-09-26T19:22:49.156705Z",
     "iopub.status.idle": "2024-09-26T19:22:49.244529Z",
     "shell.execute_reply": "2024-09-26T19:22:49.243711Z"
    },
    "papermill": {
     "duration": 0.112744,
     "end_time": "2024-09-26T19:22:49.246355",
     "exception": false,
     "start_time": "2024-09-26T19:22:49.133611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Series</th>\n",
       "      <th>N</th>\n",
       "      <th>NF</th>\n",
       "      <th>Category</th>\n",
       "      <th>Starting Year</th>\n",
       "      <th>Starting Month</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>...</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N1402</td>\n",
       "      <td>68</td>\n",
       "      <td>18</td>\n",
       "      <td>MICRO</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N1403</td>\n",
       "      <td>68</td>\n",
       "      <td>18</td>\n",
       "      <td>MICRO</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>1920.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N1404</td>\n",
       "      <td>68</td>\n",
       "      <td>18</td>\n",
       "      <td>MICRO</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>4860.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N1405</td>\n",
       "      <td>68</td>\n",
       "      <td>18</td>\n",
       "      <td>MICRO</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>180.0</td>\n",
       "      <td>940.0</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N1406</td>\n",
       "      <td>68</td>\n",
       "      <td>18</td>\n",
       "      <td>MICRO</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1550.0</td>\n",
       "      <td>4450.0</td>\n",
       "      <td>3050.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Series   N  NF      Category  Starting Year  Starting Month       1       2  \\\n",
       "0  N1402  68  18  MICRO                  1990               1  2640.0  2640.0   \n",
       "1  N1403  68  18  MICRO                  1990               1  1680.0  1920.0   \n",
       "2  N1404  68  18  MICRO                  1990               1  1140.0   720.0   \n",
       "3  N1405  68  18  MICRO                  1990               1   180.0   940.0   \n",
       "4  N1406  68  18  MICRO                  1990               1  2000.0  1550.0   \n",
       "\n",
       "        3       4  ...  135  136  137  138  139  140  141  142  143  144  \n",
       "0  2160.0  4200.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1   120.0  1080.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "2  4860.0  1200.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "3  2040.0   800.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "4  4450.0  3050.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 150 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3 = pd.read_csv(\"/kaggle/input/m3monthdataset/M3Month.csv\")\n",
    "m3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc635cdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:49.292653Z",
     "iopub.status.busy": "2024-09-26T19:22:49.292371Z",
     "iopub.status.idle": "2024-09-26T19:22:49.307581Z",
     "shell.execute_reply": "2024-09-26T19:22:49.306706Z"
    },
    "papermill": {
     "duration": 0.04074,
     "end_time": "2024-09-26T19:22:49.309597",
     "exception": false,
     "start_time": "2024-09-26T19:22:49.268857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MICRO       \n",
      "INDUSTRY    \n",
      "MACRO       \n",
      "FINANCE     \n",
      "DEMOGRAPHIC \n",
      "OTHER       \n"
     ]
    }
   ],
   "source": [
    "class_dataframes = {}  # A dictionary to store DataFrames for each class\n",
    "\n",
    "# Iterate over unique class labels in the 'category' column\n",
    "for class_label in m3['Category'].unique():\n",
    "    print(class_label)\n",
    "    # Filter the original DataFrame for rows with the current class label\n",
    "    class_df = m3[m3['Category'] == class_label]\n",
    "    class_label = class_label.replace(\" \", \"\")\n",
    "    # Store the class-specific DataFrame in the dictionary with the class label as the key\n",
    "    class_dataframes[class_label] = class_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8576e7d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:49.355914Z",
     "iopub.status.busy": "2024-09-26T19:22:49.355609Z",
     "iopub.status.idle": "2024-09-26T19:22:49.361234Z",
     "shell.execute_reply": "2024-09-26T19:22:49.360394Z"
    },
    "papermill": {
     "duration": 0.031105,
     "end_time": "2024-09-26T19:22:49.363244",
     "exception": false,
     "start_time": "2024-09-26T19:22:49.332139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    ts_train = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        temp=np.array(list(data.iloc[i][6:].dropna()))\n",
    "        temp = temp[:-18]\n",
    "        temp=temp.reshape(1,len(temp),1)\n",
    "        temp2 = TimeSeriesScalerMeanVariance().fit_transform(temp)\n",
    "        ts_train.append(temp2.reshape(-1,1))\n",
    "\n",
    "    return ts_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "408c5dbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:49.410589Z",
     "iopub.status.busy": "2024-09-26T19:22:49.410303Z",
     "iopub.status.idle": "2024-09-26T19:22:49.501470Z",
     "shell.execute_reply": "2024-09-26T19:22:49.500762Z"
    },
    "papermill": {
     "duration": 0.117013,
     "end_time": "2024-09-26T19:22:49.503398",
     "exception": false,
     "start_time": "2024-09-26T19:22:49.386385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DTC:\n",
    "    def __init__(self, n_clusters, input_dim, timesteps,max_dim=None,dataset_name=None,\n",
    "                 alpha=1.0, dist_metric='eucl', cluster_init='kmeans', heatmap=False):\n",
    "        self.n_clusters = 2\n",
    "        self.input_dim = None\n",
    "        self.max_dim=max_dim\n",
    "        self.dataset_name=dataset_name\n",
    "        self.timesteps = 791\n",
    "        self.latent_shape = 16\n",
    "        self.alpha = alpha\n",
    "        self.dist_metric = dist_metric\n",
    "        self.cluster_init = cluster_init\n",
    "        self.heatmap = heatmap\n",
    "        self.pretrained = False\n",
    "        self.alpha2 = 0.8\n",
    "        self.learning_rate = None\n",
    "        self.optimizer = keras.optimizers.Adam()\n",
    "        self.model = self.autoencoder = self.encoder = self.decoder = self.predmodel =  None\n",
    "        self.autoencoderNeighbour = self.encoderNeighbour = self.decoderNeighbour =  None\n",
    "        self.FeatureWeight = None\n",
    "        self.a = 1.0\n",
    "        self.b = 1.0\n",
    "        self.initial_weights = None\n",
    "        self.a_b = [1.93, 0.79]\n",
    "        self.KNN = None\n",
    "        self.b1 = -0.9\n",
    "        self.b2 = self.b1/2\n",
    "\n",
    "        \n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Create DTC model\n",
    "        \"\"\"\n",
    "        # Create AE models\n",
    "        self.FeatureWeight = np.ones((self.n_clusters, self.latent_shape),dtype='float32')/self.latent_shape\n",
    "        self.autoencoder, self.encoder, self.decoder = autoencoder(series_len=self.max_dim)\n",
    "        self.autoencoderNeighbour , self.encoderNeighbour , self.decoderNeighbour = autoencoderNeighbour(series_len=self.max_dim)\n",
    "        clustering_layer = TSClusteringLayer(self.n_clusters,\n",
    "                                             alpha=self.alpha,\n",
    "                                             dist_metric=self.dist_metric,\n",
    "                                             name='TSClustering')(self.encoderNeighbour.output)\n",
    "        # Create DTC model\n",
    "        self.model = Model(inputs=[self.autoencoderNeighbour.input],\n",
    "                               outputs=[self.autoencoderNeighbour.output, clustering_layer])\n",
    "        self.KNN = NearestNeighbors(n_neighbors=3,metric='euclidean')\n",
    "\n",
    "    @property\n",
    "    def cluster_centers_(self):\n",
    "        \"\"\"\n",
    "        Returns cluster centers\n",
    "        \"\"\"\n",
    "        return self.model.get_layer(name='TSClustering').get_weights()[0]\n",
    "    \n",
    "    def reconstruction_loss(self, data, reconstruction):\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.mean_squared_error(data, reconstruction), axis=(0, 1)\n",
    "                )\n",
    "            )\n",
    "        return reconstruction_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def weighted_kld(loss_weight):\n",
    "        \"\"\"\n",
    "        Custom KL-divergence loss with a variable weight parameter\n",
    "        \"\"\"\n",
    "        def loss(y_true, y_pred):\n",
    "            return loss_weight * kullback_leibler_divergence(y_true, y_pred)\n",
    "        return loss\n",
    "    \n",
    "    def loss_kld(self, y_true, y_pred):\n",
    "        return kullback_leibler_divergence(y_true, y_pred)\n",
    "    \n",
    "    \n",
    "    def loss_kld(self, y_true, y_pred):\n",
    "        return kullback_leibler_divergence(y_true, y_pred)\n",
    "    \n",
    "    def categorical_cross_entropy(self, y_true, y_pred):\n",
    "        cce = CategoricalCrossentropy()\n",
    "        return cce(y_true, y_pred)\n",
    "        \n",
    "    def on_epoch_end(self, epoch):\n",
    "        \"\"\"\n",
    "        Update heatmap loss weight on epoch end\n",
    "        \"\"\"\n",
    "        if epoch > self.finetune_heatmap_at_epoch:\n",
    "            K.set_value(self.heatmap_loss_weight, self.final_heatmap_loss_weight)\n",
    "\n",
    "    def compile(self, gamma, optimizer, initial_heatmap_loss_weight=None, final_heatmap_loss_weight=None):\n",
    "        \"\"\"\n",
    "        Compile DTC model\n",
    "\n",
    "        # Arguments\n",
    "            gamma: coefficient of TS clustering loss\n",
    "            optimizer: optimization algorithm\n",
    "            initial_heatmap_loss_weight (optional): initial weight of heatmap loss vs clustering loss\n",
    "            final_heatmap_loss_weight (optional): final weight of heatmap loss vs clustering loss (heatmap finetuning)\n",
    "        \"\"\"\n",
    "        if self.heatmap:\n",
    "            self.initial_heatmap_loss_weight = initial_heatmap_loss_weight\n",
    "            self.final_heatmap_loss_weight = final_heatmap_loss_weight\n",
    "            self.heatmap_loss_weight = K.variable(self.initial_heatmap_loss_weight)\n",
    "            self.model.compile(loss=['mse', DTC.weighted_kld(1.0 - self.heatmap_loss_weight), DTC.weighted_kld(self.heatmap_loss_weight)],\n",
    "                               loss_weights=[1.0, gamma, gamma],\n",
    "                               optimizer=optimizer)\n",
    "        else:\n",
    "            self.model.compile(loss=['mse', 'CategoricalCrossentropy'],\n",
    "                               loss_weights=[1.0, gamma],\n",
    "                               optimizer=optimizer)\n",
    "\n",
    "    def load_weights(self, weights_path):\n",
    "        \"\"\"\n",
    "        Load pre-trained weights of DTC model\n",
    "\n",
    "        # Arguments\n",
    "            weight_path: path to weights file (.h5)\n",
    "        \"\"\"\n",
    "        self.model.load_weights(weights_path)\n",
    "        self.pretrained = True\n",
    "\n",
    "    def load_ae_weights(self, ae_weights_path):\n",
    "        \"\"\"\n",
    "        Load pre-trained weights of AE\n",
    "\n",
    "        # Arguments\n",
    "            ae_weight_path: path to weights file (.h5)\n",
    "        \"\"\"\n",
    "        self.autoencoder.load_weights(ae_weights_path)\n",
    "        self.pretrained = True\n",
    "        \n",
    "    def load_ae_neighbour_weights(self):\n",
    "        input = self.autoencoder.get_layer(name='input').get_weights()\n",
    "        encoder_0 = self.autoencoder.get_layer(name='encoded_lstm1').get_weights()\n",
    "        encoder_1 = self.autoencoder.get_layer(name='encoded_lstm2').get_weights()\n",
    "        encoder_2 = self.autoencoder.get_layer(name='encoded_lstm3').get_weights()\n",
    "        atten_encode = self.autoencoder.get_layer(name='encoded_atten').get_weights()\n",
    "        dense_encode = self.autoencoder.get_layer(name=\"encoded_dense\").get_weights()\n",
    "        decoder_0 = self.autoencoder.get_layer(name='decoded_lstm1').get_weights()\n",
    "        decoder_1 = self.autoencoder.get_layer(name='decoded_lstm2').get_weights()\n",
    "        decoder_2 = self.autoencoder.get_layer(name='decoded_lstm3').get_weights()\n",
    "        decoder_atten = self.autoencoder.get_layer(name='decoded_atten').get_weights()\n",
    "        decoder_timeD = self.autoencoder.get_layer(name=\"decoded_timeD\").get_weights()\n",
    "\n",
    "        # Image weight\n",
    "        self.autoencoderNeighbour.get_layer(name='input').set_weights(input)\n",
    "        self.autoencoderNeighbour.get_layer(name='lstm_input_256').set_weights(encoder_0)\n",
    "        self.autoencoderNeighbour.get_layer(name='lstm_input_128').set_weights(encoder_1)\n",
    "        self.autoencoderNeighbour.get_layer(name='lstm_input_64').set_weights(encoder_2)\n",
    "        self.autoencoderNeighbour.get_layer(name='attention_input').set_weights(atten_encode)\n",
    "        self.autoencoderNeighbour.get_layer(name='dense_input').set_weights(dense_encode)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        self.autoencoderNeighbour.get_layer(name='decoded_lstm1').set_weights(decoder_0)\n",
    "        self.autoencoderNeighbour.get_layer(name='decoded_lstm2').set_weights(decoder_1)\n",
    "        self.autoencoderNeighbour.get_layer(name='decoded_lstm3').set_weights(decoder_2)\n",
    "        self.autoencoderNeighbour.get_layer(name='decoded_atten').set_weights(decoder_atten)\n",
    "\n",
    "        self.autoencoderNeighbour.get_layer(name='lstm_Neighbour1_256').set_weights(encoder_0)\n",
    "        self.autoencoderNeighbour.get_layer(name='lstm_Neighbour1_128').set_weights(encoder_1)\n",
    "        self.autoencoderNeighbour.get_layer(name='lstm_Neighbour1_64').set_weights(encoder_2)\n",
    "        self.autoencoderNeighbour.get_layer(name='attention_Neighbour1').set_weights(decoder_atten)\n",
    "        \n",
    "        self.autoencoderNeighbour.get_layer(name='lstm_Neighbour2_256').set_weights(encoder_0)\n",
    "        self.autoencoderNeighbour.get_layer(name='lstm_Neighbour2_128').set_weights(encoder_1)\n",
    "        self.autoencoderNeighbour.get_layer(name='lstm_Neighbour2_64').set_weights(encoder_2)\n",
    "        self.autoencoderNeighbour.get_layer(name='attention_Neighbour2').set_weights(decoder_atten)\n",
    "\n",
    "        self.autoencoderNeighbour.get_layer(name='Neighbour1_input').set_weights(input)\n",
    "        self.autoencoderNeighbour.get_layer(name='Neighbour2_input').set_weights(input)\n",
    "\n",
    "        # Image weight (encoder)\n",
    "        self.encoderNeighbour.get_layer(name='input').set_weights(input)\n",
    "        self.encoderNeighbour.get_layer(name='lstm_input_256').set_weights(encoder_0)\n",
    "        self.encoderNeighbour.get_layer(name='lstm_input_128').set_weights(encoder_1)\n",
    "        self.encoderNeighbour.get_layer(name='lstm_input_64').set_weights(encoder_2)\n",
    "        self.encoderNeighbour.get_layer(name='attention_input').set_weights(atten_encode)\n",
    "        \n",
    "        self.encoderNeighbour.get_layer(name='lstm_Neighbour1_256').set_weights(encoder_0)\n",
    "        self.encoderNeighbour.get_layer(name='lstm_Neighbour1_128').set_weights(encoder_1)\n",
    "        self.encoderNeighbour.get_layer(name='lstm_Neighbour1_64').set_weights(encoder_2)\n",
    "        self.encoderNeighbour.get_layer(name='attention_Neighbour1').set_weights(decoder_atten)\n",
    "        \n",
    "        self.encoderNeighbour.get_layer(name='lstm_Neighbour2_256').set_weights(encoder_0)\n",
    "        self.encoderNeighbour.get_layer(name='lstm_Neighbour2_128').set_weights(encoder_1)\n",
    "        self.encoderNeighbour.get_layer(name='lstm_Neighbour2_64').set_weights(encoder_2)\n",
    "        self.encoderNeighbour.get_layer(name='attention_Neighbour2').set_weights(decoder_atten)\n",
    "\n",
    "        self.encoderNeighbour.get_layer(name='Neighbour1_input').set_weights(input)\n",
    "        self.encoderNeighbour.get_layer(name='Neighbour2_input').set_weights(input)\n",
    "\n",
    "    def dist(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Compute distance between two multivariate time series using chosen distance metric\n",
    "\n",
    "        # Arguments\n",
    "            x1: first input (np array)\n",
    "            x2: second input (np array)\n",
    "        # Return\n",
    "            distance\n",
    "        \"\"\"\n",
    "        if self.dist_metric == 'eucl':\n",
    "            return tsdistances.eucl(x1, x2)\n",
    "        elif self.dist_metric == 'cid':\n",
    "            return tsdistances.cid(x1, x2)\n",
    "        elif self.dist_metric == 'cor':\n",
    "            return tsdistances.cor(x1, x2)\n",
    "        elif self.dist_metric == 'acf':\n",
    "            return tsdistances.acf(x1, x2)\n",
    "        else:\n",
    "            raise ValueError('Available distances are eucl, cid, cor and acf!')\n",
    "\n",
    "    def init_cluster_weights(self, X):\n",
    "        \"\"\"\n",
    "        Initialize with complete-linkage hierarchical clustering or k-means.\n",
    "\n",
    "        # Arguments\n",
    "            X: numpy array containing training set or batch\n",
    "        \"\"\"\n",
    "        assert(self.cluster_init in ['hierarchical', 'kmeans'])\n",
    "        print('Initializing cluster...')\n",
    "\n",
    "        features = self.encode(X)\n",
    "\n",
    "        if self.cluster_init == 'hierarchical':\n",
    "            if self.dist_metric == 'eucl':  # use AgglomerativeClustering off-the-shelf\n",
    "                hc = AgglomerativeClustering(n_clusters=self.n_clusters,\n",
    "                                             affinity='euclidean',\n",
    "                                             linkage='complete').fit(features.reshape(features.shape[0], -1))\n",
    "            else:  # compute distance matrix using dist\n",
    "                d = np.zeros((features.shape[0], features.shape[0]))\n",
    "                for i in range(features.shape[0]):\n",
    "                    for j in range(i):\n",
    "                        d[i, j] = d[j, i] = self.dist(features[i], features[j])\n",
    "                hc = AgglomerativeClustering(n_clusters=self.n_clusters,\n",
    "                                             affinity='precomputed',\n",
    "                                             linkage='complete').fit(d)\n",
    "            # compute centroid\n",
    "            cluster_centers = np.array([features[hc.labels_ == c].mean(axis=0) for c in range(self.n_clusters)])\n",
    "        elif self.cluster_init == 'kmeans':\n",
    "            # fit k-means on flattened features\n",
    "            km = KMeans(n_clusters=self.n_clusters, n_init=10).fit(features.reshape(features.shape[0], -1))\n",
    "            cluster_centers = km.cluster_centers_.reshape(self.n_clusters, features.shape[1])\n",
    "\n",
    "        self.model.get_layer(name='TSClustering').set_weights([cluster_centers])\n",
    "        print('Done!')\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encoding function. Extract latent features from hidden layer\n",
    "\n",
    "        # Arguments\n",
    "            x: data point\n",
    "        # Return\n",
    "            encoded (latent) data point\n",
    "        \"\"\"\n",
    "        return self.encoder.predict(x)\n",
    "\n",
    "    def decode(self, x):\n",
    "        \"\"\"\n",
    "        Decoding function. Decodes encoded sequence from latent space.\n",
    "\n",
    "        # Arguments\n",
    "            x: encoded (latent) data point\n",
    "        # Return\n",
    "            decoded data point\n",
    "        \"\"\"\n",
    "        return self.decoder.predict(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict cluster assignment.\n",
    "\n",
    "        \"\"\"\n",
    "        q = self.model.predict(x, verbose=0)[1]\n",
    "        return q.argmax(axis=1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def target_distribution(q):  # target distribution p which enhances the discrimination of soft label q\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def predict_heatmap(self, x):\n",
    "        \"\"\"\n",
    "        Produces TS clustering heatmap from input sequence.\n",
    "\n",
    "        # Arguments\n",
    "            x: data point\n",
    "        # Return\n",
    "            heatmap\n",
    "        \"\"\"\n",
    "        return self.heatmap_model.predict(x, verbose=0)\n",
    "\n",
    "    def pretrain(self, X,\n",
    "                 optimizer='adam',\n",
    "                 epochs=10,\n",
    "                 batch_size=64,\n",
    "                 save_dir='results/tmp',\n",
    "                 verbose=1):\n",
    "        \"\"\"\n",
    "        Pre-train the autoencoder using only MSE reconstruction loss\n",
    "        Saves weights in h5 format.\n",
    "\n",
    "        # Arguments\n",
    "            X: training set\n",
    "            optimizer: optimization algorithm\n",
    "            epochs: number of pre-training epochs\n",
    "            batch_size: training batch size\n",
    "            save_dir: path to existing directory where weights will be saved\n",
    "        \"\"\"\n",
    "        print('Pretraining...')\n",
    "        self.autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        # Begin pretraining\n",
    "        #t0 = time()\n",
    "        self.autoencoder.fit(X, X, batch_size=batch_size, epochs=epochs)\n",
    "        #print('Pretraining time: ', time() - t0)\n",
    "        self.autoencoder.save_weights('{}/ae_weights-epoch{}.h5'.format(save_dir, epochs))\n",
    "        print('Pretrained weights are saved to {}/ae_weights-epoch{}.h5'.format(save_dir, epochs))\n",
    "        self.pretrained = True\n",
    "        \n",
    "    def mutate_clustering(self, clustering, mutation_rate=0.3):\n",
    "        new_clustering = clustering.copy()\n",
    "        n_samples = len(clustering)\n",
    "        for i in range(n_samples):\n",
    "            if np.random.rand() < mutation_rate:\n",
    "                new_clustering[i] = 1 - new_clustering[i]\n",
    "        return new_clustering\n",
    "\n",
    "    def crossover_clustering(self, clustering1, clustering2, crossover_rate=0.3):\n",
    "        n_samples = len(clustering1)\n",
    "        new_clustering = clustering1.copy()\n",
    "        for i in range(n_samples):\n",
    "            if np.random.rand() < crossover_rate:\n",
    "                new_clustering[i] = clustering2[i]\n",
    "        return new_clustering\n",
    "    \n",
    "    def save_metrics(self, metrics, file_path='metrics.csv'):\n",
    "        \"\"\"\n",
    "        Save training metrics to a CSV file.\n",
    "\n",
    "        # Arguments\n",
    "            metrics: List of dictionaries containing metrics for each epoch\n",
    "            file_path: Path to the CSV file where metrics will be saved\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(metrics)\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f'Metrics saved to {file_path}')\n",
    "    \n",
    "    def fit(self, X_train, y_train=None, X_val=None, y_val=None,\n",
    "        epochs=10,\n",
    "        eval_epochs=10,\n",
    "        save_epochs=10,\n",
    "        batch_size=64,\n",
    "        tol=0.001,\n",
    "        patience=5,\n",
    "        finetune_heatmap_at_epoch=8,\n",
    "        save_dir='results/tmp',\n",
    "        mutation_rate=0.01,\n",
    "        crossover_rate=0.5,\n",
    "        num_iterations=5):\n",
    "        \n",
    "    \n",
    "            if not self.pretrained:\n",
    "                print('Autoencoder was not pre-trained!')\n",
    "\n",
    "            calculations_method = 'per_series' # single_value | per_series\n",
    "\n",
    "            y_pred_last = None\n",
    "            patience_cnt = 0\n",
    "            index = 0\n",
    "            best = []\n",
    "            metrics = []\n",
    "            print('Training for {} epochs.\\nEvaluating every {} and saving model every {} epochs.'.format(epochs, eval_epochs, save_epochs))\n",
    "            best_val_error = float('inf')\n",
    "            index_array = np.arange(X_train.shape[0])\n",
    "            best_clustering = None\n",
    "            for epoch in range(epochs):\n",
    "                dataset, features, lag, look_forward, sample_overlap, learning_rate, dataset_path, suilin_smape, frequency = get_dataset_params(self.dataset_name)\n",
    "                dataset, data_means = normalize_dataset(dataset, look_forward=look_forward)\n",
    "                dataset, seasonal, trend = stl_decomposition2(dataset, frequency, look_forward)\n",
    "                dataset = np.array(dataset)\n",
    "                # Initial clustering\n",
    "                if epoch == 0:\n",
    "                    q = self.model.predict([X_train, X_train, X_train])[1]\n",
    "                    extract = Model(inputs = self.model.input, outputs = self.model.get_layer('add_inputs').output)\n",
    "                    z = extract.predict([X_train, X_train, X_train])\n",
    "                else:\n",
    "                    self.KNN.fit(z)\n",
    "                    _,Neighbors_list=self.KNN.kneighbors(z)\n",
    "                    q = self.model.predict([X_train,X_train[Neighbors_list[:,1]], X_train[Neighbors_list[:,2]]])[1]\n",
    "\n",
    "                p = DTC.target_distribution(q)\n",
    "                p_pred = p.argmax(axis=1)\n",
    "                new_clustering = p_pred.copy()\n",
    "                reliableindex, _ = np.unique(np.unique(np.where(np.sort(q,axis=1)[:,-1] - np.sort(q,axis=1)[:,-2]>=self.b2), np.where(np.max(q,axis=1)>=self.b1)),\n",
    "                           np.unique(np.where(np.sort(p,axis=1)[:,-1] - np.sort(p,axis=1)[:,-2]>=self.b2), np.where(np.max(p,axis=1)>=self.b1)))\n",
    "\n",
    "                print('Number of reliable samples:', len(q[reliableindex]) )\n",
    "                print('Number of unreliable samples:', len(q)-len(q[reliableindex]) )\n",
    "#                 print(\"my\",new_clustering)\n",
    "                Number_Unreliable = len(q)-len(q[reliableindex])\n",
    "                Number_reliable = len(q[reliableindex])\n",
    "                for iteration in range(num_iterations):  # Number of mutation and crossover iterations\n",
    "                    testing_RMSE = np.array([])\n",
    "                    validation_RMSE = np.array([])\n",
    "                    testing_SMAPE = np.array([])\n",
    "                    validation_SMAPE = np.array([])\n",
    "                    validation_loss_update = []\n",
    "                    for cluster_label in range(self.n_clusters):\n",
    "                        idx = [x for x in range(len(p_pred)) if new_clustering[x] == cluster_label]\n",
    "                        cluster_dataset = np.array(dataset)[idx]\n",
    "                        cluster_dataset_means = data_means[idx]\n",
    "                        cluster_dataset_seasonal = seasonal[idx]\n",
    "                        self.predmodel = Prediction_Model(lag, look_forward, learning_rate, look_forward)\n",
    "\n",
    "                        look_back = lag\n",
    "                        calculations_method = 'per_series' \n",
    "\n",
    "                        trainX, valX, testX, trainY, valY, testY, test_means, val_means, val_seasonal, test_seasonal, test_seasonal2 = all_pre_process_aug(cluster_dataset, lag, look_forward, sample_overlap, cluster_dataset_means, cluster_dataset_seasonal, frequency, dataset_name =self.dataset_name)\n",
    "\n",
    "                        history = self.predmodel.fit([trainX], trainY, validation_data=([valX, valY]),\n",
    "                                                     epochs=10,\n",
    "                                                     verbose=0,\n",
    "                                                     batch_size=10).history\n",
    "\n",
    "                        val_prediction_results = self.predmodel.predict([valX], batch_size=16, verbose=0)\n",
    "                        val_RMSE = root_mean_squared_error(valY, val_prediction_results, calculations_method)\n",
    "                        val_SMAPE = smape(valY, val_prediction_results, calculations_method, suilin_smape)\n",
    "\n",
    "                        test_prediction_results = self.predmodel.predict([testX], batch_size=16, verbose=0)\n",
    "                        test_RMSE = root_mean_squared_error(testY, test_prediction_results, calculations_method)\n",
    "                        test_SMAPE = smape(testY, test_prediction_results, calculations_method, suilin_smape)\n",
    "\n",
    "                        validation_loss_update.extend(val_RMSE)\n",
    "\n",
    "                        rescaled_valY = rescale_data_to_main_value(valY, val_means, val_seasonal)\n",
    "                        rescaled_val_prediction_results = rescale_data_to_main_value(val_prediction_results, val_means, val_seasonal)\n",
    "                        val_SMAPE = smape(rescaled_valY, rescaled_val_prediction_results, calculations_method, suilin_smape)\n",
    "                        val_RMSE = root_mean_squared_error(rescaled_valY, rescaled_val_prediction_results, calculations_method)\n",
    "\n",
    "                        rescaled_testY = rescale_data_to_main_value(testY, test_means, test_seasonal)\n",
    "                        rescaled_test_prediction_results = rescale_data_to_main_value(test_prediction_results, test_means, test_seasonal2)\n",
    "                        test_SMAPE = smape(rescaled_testY, rescaled_test_prediction_results, calculations_method)\n",
    "                        test_RMSE = root_mean_squared_error(rescaled_testY, rescaled_test_prediction_results, calculations_method)\n",
    "                        \n",
    "                        validation_RMSE = np.concatenate((validation_RMSE, val_RMSE))\n",
    "                        testing_RMSE = np.concatenate((testing_RMSE, test_RMSE))\n",
    "                        validation_SMAPE = np.concatenate((validation_SMAPE, val_SMAPE))\n",
    "                        testing_SMAPE = np.concatenate((testing_SMAPE, test_SMAPE))\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                    validation_loss_update = np.array(validation_loss_update).astype(np.float32)\n",
    "\n",
    "                    total_val_error = np.mean(validation_loss_update)\n",
    "                    if np.mean(validation_SMAPE) < best_val_error:\n",
    "                        best_val_error = np.mean(validation_SMAPE)\n",
    "                        best_clustering = new_clustering\n",
    "                        tensor_best_clustering = tf.convert_to_tensor(new_clustering, dtype=tf.float32) \n",
    "                        best.append(best_clustering)\n",
    "                        print(\"list: \", best)\n",
    "                        print(\"Best Clusters: \", best_clustering)\n",
    "                        print(\"Mean Validation SMAPE: \", np.mean(validation_SMAPE))\n",
    "                        print(\"Mean Test SMAPE: \", np.mean(testing_SMAPE))\n",
    "                        print(\"Mean Validation RMSE: \", np.mean(validation_RMSE))\n",
    "                        print(\"Mean Test RMSE: \", np.mean(testing_RMSE))\n",
    "                        print(\"Median Validation SMAPE: \", np.median(validation_SMAPE))\n",
    "                        print(\"Median Test SMAPE: \", np.median(testing_SMAPE))\n",
    "                        print(\"Median Validation RMSE: \", np.median(validation_RMSE))\n",
    "                        print(\"Median Test RMSE: \", np.median(testing_RMSE))\n",
    "                        # Record metrics for this epoch\n",
    "                        metrics.append({\n",
    "                            'Epoch': epoch + 1,\n",
    "                            'Mean Validation SMAPE': np.mean(validation_SMAPE),\n",
    "                            'Mean Test SMAPE': np.mean(testing_SMAPE),\n",
    "                            'Mean Validation RMSE': np.mean(validation_RMSE),\n",
    "                            'Mean Test RMSE': np.mean(testing_RMSE),\n",
    "                            'Median Validation SMAPE': np.median(validation_SMAPE),\n",
    "                            'Median Test SMAPE': np.median(testing_SMAPE),\n",
    "                            'Median Validation RMSE': np.median(validation_RMSE),\n",
    "                            'Median Test RMSE': np.median(testing_RMSE),\n",
    "                        })\n",
    "                        # Custom training step\n",
    "                        with tf.GradientTape() as tape:\n",
    "                            idx = index_array[index:reliableindex.shape[0]]\n",
    "                            self.KNN.fit(z[reliableindex[idx]])\n",
    "                            _,Neighbors_list=self.KNN.kneighbors(z[reliableindex[idx]])\n",
    "                            outputs = self.model([X_train[reliableindex[idx]], X_train[Neighbors_list[:,1]], X_train[Neighbors_list[:,2]]], training=True)\n",
    "                            reconstruction_output, clustering_output = outputs\n",
    "                            if iteration == 0:\n",
    "                                reconstruction_loss = self.reconstruction_loss(X_train[reliableindex[idx]], reconstruction_output)\n",
    "                                clustering_loss = self.categorical_cross_entropy(p[reliableindex[idx]], clustering_output)\n",
    "                            else:\n",
    "                                clustering_output = self.model.predict([X_train[reliableindex[idx]], X_train[Neighbors_list[:,1]], X_train[Neighbors_list[:,2]]])[1]\n",
    "                                clustering_output = DTC.target_distribution(clustering_output)\n",
    "                                clustering_output = clustering_output.argmax(axis=1)\n",
    "                                clustering_output = tf.convert_to_tensor(clustering_output, dtype=tf.float32)\n",
    "                                reconstruction_loss = self.reconstruction_loss(X_train[reliableindex[idx]], reconstruction_output)\n",
    "                                clustering_loss = self.categorical_cross_entropy(tensor_best_clustering, clustering_output)\n",
    "\n",
    "                            validation_loss_update = np.array(validation_loss_update).astype(np.float32)\n",
    "                            total_loss = ((1 - self.alpha2) * (tf.reduce_sum(reconstruction_loss) + tf.reduce_sum(clustering_loss))) + (self.alpha2 * tf.reduce_sum(validation_loss_update))\n",
    "                            total_loss = tf.reduce_sum(total_loss)\n",
    "                            print(\"reconstruction_loss: \",tf.reduce_sum(reconstruction_loss))\n",
    "                            print(\"clustering_loss: \",tf.reduce_sum(clustering_loss))\n",
    "                            print(\"validation_loss: \",tf.reduce_sum(validation_loss_update))\n",
    "\n",
    "                            \n",
    "                        grads = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "                        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "                    new_clustering = self.mutate_clustering(best_clustering)\n",
    "                    new_clustering = self.crossover_clustering(new_clustering, p_pred)\n",
    "                    print(\"Modified Clustering: \", new_clustering)\n",
    "                num_elements = len(reliableindex[idx])\n",
    "                n1 = Neighbors_list[:,1]\n",
    "                n1 = n1[:num_elements]\n",
    "                n2 = Neighbors_list[:,2]\n",
    "                n2 = n2[:num_elements]\n",
    "                z[idx] = extract.predict([X_train[reliableindex[idx]], X_train[n1], X_train[n2]])\n",
    "                print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "                print(\"Mean Validation SMAPE: \", np.mean(validation_SMAPE))\n",
    "                print(\"Mean Test SMAPE: \", np.mean(testing_SMAPE))\n",
    "                print(\"Mean Validation RMSE: \", np.mean(validation_RMSE))\n",
    "                print(\"Mean Test RMSE: \", np.mean(testing_RMSE))\n",
    "                print(\"Median Validation SMAPE: \", np.median(validation_SMAPE))\n",
    "                print(\"Median Test SMAPE: \", np.median(testing_SMAPE))\n",
    "                print(\"Median Validation RMSE: \", np.median(validation_RMSE))\n",
    "                print(\"Median Test RMSE: \", np.median(testing_RMSE))\n",
    "                print(\"Total Loss: \", total_loss)\n",
    "            self.save_metrics(metrics, file_path='metrics.csv')\n",
    "            np.save('BestCluster.npy', np.array(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0408fe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:49.550709Z",
     "iopub.status.busy": "2024-09-26T19:22:49.550447Z",
     "iopub.status.idle": "2024-09-26T19:22:51.264505Z",
     "shell.execute_reply": "2024-09-26T19:22:51.263747Z"
    },
    "papermill": {
     "duration": 1.740914,
     "end_time": "2024-09-26T19:22:51.267124",
     "exception": false,
     "start_time": "2024-09-26T19:22:49.526210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_MICRO = preprocessing(class_dataframes['MICRO'])\n",
    "dataset_INDUSTRY = preprocessing(class_dataframes['INDUSTRY'])\n",
    "dataset_MACRO = preprocessing(class_dataframes['MACRO'])\n",
    "dataset_FINANCE = preprocessing(class_dataframes['FINANCE'])\n",
    "dataset_DEMOGRAPHIC = preprocessing(class_dataframes['DEMOGRAPHIC'])\n",
    "dataset_OTHER = preprocessing(class_dataframes['OTHER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4e0e511",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:51.314438Z",
     "iopub.status.busy": "2024-09-26T19:22:51.314143Z",
     "iopub.status.idle": "2024-09-26T19:22:51.320211Z",
     "shell.execute_reply": "2024-09-26T19:22:51.319299Z"
    },
    "papermill": {
     "duration": 0.031456,
     "end_time": "2024-09-26T19:22:51.322111",
     "exception": false,
     "start_time": "2024-09-26T19:22:51.290655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_seq_length_MICRO = max(len(seq) for seq in dataset_MICRO)\n",
    "max_seq_length_INDUSTRY = max(len(seq) for seq in dataset_INDUSTRY)\n",
    "max_seq_length_MACRO = max(len(seq) for seq in dataset_MACRO)\n",
    "max_seq_length_FINANCE = max(len(seq) for seq in dataset_FINANCE)\n",
    "max_seq_length_DEMOGRAPHIC = max(len(seq) for seq in dataset_DEMOGRAPHIC)\n",
    "max_seq_length_OTHER = max(len(seq) for seq in dataset_OTHER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5b188da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:51.369977Z",
     "iopub.status.busy": "2024-09-26T19:22:51.369628Z",
     "iopub.status.idle": "2024-09-26T19:22:51.383893Z",
     "shell.execute_reply": "2024-09-26T19:22:51.383023Z"
    },
    "papermill": {
     "duration": 0.0406,
     "end_time": "2024-09-26T19:22:51.385809",
     "exception": false,
     "start_time": "2024-09-26T19:22:51.345209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "padded_sequences_MICRO = tf.keras.preprocessing.sequence.pad_sequences(dataset_MICRO, maxlen=max_seq_length_MICRO, padding='post', dtype='float32')\n",
    "padded_sequences_INDUSTRY = tf.keras.preprocessing.sequence.pad_sequences(dataset_INDUSTRY, maxlen=max_seq_length_INDUSTRY, padding='post', dtype='float32')\n",
    "padded_sequences_MACRO = tf.keras.preprocessing.sequence.pad_sequences(dataset_MACRO, maxlen=max_seq_length_MACRO, padding='post', dtype='float32')\n",
    "padded_sequences_FINANCE = tf.keras.preprocessing.sequence.pad_sequences(dataset_FINANCE, maxlen=max_seq_length_FINANCE, padding='post', dtype='float32')\n",
    "padded_sequences_DEMOGRAPHIC = tf.keras.preprocessing.sequence.pad_sequences(dataset_DEMOGRAPHIC, maxlen=max_seq_length_DEMOGRAPHIC, padding='post', dtype='float32')\n",
    "padded_sequences_OTHER = tf.keras.preprocessing.sequence.pad_sequences(dataset_OTHER, maxlen=max_seq_length_OTHER, padding='post', dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "791fcc9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:51.433357Z",
     "iopub.status.busy": "2024-09-26T19:22:51.432705Z",
     "iopub.status.idle": "2024-09-26T19:22:51.438911Z",
     "shell.execute_reply": "2024-09-26T19:22:51.438115Z"
    },
    "papermill": {
     "duration": 0.031537,
     "end_time": "2024-09-26T19:22:51.440770",
     "exception": false,
     "start_time": "2024-09-26T19:22:51.409233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reshaped_array_MICRO = padded_sequences_MICRO.reshape(padded_sequences_MICRO.shape[0], 1, padded_sequences_MICRO.shape[1])\n",
    "reshaped_array_INDUSTRY = padded_sequences_INDUSTRY.reshape(padded_sequences_INDUSTRY.shape[0], 1, padded_sequences_INDUSTRY.shape[1])\n",
    "reshaped_array_MACRO = padded_sequences_MACRO.reshape(padded_sequences_MACRO.shape[0], 1, padded_sequences_MACRO.shape[1])\n",
    "reshaped_array_FINANCE = padded_sequences_FINANCE.reshape(padded_sequences_FINANCE.shape[0], 1, padded_sequences_FINANCE.shape[1])\n",
    "reshaped_array_DEMOGRAPHIC = padded_sequences_DEMOGRAPHIC.reshape(padded_sequences_DEMOGRAPHIC.shape[0], 1, padded_sequences_DEMOGRAPHIC.shape[1])\n",
    "reshaped_array_OTHER = padded_sequences_OTHER.reshape(padded_sequences_OTHER.shape[0], 1, padded_sequences_OTHER.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd159d95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:51.486486Z",
     "iopub.status.busy": "2024-09-26T19:22:51.486236Z",
     "iopub.status.idle": "2024-09-26T19:22:51.491249Z",
     "shell.execute_reply": "2024-09-26T19:22:51.490411Z"
    },
    "papermill": {
     "duration": 0.030297,
     "end_time": "2024-09-26T19:22:51.493364",
     "exception": false,
     "start_time": "2024-09-26T19:22:51.463067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(474, 1, 108)\n",
      "(334, 1, 126)\n",
      "(312, 1, 126)\n",
      "(145, 1, 126)\n",
      "(111, 1, 120)\n",
      "(52, 1, 102)\n"
     ]
    }
   ],
   "source": [
    "print(reshaped_array_MICRO.shape)\n",
    "print(reshaped_array_INDUSTRY.shape)\n",
    "print(reshaped_array_MACRO.shape)\n",
    "print(reshaped_array_FINANCE.shape)\n",
    "print(reshaped_array_DEMOGRAPHIC.shape)\n",
    "print(reshaped_array_OTHER.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9cd5de44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:51.539534Z",
     "iopub.status.busy": "2024-09-26T19:22:51.538904Z",
     "iopub.status.idle": "2024-09-26T19:22:51.542551Z",
     "shell.execute_reply": "2024-09-26T19:22:51.541734Z"
    },
    "papermill": {
     "duration": 0.028622,
     "end_time": "2024-09-26T19:22:51.544375",
     "exception": false,
     "start_time": "2024-09-26T19:22:51.515753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "# dtc = DTC(n_clusters=2,input_dim=reshaped_array_DEMOGRAPHIC.shape[-1], timesteps=reshaped_array_DEMOGRAPHIC.shape[1], max_dim=max_seq_length_DEMOGRAPHIC,dataset_name='m3-demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "514b5b9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:51.590145Z",
     "iopub.status.busy": "2024-09-26T19:22:51.589853Z",
     "iopub.status.idle": "2024-09-26T19:22:51.593560Z",
     "shell.execute_reply": "2024-09-26T19:22:51.592562Z"
    },
    "papermill": {
     "duration": 0.02858,
     "end_time": "2024-09-26T19:22:51.595337",
     "exception": false,
     "start_time": "2024-09-26T19:22:51.566757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "# dtc = DTC(n_clusters=2,input_dim=reshaped_array_cif012.shape[-1], timesteps=reshaped_array_cif012.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2934349f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:51.641310Z",
     "iopub.status.busy": "2024-09-26T19:22:51.640750Z",
     "iopub.status.idle": "2024-09-26T19:22:51.646076Z",
     "shell.execute_reply": "2024-09-26T19:22:51.645229Z"
    },
    "papermill": {
     "duration": 0.030146,
     "end_time": "2024-09-26T19:22:51.647877",
     "exception": false,
     "start_time": "2024-09-26T19:22:51.617731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocessing_tourism_hospital(data):\n",
    "    ts_train = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        temp=np.array(list(data.iloc[i][:].dropna()))\n",
    "#         temp = temp[:-18]\n",
    "        temp=temp.reshape(1,len(temp),1)\n",
    "        temp2 = TimeSeriesScalerMeanVariance().fit_transform(temp)\n",
    "        ts_train.append(temp2.reshape(-1,1))\n",
    "\n",
    "    return ts_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aaa7a9f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:51.693541Z",
     "iopub.status.busy": "2024-09-26T19:22:51.693276Z",
     "iopub.status.idle": "2024-09-26T19:22:53.224054Z",
     "shell.execute_reply": "2024-09-26T19:22:53.222919Z"
    },
    "papermill": {
     "duration": 1.556062,
     "end_time": "2024-09-26T19:22:53.226288",
     "exception": false,
     "start_time": "2024-09-26T19:22:51.670226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(427, 1, 122)\n"
     ]
    }
   ],
   "source": [
    "dataset_tourism = pd.read_excel(\"/kaggle/input/newtsdatasets/Tourism-new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "\n",
    "dataset_tourism=dataset_tourism.iloc[:,:-8]\n",
    "dataset_tourism = preprocessing_tourism_hospital(dataset_tourism)\n",
    "max_seq_length_tourism= max(len(seq) for seq in dataset_tourism)\n",
    "padded_sequences_tourism = tf.keras.preprocessing.sequence.pad_sequences(dataset_tourism, maxlen=max_seq_length_tourism, padding='post', dtype='float32')\n",
    "reshaped_array_tourism = padded_sequences_tourism.reshape(padded_sequences_tourism.shape[0], 1, padded_sequences_tourism.shape[1])\n",
    "print(reshaped_array_tourism.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0b7f126",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:53.280636Z",
     "iopub.status.busy": "2024-09-26T19:22:53.279243Z",
     "iopub.status.idle": "2024-09-26T19:22:55.129927Z",
     "shell.execute_reply": "2024-09-26T19:22:55.128911Z"
    },
    "papermill": {
     "duration": 1.878343,
     "end_time": "2024-09-26T19:22:55.131974",
     "exception": false,
     "start_time": "2024-09-26T19:22:53.253631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(767, 1, 72)\n"
     ]
    }
   ],
   "source": [
    "dataset_hospital = pd.read_excel(\"/kaggle/input/newtsdatasets/Hospital_new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "\n",
    "dataset_hospital=dataset_hospital.iloc[:,:-12]\n",
    "dataset_hospital = preprocessing_tourism_hospital(dataset_hospital)\n",
    "max_seq_length_hospital= max(len(seq) for seq in dataset_hospital)\n",
    "padded_sequences_hospital = tf.keras.preprocessing.sequence.pad_sequences(dataset_hospital, maxlen=max_seq_length_hospital, padding='post', dtype='float32')\n",
    "reshaped_array_hospital= padded_sequences_hospital.reshape(padded_sequences_hospital.shape[0], 1, padded_sequences_hospital.shape[1])\n",
    "print(reshaped_array_hospital.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a91e106",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:55.181146Z",
     "iopub.status.busy": "2024-09-26T19:22:55.180749Z",
     "iopub.status.idle": "2024-09-26T19:22:55.375699Z",
     "shell.execute_reply": "2024-09-26T19:22:55.374696Z"
    },
    "papermill": {
     "duration": 0.221868,
     "end_time": "2024-09-26T19:22:55.377677",
     "exception": false,
     "start_time": "2024-09-26T19:22:55.155809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 1, 108)\n"
     ]
    }
   ],
   "source": [
    "dataset_cif = pd.read_excel(\"/kaggle/input/cifnewdataset/12.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "\n",
    "dataset_cif=dataset_cif.iloc[:,:-12]\n",
    "dataset_cif = preprocessing_tourism_hospital(dataset_cif)\n",
    "max_seq_length_cif= max(len(seq) for seq in dataset_cif)\n",
    "padded_sequences_cif = tf.keras.preprocessing.sequence.pad_sequences(dataset_cif, maxlen=max_seq_length_cif, padding='post', dtype='float32')\n",
    "reshaped_array_cif= padded_sequences_cif.reshape(padded_sequences_cif.shape[0], 1, padded_sequences_cif.shape[1])\n",
    "print(reshaped_array_cif.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e7b0d12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:55.425690Z",
     "iopub.status.busy": "2024-09-26T19:22:55.425393Z",
     "iopub.status.idle": "2024-09-26T19:22:55.429354Z",
     "shell.execute_reply": "2024-09-26T19:22:55.428472Z"
    },
    "papermill": {
     "duration": 0.029841,
     "end_time": "2024-09-26T19:22:55.431344",
     "exception": false,
     "start_time": "2024-09-26T19:22:55.401503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name='m3-other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f5461c39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:55.477982Z",
     "iopub.status.busy": "2024-09-26T19:22:55.477680Z",
     "iopub.status.idle": "2024-09-26T19:22:55.485022Z",
     "shell.execute_reply": "2024-09-26T19:22:55.484179Z"
    },
    "papermill": {
     "duration": 0.032635,
     "end_time": "2024-09-26T19:22:55.486806",
     "exception": false,
     "start_time": "2024-09-26T19:22:55.454171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset_name=='cif-12':\n",
    "    padded_seq=padded_sequences_cif\n",
    "    reshaped_array=reshaped_array_cif\n",
    "    max_dim=max_seq_length_cif\n",
    "elif dataset_name=='hospital':\n",
    "    padded_seq=padded_sequences_hospital\n",
    "    reshaped_array=reshaped_array_hospital\n",
    "    max_dim=max_seq_length_hospital\n",
    "elif dataset_name=='tourism':\n",
    "    padded_seq=padded_sequences_tourism\n",
    "    reshaped_array=reshaped_array_tourism\n",
    "    max_dim=max_seq_length_tourism\n",
    "elif dataset_name=='m3-demo':\n",
    "    padded_seq=padded_sequences_DEMOGRAPHIC\n",
    "    reshaped_array=reshaped_array_DEMOGRAPHIC\n",
    "    max_dim=max_seq_length_DEMOGRAPHIC\n",
    "elif dataset_name=='m3-finance':\n",
    "    padded_seq=padded_sequences_FINANCE\n",
    "    reshaped_array=reshaped_array_FINANCE\n",
    "    max_dim=max_seq_length_FINANCE\n",
    "elif dataset_name=='m3-industry':\n",
    "    padded_seq=padded_sequences_INDUSTRY\n",
    "    reshaped_array=reshaped_array_INDUSTRY\n",
    "    max_dim=max_seq_length_INDUSTRY\n",
    "\n",
    "elif dataset_name=='m3-macro':\n",
    "    padded_seq=padded_sequences_MACRO\n",
    "    reshaped_array=reshaped_array_MACRO\n",
    "    max_dim=max_seq_length_MACRO\n",
    "elif dataset_name=='m3-micro':\n",
    "    padded_seq=padded_sequences_MICRO\n",
    "    reshaped_array=reshaped_array_MICRO\n",
    "    max_dim=max_seq_length_MICRO\n",
    "else:\n",
    "    padded_seq=padded_sequences_OTHER\n",
    "    reshaped_array=reshaped_array_OTHER\n",
    "    max_dim=max_seq_length_OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9bda8202",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:55.533691Z",
     "iopub.status.busy": "2024-09-26T19:22:55.533405Z",
     "iopub.status.idle": "2024-09-26T19:22:55.543292Z",
     "shell.execute_reply": "2024-09-26T19:22:55.542426Z"
    },
    "papermill": {
     "duration": 0.035487,
     "end_time": "2024-09-26T19:22:55.545225",
     "exception": false,
     "start_time": "2024-09-26T19:22:55.509738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtc = DTC(n_clusters=2,input_dim=reshaped_array.shape[-1], timesteps=padded_seq.shape[1], max_dim=max_dim,dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84ee94e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:22:55.591708Z",
     "iopub.status.busy": "2024-09-26T19:22:55.591426Z",
     "iopub.status.idle": "2024-09-26T19:23:02.982039Z",
     "shell.execute_reply": "2024-09-26T19:23:02.980923Z"
    },
    "papermill": {
     "duration": 7.431182,
     "end_time": "2024-09-26T19:23:02.999113",
     "exception": false,
     "start_time": "2024-09-26T19:22:55.567931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Neighbour1_input (InputLayer)  [(None, None, 102)]  0           []                               \n",
      "                                                                                                  \n",
      " Neighbour2_input (InputLayer)  [(None, None, 102)]  0           []                               \n",
      "                                                                                                  \n",
      " input (InputLayer)             [(None, None, 102)]  0           []                               \n",
      "                                                                                                  \n",
      " lstm_Neighbour1_256 (LSTM)     (None, None, 256)    367616      ['Neighbour1_input[0][0]']       \n",
      "                                                                                                  \n",
      " lstm_Neighbour2_256 (LSTM)     (None, None, 256)    367616      ['Neighbour2_input[0][0]']       \n",
      "                                                                                                  \n",
      " lstm_input_256 (LSTM)          (None, None, 256)    367616      ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " lstm_Neighbour1_128 (LSTM)     (None, None, 128)    197120      ['lstm_Neighbour1_256[0][0]']    \n",
      "                                                                                                  \n",
      " lstm_Neighbour2_128 (LSTM)     (None, None, 128)    197120      ['lstm_Neighbour2_256[0][0]']    \n",
      "                                                                                                  \n",
      " lstm_input_128 (LSTM)          (None, None, 128)    197120      ['lstm_input_256[0][0]']         \n",
      "                                                                                                  \n",
      " attention_Neighbour1 (Attentio  (None, None, 128)   0           ['lstm_Neighbour1_128[0][0]',    \n",
      " n)                                                               'lstm_Neighbour2_128[0][0]',    \n",
      "                                                                  'lstm_Neighbour2_128[0][0]']    \n",
      "                                                                                                  \n",
      " attention_Neighbour2 (Attentio  (None, None, 128)   0           ['lstm_Neighbour2_128[0][0]',    \n",
      " n)                                                               'lstm_Neighbour1_128[0][0]',    \n",
      "                                                                  'lstm_Neighbour1_128[0][0]']    \n",
      "                                                                                                  \n",
      " attention_input (Attention)    (None, None, 128)    0           ['lstm_input_128[0][0]',         \n",
      "                                                                  'lstm_input_128[0][0]',         \n",
      "                                                                  'lstm_input_128[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, None, 256)    0           ['lstm_Neighbour1_128[0][0]',    \n",
      "                                                                  'attention_Neighbour1[0][0]']   \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, None, 256)    0           ['lstm_Neighbour2_128[0][0]',    \n",
      "                                                                  'attention_Neighbour2[0][0]']   \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, None, 256)    0           ['lstm_input_128[0][0]',         \n",
      "                                                                  'attention_input[0][0]']        \n",
      "                                                                                                  \n",
      " lstm_Neighbour1_64 (LSTM)      (None, 64)           82176       ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_Neighbour2_64 (LSTM)      (None, 64)           82176       ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_input_64 (LSTM)           (None, 64)           82176       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_Neighbour1 (Dense)       (None, 16)           1040        ['lstm_Neighbour1_64[0][0]']     \n",
      "                                                                                                  \n",
      " dense_Neighbour2 (Dense)       (None, 16)           1040        ['lstm_Neighbour2_64[0][0]']     \n",
      "                                                                                                  \n",
      " dense_input (Dense)            (None, 16)           1040        ['lstm_input_64[0][0]']          \n",
      "                                                                                                  \n",
      " maximum (Maximum)              (None, 16)           0           ['dense_Neighbour1[0][0]',       \n",
      "                                                                  'dense_Neighbour2[0][0]']       \n",
      "                                                                                                  \n",
      " add_inputs (Add)               (None, 16)           0           ['dense_input[0][0]',            \n",
      "                                                                  'maximum[0][0]']                \n",
      "                                                                                                  \n",
      " repeat_vector_2 (RepeatVector)  (None, 1, 16)       0           ['add_inputs[0][0]']             \n",
      "                                                                                                  \n",
      " decoded_lstm1 (LSTM)           (None, 1, 64)        20736       ['repeat_vector_2[0][0]']        \n",
      "                                                                                                  \n",
      " decoded_lstm2 (LSTM)           (None, 1, 128)       98816       ['decoded_lstm1[0][0]']          \n",
      "                                                                                                  \n",
      " decoded_lstm3 (LSTM)           (None, 1, 256)       394240      ['decoded_lstm2[0][0]']          \n",
      "                                                                                                  \n",
      " decoded_atten (Attention)      (None, 1, 256)       0           ['decoded_lstm3[0][0]',          \n",
      "                                                                  'decoded_lstm3[0][0]',          \n",
      "                                                                  'decoded_lstm3[0][0]']          \n",
      "                                                                                                  \n",
      " decoded_concat (Concatenate)   (None, 1, 512)       0           ['decoded_lstm3[0][0]',          \n",
      "                                                                  'decoded_atten[0][0]']          \n",
      "                                                                                                  \n",
      " decoded_timeD (TimeDistributed  (None, 1, 102)      52326       ['decoded_concat[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " TSClustering (TSClusteringLaye  (None, 2)           32          ['add_inputs[0][0]']             \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,510,006\n",
      "Trainable params: 2,510,006\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = 'adam'\n",
    "dtc.initialize()\n",
    "dtc.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b7a61b38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:23:03.060029Z",
     "iopub.status.busy": "2024-09-26T19:23:03.059418Z",
     "iopub.status.idle": "2024-09-26T19:23:03.079751Z",
     "shell.execute_reply": "2024-09-26T19:23:03.079046Z"
    },
    "papermill": {
     "duration": 0.053256,
     "end_time": "2024-09-26T19:23:03.081739",
     "exception": false,
     "start_time": "2024-09-26T19:23:03.028483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtc.compile(gamma=1.0, optimizer=optimizer, initial_heatmap_loss_weight=0.1,\n",
    "                final_heatmap_loss_weight=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba802cd",
   "metadata": {
    "papermill": {
     "duration": 0.029221,
     "end_time": "2024-09-26T19:23:03.140470",
     "exception": false,
     "start_time": "2024-09-26T19:23:03.111249",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9ec78978",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:23:03.200357Z",
     "iopub.status.busy": "2024-09-26T19:23:03.199732Z",
     "iopub.status.idle": "2024-09-26T19:23:43.881966Z",
     "shell.execute_reply": "2024-09-26T19:23:43.880905Z"
    },
    "papermill": {
     "duration": 40.714504,
     "end_time": "2024-09-26T19:23:43.883895",
     "exception": false,
     "start_time": "2024-09-26T19:23:03.169391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining...\n",
      "Epoch 1/50\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.6353\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.6277\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.6149\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.5896\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.5111\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 0.4494\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4202\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.4129\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.4070\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4003\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3989\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3948\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3970\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3964\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3937\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3919\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3907\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3890\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 0.3872\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.3859\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.3843\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3818\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.3795\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.3740\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.3660\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.3537\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 0.3330\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.3150\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.2932\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.2629\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.2302\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.1967\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.1794\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 0.1710\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.1676\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.1652\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.1645\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.1629\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 0.1609\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.1608\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.1610\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.1622\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.1621\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 0.1643\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.1627\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.1614\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.1598\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 0.1598\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.1592\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.1590\n",
      "Pretrained weights are saved to /kaggle/working//ae_weights-epoch50.h5\n"
     ]
    }
   ],
   "source": [
    "dtc.pretrain(X=reshaped_array, optimizer=optimizer, epochs=50, batch_size=10,save_dir=\"/kaggle/working/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5953eb99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:23:44.004490Z",
     "iopub.status.busy": "2024-09-26T19:23:44.003720Z",
     "iopub.status.idle": "2024-09-26T19:23:44.068413Z",
     "shell.execute_reply": "2024-09-26T19:23:44.067699Z"
    },
    "papermill": {
     "duration": 0.127335,
     "end_time": "2024-09-26T19:23:44.070343",
     "exception": false,
     "start_time": "2024-09-26T19:23:43.943008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtc.load_ae_neighbour_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52529cb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:23:44.188442Z",
     "iopub.status.busy": "2024-09-26T19:23:44.188125Z",
     "iopub.status.idle": "2024-09-26T19:23:44.491674Z",
     "shell.execute_reply": "2024-09-26T19:23:44.490702Z"
    },
    "papermill": {
     "duration": 0.364854,
     "end_time": "2024-09-26T19:23:44.493509",
     "exception": false,
     "start_time": "2024-09-26T19:23:44.128655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing cluster...\n",
      "2/2 [==============================] - 0s 21ms/step\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Initialize clusters\n",
    "dtc.init_cluster_weights(reshaped_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e155ead6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T19:23:44.614311Z",
     "iopub.status.busy": "2024-09-26T19:23:44.614007Z",
     "iopub.status.idle": "2024-09-26T23:00:41.073053Z",
     "shell.execute_reply": "2024-09-26T23:00:41.072199Z"
    },
    "papermill": {
     "duration": 13016.583565,
     "end_time": "2024-09-26T23:00:41.138210",
     "exception": false,
     "start_time": "2024-09-26T19:23:44.554645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10 epochs.\n",
      "Evaluating every 10 and saving model every 10 epochs.\n",
      "m3-other 71 120\n",
      "2/2 [==============================] - 0s 81ms/step\n",
      "2/2 [==============================] - 0s 58ms/step\n",
      "Number of reliable samples: 52\n",
      "Number of unreliable samples: 0\n",
      "list:  [array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "Best Clusters:  [0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Mean Validation SMAPE:  8.96096179812143\n",
      "Mean Test SMAPE:  10.514756906692021\n",
      "Mean Validation RMSE:  480.9688950382212\n",
      "Mean Test RMSE:  559.2036751647995\n",
      "Median Validation SMAPE:  6.875955738857895\n",
      "Median Test SMAPE:  9.392280189781012\n",
      "Median Validation RMSE:  430.5868121340433\n",
      "Median Test RMSE:  484.5326842469622\n",
      "reconstruction_loss:  tf.Tensor(34.06691, shape=(), dtype=float32)\n",
      "clustering_loss:  tf.Tensor(0.03755803, shape=(), dtype=float32)\n",
      "validation_loss:  tf.Tensor(6.303115, shape=(), dtype=float32)\n",
      "Modified Clustering:  [0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0\n",
      " 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0]\n",
      "Modified Clustering:  [0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 1 1 1 0 0 1 0]\n",
      "Modified Clustering:  [0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "Modified Clustering:  [1 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1]\n",
      "Modified Clustering:  [0 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "Epoch: 1/10\n",
      "Mean Validation SMAPE:  9.462366803154959\n",
      "Mean Test SMAPE:  9.71395418975036\n",
      "Mean Validation RMSE:  514.2889135459438\n",
      "Mean Test RMSE:  554.5360001697646\n",
      "Median Validation SMAPE:  7.823025848904971\n",
      "Median Test SMAPE:  7.965949549641053\n",
      "Median Validation RMSE:  396.3176449146679\n",
      "Median Test RMSE:  365.2226689403614\n",
      "Total Loss:  tf.Tensor(11.863386, shape=(), dtype=float32)\n",
      "m3-other 71 120\n",
      "2/2 [==============================] - 0s 96ms/step\n",
      "Number of reliable samples: 52\n",
      "Number of unreliable samples: 0\n",
      "list:  [array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "Best Clusters:  [0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Mean Validation SMAPE:  8.684304470841509\n",
      "Mean Test SMAPE:  9.025079519617373\n",
      "Mean Validation RMSE:  469.4180182715973\n",
      "Mean Test RMSE:  523.7295059414613\n",
      "Median Validation SMAPE:  7.213647406714363\n",
      "Median Test SMAPE:  7.5412020171566665\n",
      "Median Validation RMSE:  433.9739895529851\n",
      "Median Test RMSE:  356.3275567520593\n",
      "reconstruction_loss:  tf.Tensor(32.374935, shape=(), dtype=float32)\n",
      "clustering_loss:  tf.Tensor(0.03742018, shape=(), dtype=float32)\n",
      "validation_loss:  tf.Tensor(6.165947, shape=(), dtype=float32)\n",
      "Modified Clustering:  [0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0\n",
      " 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0]\n",
      "Modified Clustering:  [0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1]\n",
      "Modified Clustering:  [0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n",
      " 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "Modified Clustering:  [0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0\n",
      " 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0]\n",
      "Modified Clustering:  [0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1\n",
      " 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "Epoch: 2/10\n",
      "Mean Validation SMAPE:  9.287521860982482\n",
      "Mean Test SMAPE:  9.773594589903505\n",
      "Mean Validation RMSE:  499.9975203766756\n",
      "Mean Test RMSE:  565.57563114579\n",
      "Median Validation SMAPE:  8.029592473630414\n",
      "Median Test SMAPE:  7.845725179558491\n",
      "Median Validation RMSE:  437.74992417064743\n",
      "Median Test RMSE:  493.36147739063784\n",
      "Total Loss:  tf.Tensor(11.415229, shape=(), dtype=float32)\n",
      "m3-other 71 120\n",
      "2/2 [==============================] - 0s 80ms/step\n",
      "Number of reliable samples: 52\n",
      "Number of unreliable samples: 0\n",
      "Modified Clustering:  [1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1]\n",
      "Modified Clustering:  [0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0 0\n",
      " 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Modified Clustering:  [0 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0]\n",
      "Modified Clustering:  [0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0\n",
      " 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "Modified Clustering:  [1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1]\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Epoch: 3/10\n",
      "Mean Validation SMAPE:  9.37286661926457\n",
      "Mean Test SMAPE:  9.488227347438947\n",
      "Mean Validation RMSE:  520.1815898042044\n",
      "Mean Test RMSE:  548.6123819252226\n",
      "Median Validation SMAPE:  7.367690463334998\n",
      "Median Test SMAPE:  8.165124633231404\n",
      "Median Validation RMSE:  483.46068180703105\n",
      "Median Test RMSE:  454.2368669603207\n",
      "Total Loss:  tf.Tensor(11.415229, shape=(), dtype=float32)\n",
      "m3-other 71 120\n",
      "2/2 [==============================] - 0s 77ms/step\n",
      "Number of reliable samples: 52\n",
      "Number of unreliable samples: 0\n",
      "Modified Clustering:  [0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 0\n",
      " 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0]\n",
      "Modified Clustering:  [0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
      "Modified Clustering:  [0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Modified Clustering:  [0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0\n",
      " 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0]\n",
      "Modified Clustering:  [0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1]\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "Epoch: 4/10\n",
      "Mean Validation SMAPE:  9.387628499720327\n",
      "Mean Test SMAPE:  9.472738424007149\n",
      "Mean Validation RMSE:  514.1451553790502\n",
      "Mean Test RMSE:  552.3840807796079\n",
      "Median Validation SMAPE:  8.48834197306259\n",
      "Median Test SMAPE:  7.427197722316942\n",
      "Median Validation RMSE:  479.825730163594\n",
      "Median Test RMSE:  414.5499029847517\n",
      "Total Loss:  tf.Tensor(11.415229, shape=(), dtype=float32)\n",
      "m3-other 71 120\n",
      "2/2 [==============================] - 0s 77ms/step\n",
      "Number of reliable samples: 52\n",
      "Number of unreliable samples: 0\n",
      "Modified Clustering:  [1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1\n",
      " 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0]\n",
      "Modified Clustering:  [0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 1 1 0 0 0 0 0 1 1 1 0 0 0 1 0]\n",
      "Modified Clustering:  [0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0]\n",
      "Modified Clustering:  [0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Modified Clustering:  [0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0]\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "Epoch: 5/10\n",
      "Mean Validation SMAPE:  9.285532552670976\n",
      "Mean Test SMAPE:  9.953201547530846\n",
      "Mean Validation RMSE:  508.25388596999113\n",
      "Mean Test RMSE:  560.0180153883404\n",
      "Median Validation SMAPE:  8.071113780630958\n",
      "Median Test SMAPE:  8.463020204151853\n",
      "Median Validation RMSE:  485.8577809488377\n",
      "Median Test RMSE:  417.52210183225054\n",
      "Total Loss:  tf.Tensor(11.415229, shape=(), dtype=float32)\n",
      "m3-other 71 120\n",
      "2/2 [==============================] - 0s 80ms/step\n",
      "Number of reliable samples: 52\n",
      "Number of unreliable samples: 0\n",
      "Modified Clustering:  [0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Modified Clustering:  [1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0\n",
      " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0]\n",
      "Modified Clustering:  [0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0\n",
      " 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Modified Clustering:  [0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "Modified Clustering:  [0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0]\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "Epoch: 6/10\n",
      "Mean Validation SMAPE:  9.076680295586112\n",
      "Mean Test SMAPE:  10.382445434360296\n",
      "Mean Validation RMSE:  499.59980058489185\n",
      "Mean Test RMSE:  570.8456955733537\n",
      "Median Validation SMAPE:  6.895428585109232\n",
      "Median Test SMAPE:  8.891717009239855\n",
      "Median Validation RMSE:  477.16159612952606\n",
      "Median Test RMSE:  499.00036206097224\n",
      "Total Loss:  tf.Tensor(11.415229, shape=(), dtype=float32)\n",
      "m3-other 71 120\n",
      "2/2 [==============================] - 0s 77ms/step\n",
      "Number of reliable samples: 52\n",
      "Number of unreliable samples: 0\n",
      "Modified Clustering:  [1 0 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "Modified Clustering:  [0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "Modified Clustering:  [0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0]\n",
      "Modified Clustering:  [0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "Modified Clustering:  [0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1]\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Epoch: 7/10\n",
      "Mean Validation SMAPE:  9.754462447976534\n",
      "Mean Test SMAPE:  9.89810610283386\n",
      "Mean Validation RMSE:  540.7255958181407\n",
      "Mean Test RMSE:  567.4792229334312\n",
      "Median Validation SMAPE:  7.9304187137537125\n",
      "Median Test SMAPE:  8.488747696963072\n",
      "Median Validation RMSE:  489.18647713018925\n",
      "Median Test RMSE:  437.37880325254685\n",
      "Total Loss:  tf.Tensor(11.415229, shape=(), dtype=float32)\n",
      "m3-other 71 120\n",
      "2/2 [==============================] - 0s 85ms/step\n",
      "Number of reliable samples: 52\n",
      "Number of unreliable samples: 0\n",
      "Modified Clustering:  [1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0\n",
      " 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0]\n",
      "Modified Clustering:  [1 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0\n",
      " 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Modified Clustering:  [0 0 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
      "Modified Clustering:  [0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0\n",
      " 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0]\n",
      "Modified Clustering:  [1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Epoch: 8/10\n",
      "Mean Validation SMAPE:  9.623893377557357\n",
      "Mean Test SMAPE:  10.103338180032827\n",
      "Mean Validation RMSE:  522.8416894582905\n",
      "Mean Test RMSE:  585.2582392153579\n",
      "Median Validation SMAPE:  7.8501723107717485\n",
      "Median Test SMAPE:  7.96141760750877\n",
      "Median Validation RMSE:  405.8975640879505\n",
      "Median Test RMSE:  525.8630835584004\n",
      "Total Loss:  tf.Tensor(11.415229, shape=(), dtype=float32)\n",
      "m3-other 71 120\n",
      "2/2 [==============================] - 0s 78ms/step\n",
      "Number of reliable samples: 52\n",
      "Number of unreliable samples: 0\n",
      "Modified Clustering:  [0 0 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "Modified Clustering:  [0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1\n",
      " 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0]\n",
      "Modified Clustering:  [0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0]\n",
      "Modified Clustering:  [0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0\n",
      " 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1]\n",
      "Modified Clustering:  [0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0]\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "Epoch: 9/10\n",
      "Mean Validation SMAPE:  9.274831851406784\n",
      "Mean Test SMAPE:  9.400531102893492\n",
      "Mean Validation RMSE:  506.8708069561035\n",
      "Mean Test RMSE:  535.0922413048834\n",
      "Median Validation SMAPE:  7.834466894596895\n",
      "Median Test SMAPE:  7.81375276423573\n",
      "Median Validation RMSE:  410.13843065447054\n",
      "Median Test RMSE:  429.78841987505484\n",
      "Total Loss:  tf.Tensor(11.415229, shape=(), dtype=float32)\n",
      "m3-other 71 120\n",
      "2/2 [==============================] - 0s 78ms/step\n",
      "Number of reliable samples: 52\n",
      "Number of unreliable samples: 0\n",
      "Modified Clustering:  [0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Modified Clustering:  [0 0 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "Modified Clustering:  [0 1 0 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1]\n",
      "Modified Clustering:  [0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0\n",
      " 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Modified Clustering:  [1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Epoch: 10/10\n",
      "Mean Validation SMAPE:  9.09150502048522\n",
      "Mean Test SMAPE:  9.802733125821426\n",
      "Mean Validation RMSE:  501.7403398254081\n",
      "Mean Test RMSE:  565.8858577494732\n",
      "Median Validation SMAPE:  7.71538002474189\n",
      "Median Test SMAPE:  8.115636775325603\n",
      "Median Validation RMSE:  477.87282093652516\n",
      "Median Test RMSE:  426.1947825047137\n",
      "Total Loss:  tf.Tensor(11.415229, shape=(), dtype=float32)\n",
      "Metrics saved to metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "dtc.fit(reshaped_array, y_train=None, X_val=None, y_val=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "01d98f40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T23:00:41.275107Z",
     "iopub.status.busy": "2024-09-26T23:00:41.274758Z",
     "iopub.status.idle": "2024-09-26T23:00:41.278672Z",
     "shell.execute_reply": "2024-09-26T23:00:41.277922Z"
    },
    "papermill": {
     "duration": 0.074909,
     "end_time": "2024-09-26T23:00:41.280566",
     "exception": false,
     "start_time": "2024-09-26T23:00:41.205657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Evaluate\n",
    "# print('Performance (TRAIN)')\n",
    "# results = {}\n",
    "# q = dtc.model.predict(reshaped_array)[1]\n",
    "# y_pred = q.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7fb8b44f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T23:00:41.417563Z",
     "iopub.status.busy": "2024-09-26T23:00:41.417259Z",
     "iopub.status.idle": "2024-09-26T23:00:41.420976Z",
     "shell.execute_reply": "2024-09-26T23:00:41.420126Z"
    },
    "papermill": {
     "duration": 0.075033,
     "end_time": "2024-09-26T23:00:41.422788",
     "exception": false,
     "start_time": "2024-09-26T23:00:41.347755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# np.unique(y_pred, return_counts=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3743351,
     "sourceId": 6479400,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3792584,
     "sourceId": 6564428,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4520850,
     "sourceId": 7735771,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4454423,
     "sourceId": 9326749,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 146550165,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 146550185,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 166856298,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13183.728163,
   "end_time": "2024-09-26T23:00:45.081317",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-26T19:21:01.353154",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
