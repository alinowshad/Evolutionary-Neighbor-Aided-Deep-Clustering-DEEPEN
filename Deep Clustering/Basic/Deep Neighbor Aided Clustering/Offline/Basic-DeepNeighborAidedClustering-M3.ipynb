{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bba3c27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:15:28.426435Z",
     "iopub.status.busy": "2024-09-20T18:15:28.424114Z",
     "iopub.status.idle": "2024-09-20T18:15:46.391866Z",
     "shell.execute_reply": "2024-09-20T18:15:46.390327Z"
    },
    "papermill": {
     "duration": 17.989375,
     "end_time": "2024-09-20T18:15:46.395081",
     "exception": false,
     "start_time": "2024-09-20T18:15:28.405706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tslearn\r\n",
      "  Downloading tslearn-0.6.3-py3-none-any.whl (374 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tslearn) (1.23.5)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from tslearn) (1.11.2)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from tslearn) (1.2.2)\r\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from tslearn) (0.57.1)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from tslearn) (1.3.2)\r\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->tslearn) (0.40.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->tslearn) (3.1.0)\r\n",
      "Installing collected packages: tslearn\r\n",
      "Successfully installed tslearn-0.6.3\r\n"
     ]
    }
   ],
   "source": [
    "! pip install tslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9496ad91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:15:46.429277Z",
     "iopub.status.busy": "2024-09-20T18:15:46.428744Z",
     "iopub.status.idle": "2024-09-20T18:16:02.212527Z",
     "shell.execute_reply": "2024-09-20T18:16:02.210611Z"
    },
    "papermill": {
     "duration": 15.804898,
     "end_time": "2024-09-20T18:16:02.216091",
     "exception": false,
     "start_time": "2024-09-20T18:15:46.411193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn-extra\r\n",
      "  Downloading scikit_learn_extra-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn-extra) (1.23.5)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn-extra) (1.11.2)\r\n",
      "Requirement already satisfied: scikit-learn>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn-extra) (1.2.2)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.1.0)\r\n",
      "Installing collected packages: scikit-learn-extra\r\n",
      "Successfully installed scikit-learn-extra-0.3.0\r\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-learn-extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f330d3c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:02.251548Z",
     "iopub.status.busy": "2024-09-20T18:16:02.251017Z",
     "iopub.status.idle": "2024-09-20T18:16:17.314288Z",
     "shell.execute_reply": "2024-09-20T18:16:17.312733Z"
    },
    "papermill": {
     "duration": 15.0847,
     "end_time": "2024-09-20T18:16:17.317434",
     "exception": false,
     "start_time": "2024-09-20T18:16:02.232734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rstl\r\n",
      "  Downloading rstl-0.1.3-py3-none-any.whl (5.2 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rstl) (1.23.5)\r\n",
      "Installing collected packages: rstl\r\n",
      "Successfully installed rstl-0.1.3\r\n"
     ]
    }
   ],
   "source": [
    "! pip install rstl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e726192",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:17.352875Z",
     "iopub.status.busy": "2024-09-20T18:16:17.352336Z",
     "iopub.status.idle": "2024-09-20T18:16:37.064937Z",
     "shell.execute_reply": "2024-09-20T18:16:37.063351Z"
    },
    "papermill": {
     "duration": 19.734093,
     "end_time": "2024-09-20T18:16:37.067958",
     "exception": false,
     "start_time": "2024-09-20T18:16:17.333865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import os\n",
    "import csv\n",
    "import argparse\n",
    "from time import time\n",
    "# Keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Reshape, UpSampling2D, Conv2DTranspose, GlobalAveragePooling1D, Softmax\n",
    "from keras.losses import kullback_leibler_divergence\n",
    "import keras.backend as K\n",
    "# scikit-learn\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "# Dataset helper function\n",
    "# DTC components\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, LeakyReLU, MaxPool1D, LSTM, Bidirectional, TimeDistributed, Dense, Reshape\n",
    "from keras.layers import UpSampling2D, Conv2DTranspose\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from statsmodels.tsa import stattools\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, Attention\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9a354be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:37.104059Z",
     "iopub.status.busy": "2024-09-20T18:16:37.103059Z",
     "iopub.status.idle": "2024-09-20T18:16:37.321251Z",
     "shell.execute_reply": "2024-09-20T18:16:37.319729Z"
    },
    "papermill": {
     "duration": 0.240388,
     "end_time": "2024-09-20T18:16:37.325078",
     "exception": false,
     "start_time": "2024-09-20T18:16:37.084690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "import tensorflow_addons as tfa\n",
    "from math import pi, ceil\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from keras.losses import CategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "427fe2b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:37.363830Z",
     "iopub.status.busy": "2024-09-20T18:16:37.363252Z",
     "iopub.status.idle": "2024-09-20T18:16:37.389024Z",
     "shell.execute_reply": "2024-09-20T18:16:37.387604Z"
    },
    "papermill": {
     "duration": 0.049557,
     "end_time": "2024-09-20T18:16:37.392355",
     "exception": false,
     "start_time": "2024-09-20T18:16:37.342798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import SpectralClustering\n",
    "# %tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import layers\n",
    "from keras.models import Sequential,Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import time\n",
    "print(tf.__version__)\n",
    "from keras.layers import MultiHeadAttention\n",
    "from keras.layers import Dense\n",
    "import gc\n",
    "from keras.layers import concatenate\n",
    "import csv\n",
    "import math\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# import xgboost as xgb\n",
    "warnings.filterwarnings('ignore')\n",
    "# import GPy, GPyOpt\n",
    "tfkl = tf.keras.layers\n",
    "tfk = tf.keras\n",
    "\n",
    "from rstl import STL\n",
    "from texttable import Texttable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f9b11aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:37.429880Z",
     "iopub.status.busy": "2024-09-20T18:16:37.429360Z",
     "iopub.status.idle": "2024-09-20T18:16:37.445715Z",
     "shell.execute_reply": "2024-09-20T18:16:37.443973Z"
    },
    "papermill": {
     "duration": 0.03884,
     "end_time": "2024-09-20T18:16:37.448525",
     "exception": false,
     "start_time": "2024-09-20T18:16:37.409685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def autoencoder(latent_dim=16, series_len=None):\n",
    "\n",
    "    # Input\n",
    "    x = Input((None, series_len),name='input')\n",
    "    #masked_seq = layers.Masking(mask_value=0.0)(encoder_inputs)\n",
    "    # Encoder\n",
    "    encoded = LSTM(256, return_sequences=True, name=\"encoded_lstm1\")(x)\n",
    "    encoded = LSTM(128, return_sequences=True, name=\"encoded_lstm2\")(encoded)\n",
    "    attention = Attention(name=\"encoded_atten\")([encoded, encoded, encoded])  # Apply self-attention to the encoder outputs\n",
    "    merged = layers.Concatenate(axis=-1)([encoded, attention])        \n",
    "    encoded = LSTM(64, name=\"encoded_lstm3\")(merged)\n",
    "    encoded = Dense(latent_dim, name=\"encoded_dense\")(encoded)\n",
    "\n",
    "    # Decoder\n",
    "    decoded = RepeatVector(1)(encoded)\n",
    "    decoded = LSTM(64, return_sequences=True, name=\"decoded_lstm1\")(decoded)\n",
    "    decoded = LSTM(128, return_sequences=True, name=\"decoded_lstm2\")(decoded)\n",
    "    decoded = LSTM(256, return_sequences=True, name=\"decoded_lstm3\")(decoded)\n",
    "    attention = Attention(name=\"decoded_atten\")([decoded, decoded, decoded])  # Apply self-attention to the decoder outputs\n",
    "    decoded = layers.Concatenate(axis=-1, name=\"decoded_concat\")([decoded, attention])\n",
    "    decoder_outputs = TimeDistributed(Dense(series_len), name=\"decoded_timeD\")(decoded)\n",
    "    \n",
    "    # AE model\n",
    "    autoencoder = Model(inputs=x, outputs=decoder_outputs, name='AE')\n",
    "\n",
    "    # Encoder model\n",
    "    encoder = Model(inputs=x, outputs=encoded, name='encoder')\n",
    "\n",
    "    # Create input for decoder model\n",
    "    encoded_input =Input(shape=(latent_dim,))\n",
    "    # Internal layers in decoder\n",
    "    decoded = RepeatVector(1)(encoded_input)\n",
    "    decoded = autoencoder.get_layer('decoded_lstm1')(decoded)\n",
    "    decoded = autoencoder.get_layer('decoded_lstm2')(decoded)\n",
    "    decoded = autoencoder.get_layer('decoded_lstm3')(decoded)\n",
    "    attention = autoencoder.get_layer('decoded_atten')([decoded, decoded, decoded])\n",
    "    decoded = autoencoder.get_layer('decoded_concat')([decoded, attention])\n",
    "    decoder_output = autoencoder.get_layer('decoded_timeD')(decoded)\n",
    "    # Decoder model\n",
    "    decoder = Model(inputs=encoded_input, outputs=decoder_output, name='decoder')\n",
    "\n",
    "    return autoencoder, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fc132a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:37.487873Z",
     "iopub.status.busy": "2024-09-20T18:16:37.487273Z",
     "iopub.status.idle": "2024-09-20T18:16:37.511788Z",
     "shell.execute_reply": "2024-09-20T18:16:37.510415Z"
    },
    "papermill": {
     "duration": 0.047888,
     "end_time": "2024-09-20T18:16:37.514839",
     "exception": false,
     "start_time": "2024-09-20T18:16:37.466951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def autoencoderNeighbour(latent_dim=16, series_len=None):\n",
    "\n",
    "    # Input\n",
    "    x = Input((None, series_len), name='input')\n",
    "    #masked_seq = layers.Masking(mask_value=0.0)(encoder_inputs)\n",
    "    # Encoder\n",
    "    encoded = LSTM(256, return_sequences=True, name='lstm_input_256')(x)\n",
    "    encoded = LSTM(128, return_sequences=True, name='lstm_input_128')(encoded)\n",
    "    attention = Attention(name='attention_input')([encoded, encoded, encoded])  # Apply self-attention to the encoder outputs\n",
    "    merged = layers.Concatenate(axis=-1)([encoded, attention])        \n",
    "    encoded = LSTM(64, name='lstm_input_64')(merged)\n",
    "    encoded = Dense(latent_dim, name='dense_input')(encoded)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    x_neighbour1 = Input((None, series_len), name='Neighbour1_input')\n",
    "    x_neighbour2 = Input((None, series_len), name='Neighbour2_input')\n",
    "    \n",
    "    encoded_neighbour1 = LSTM(256, return_sequences=True, name='lstm_Neighbour1_256')(x_neighbour1)\n",
    "    encoded_neighbour2 = LSTM(256, return_sequences=True, name='lstm_Neighbour2_256')(x_neighbour2)\n",
    "    encoded_neighbour1 = LSTM(128, return_sequences=True, name='lstm_Neighbour1_128')(encoded_neighbour1)\n",
    "    encoded_neighbour2 = LSTM(128, return_sequences=True, name='lstm_Neighbour2_128')(encoded_neighbour2)\n",
    "    attention_neighbour1 = Attention(name='attention_Neighbour1')([encoded_neighbour1, encoded_neighbour2, encoded_neighbour2])  # Apply self-attention to the encoder outputs\n",
    "    attention_neighbour2 = Attention(name='attention_Neighbour2')([encoded_neighbour2, encoded_neighbour1, encoded_neighbour1])  # Apply self-attention to the encoder outputs\n",
    "    merged_neighbour1 = layers.Concatenate(axis=-1)([encoded_neighbour1, attention_neighbour1])        \n",
    "    merged_neighbour2 = layers.Concatenate(axis=-1)([encoded_neighbour2, attention_neighbour2])        \n",
    "    encoded_neighbour1 = LSTM(64, name='lstm_Neighbour1_64')(merged_neighbour1)\n",
    "    encoded_neighbour2 = LSTM(64, name='lstm_Neighbour2_64')(merged_neighbour2)\n",
    "    encoded_neighbour1 = Dense(latent_dim, name='dense_Neighbour1')(encoded_neighbour1)\n",
    "    encoded_neighbour2 = Dense(latent_dim, name='dense_Neighbour2')(encoded_neighbour2)\n",
    "\n",
    "    out_encoder_Neighbor = layers.Maximum()([encoded_neighbour1, encoded_neighbour2]) \n",
    "    # add Final encoder\n",
    "    encoded = layers.add([encoded ,out_encoder_Neighbor],name='add_inputs')\n",
    "    \n",
    "    \n",
    "    # Decoder\n",
    "    decoded = RepeatVector(1)(encoded)\n",
    "    decoded = LSTM(64, return_sequences=True, name=\"decoded_lstm1\")(decoded)\n",
    "    decoded = LSTM(128, return_sequences=True, name=\"decoded_lstm2\")(decoded)\n",
    "    decoded = LSTM(256, return_sequences=True, name=\"decoded_lstm3\")(decoded)\n",
    "    attention = Attention(name=\"decoded_atten\")([decoded, decoded, decoded])  # Apply self-attention to the decoder outputs\n",
    "    decoded = layers.Concatenate(axis=-1, name=\"decoded_concat\")([decoded, attention])\n",
    "    decoder_outputs = TimeDistributed(Dense(series_len), name=\"decoded_timeD\")(decoded)\n",
    "    \n",
    "    # AE model\n",
    "    autoencoder = Model(inputs=[x,x_neighbour1,x_neighbour2], outputs=decoder_outputs, name='AE')\n",
    "\n",
    "    # Encoder model\n",
    "    encoder = Model(inputs=[x,x_neighbour1,x_neighbour2], outputs=encoded, name='encoder')\n",
    "\n",
    "    # Create input for decoder model\n",
    "    encoded_input =Input(shape=(latent_dim,))\n",
    "    # Internal layers in decoder\n",
    "    decoded = RepeatVector(1)(encoded_input)\n",
    "    decoded = autoencoder.get_layer('decoded_lstm1')(decoded)\n",
    "    decoded = autoencoder.get_layer('decoded_lstm2')(decoded)\n",
    "    decoded = autoencoder.get_layer('decoded_lstm3')(decoded)\n",
    "    attention = autoencoder.get_layer('decoded_atten')([decoded, decoded, decoded])\n",
    "    decoded = autoencoder.get_layer('decoded_concat')([decoded, attention])\n",
    "    decoder_output = autoencoder.get_layer('decoded_timeD')(decoded)\n",
    "    # Decoder model\n",
    "    decoder = Model(inputs=encoded_input, outputs=decoder_output, name='decoder')\n",
    "\n",
    "    return autoencoder, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2653da09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:37.551492Z",
     "iopub.status.busy": "2024-09-20T18:16:37.550933Z",
     "iopub.status.idle": "2024-09-20T18:16:37.568367Z",
     "shell.execute_reply": "2024-09-20T18:16:37.566876Z"
    },
    "papermill": {
     "duration": 0.03907,
     "end_time": "2024-09-20T18:16:37.571344",
     "exception": false,
     "start_time": "2024-09-20T18:16:37.532274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eucl(x, y):\n",
    "    \"\"\"\n",
    "    Euclidean distance between two multivariate time series given as arrays of shape (timesteps, dim)\n",
    "    \"\"\"\n",
    "    d = np.sqrt(np.sum(np.square(x - y), axis=0))\n",
    "    return np.sum(d)\n",
    "\n",
    "\n",
    "def cid(x, y):\n",
    "    \"\"\"\n",
    "    Complexity-Invariant Distance (CID) between two multivariate time series given as arrays of shape (timesteps, dim)\n",
    "    Reference: Batista, Wang & Keogh (2011). A Complexity-Invariant Distance Measure for Time Series. https://doi.org/10.1137/1.9781611972818.60\n",
    "    \"\"\"\n",
    "    assert(len(x.shape) == 2 and x.shape == y.shape)  # time series must have same length and dimensionality\n",
    "    ce_x = np.sqrt(np.sum(np.square(np.diff(x, axis=0)), axis=0) + 1e-9)\n",
    "    ce_y = np.sqrt(np.sum(np.square(np.diff(y, axis=0)), axis=0) + 1e-9)\n",
    "    d = np.sqrt(np.sum(np.square(x - y), axis=0)) * np.divide(np.maximum(ce_x, ce_y), np.minimum(ce_x, ce_y))\n",
    "    return np.sum(d)\n",
    "\n",
    "\n",
    "def cor(x, y):\n",
    "    \"\"\"\n",
    "    Correlation-based distance (COR) between two multivariate time series given as arrays of shape (timesteps, dim)\n",
    "    \"\"\"\n",
    "    scaler = TimeSeriesScalerMeanVariance()\n",
    "    x_norm = scaler.fit_transform(x)\n",
    "    y_norm = scaler.fit_transform(y)\n",
    "    pcc = np.mean(x_norm * y_norm)  # Pearson correlation coefficients\n",
    "    d = np.sqrt(2.0 * (1.0 - pcc + 1e-9))  # correlation-based similarities\n",
    "    return np.sum(d)\n",
    "\n",
    "\n",
    "def acf(x, y):\n",
    "    \"\"\"\n",
    "    Autocorrelation-based distance (ACF) between two multivariate time series given as arrays of shape (timesteps, dim)\n",
    "    Computes a linearly weighted euclidean distance between the autocorrelation coefficients of the input time series.\n",
    "    Reference: Galeano & Pena (2000). Multivariate Analysis in Vector Time Series.\n",
    "    \"\"\"\n",
    "    assert (len(x.shape) == 2 and x.shape == y.shape)  # time series must have same length and dimensionality\n",
    "    x_acf = np.apply_along_axis(lambda z: stattools.acf(z, nlags=z.shape[0]), 0, x)\n",
    "    y_acf = np.apply_along_axis(lambda z: stattools.acf(z, nlags=z.shape[0]), 0, y)\n",
    "    weights = np.linspace(1.0, 0.0, x.shape[0])\n",
    "    d = np.sqrt(np.sum(np.expand_dims(weights, axis=1) * np.square(x_acf - y_acf), axis=0))\n",
    "    return np.sum(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f0a90a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:37.608445Z",
     "iopub.status.busy": "2024-09-20T18:16:37.606898Z",
     "iopub.status.idle": "2024-09-20T18:16:37.624667Z",
     "shell.execute_reply": "2024-09-20T18:16:37.623029Z"
    },
    "papermill": {
     "duration": 0.039249,
     "end_time": "2024-09-20T18:16:37.627504",
     "exception": false,
     "start_time": "2024-09-20T18:16:37.588255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cluster_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate unsupervised clustering accuracy. Requires scikit-learn installed\n",
    "\n",
    "    # Arguments\n",
    "        y_true: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
    "    return w[row_ind, col_ind].sum() * 1.0 / y_pred.size\n",
    "\n",
    "\n",
    "def cluster_purity(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering purity\n",
    "\n",
    "    # Arguments\n",
    "        y_true: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "\n",
    "    # Return\n",
    "        purity, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    label_mapping = w.argmax(axis=1)\n",
    "    y_pred_voted = y_pred.copy()\n",
    "    for i in range(y_pred.size):\n",
    "        y_pred_voted[i] = label_mapping[y_pred[i]]\n",
    "    return metrics.accuracy_score(y_pred_voted, y_true)\n",
    "\n",
    "\n",
    "def roc_auc(y_true, q_pred, n_classes):\n",
    "    \"\"\"\n",
    "    Calculate area under ROC curve (ROC AUC)\n",
    "    WARNING: DO NOT USE, MAY CONTAIN ERRORS\n",
    "    TODO: CHECK IT!\n",
    "\n",
    "    # Arguments\n",
    "        y_true: true labels, numpy.array with shape `(n_samples,)`\n",
    "        q_pred: predicted probabilities, numpy.array with shape `(n_samples,)`\n",
    "\n",
    "    # Return\n",
    "        ROC AUC score, in [0,1]\n",
    "    \"\"\"\n",
    "    if n_classes == 2:  # binary ROC AUC\n",
    "        auc = max(metrics.roc_auc_score(y_true, q_pred[:, 1]), metrics.roc_auc_score(y_true, q_pred[:, 0]))\n",
    "    else:  # micro-averaged ROC AUC (multiclass)\n",
    "        fpr, tpr, _ = metrics.roc_curve(label_binarize(y_true, classes=np.unique(y_true)).ravel(), q_pred.ravel())\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcc65f54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:37.663935Z",
     "iopub.status.busy": "2024-09-20T18:16:37.663191Z",
     "iopub.status.idle": "2024-09-20T18:16:37.694005Z",
     "shell.execute_reply": "2024-09-20T18:16:37.692599Z"
    },
    "papermill": {
     "duration": 0.052549,
     "end_time": "2024-09-20T18:16:37.697117",
     "exception": false,
     "start_time": "2024-09-20T18:16:37.644568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TSClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, timesteps, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "        dist_metric: distance metric between sequences used in similarity kernel ('eucl', 'cir', 'cor' or 'acf').\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(n_samples, timesteps, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, dist_metric='eucl', **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(TSClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.dist_metric = dist_metric\n",
    "        self.initial_weights = weights\n",
    "        self.clusters = None\n",
    "        self.built = False\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_shape[1]))\n",
    "        self.clusters = self.add_weight(shape=(self.n_clusters, input_shape[1]), initializer='glorot_uniform', name='cluster_centers')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    '''def call(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Student t-distribution kernel, probability of assigning encoded sequence i to cluster k.\n",
    "            q_{ik} = (1 + dist(z_i, m_k)^2)^{-1} / normalization.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: encoded input sequences, shape=(n_samples, timesteps, n_features)\n",
    "        Return:\n",
    "            q: soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        if self.dist_metric == 'eucl':\n",
    "            distance = K.sum(K.sqrt(K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=1)), axis=-1)\n",
    "        elif self.dist_metric == 'cid':\n",
    "            ce_x = K.sqrt(K.sum(K.square(inputs[:, 1:, :] - inputs[:, :-1, :]), axis=1))  # shape (n_samples, n_features)\n",
    "            ce_w = K.sqrt(K.sum(K.square(self.clusters[:, 1:, :] - self.clusters[:, :-1, :]), axis=1))  # shape (n_clusters, n_features)\n",
    "            ce = K.maximum(K.expand_dims(ce_x, axis=1), ce_w) / K.minimum(K.expand_dims(ce_x, axis=1), ce_w)  # shape (n_samples, n_clusters, n_features)\n",
    "            ed = K.sqrt(K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2))  # shape (n_samples, n_clusters, n_features)\n",
    "            distance = K.sum(ed * ce, axis=-1)  # shape (n_samples, n_clusters)\n",
    "        elif self.dist_metric == 'cor':\n",
    "            inputs_norm = (inputs - K.expand_dims(K.mean(inputs, axis=1), axis=1)) / K.expand_dims(K.std(inputs, axis=1), axis=1)  # shape (n_samples, timesteps, n_features)\n",
    "            clusters_norm = (self.clusters - K.expand_dims(K.mean(self.clusters, axis=1), axis=1)) / K.expand_dims(K.std(self.clusters, axis=1), axis=1)  # shape (n_clusters, timesteps, n_features)\n",
    "            pcc = K.mean(K.expand_dims(inputs_norm, axis=1) * clusters_norm, axis=2)  # Pearson correlation coefficients\n",
    "            distance = K.sum(K.sqrt(2.0 * (1.0 - pcc)), axis=-1)  # correlation-based similarities, shape (n_samples, n_clusters)\n",
    "        elif self.dist_metric == 'acf':\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise ValueError('Available distances are eucl, cid, cor and acf!')\n",
    "        q = 1.0 / (1.0 + K.square(distance) / self.alpha)\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q'''\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Student t-distribution kernel, probability of assigning encoded sequence i to cluster k.\n",
    "            q_{ik} = (1 + dist(z_i, m_k)^2)^{-1} / normalization.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: encoded input sequences, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        inputs_expanded = K.expand_dims(inputs, axis=1)  # shape=(n_samples, 1, n_features)\n",
    "        clusters_expanded = K.expand_dims(self.clusters, axis=0)  # shape=(1, n_clusters, n_features)\n",
    "\n",
    "        if self.dist_metric == 'eucl':\n",
    "            distance = K.sqrt(K.sum(K.square(inputs_expanded - clusters_expanded), axis=-1))\n",
    "        elif self.dist_metric == 'cid':\n",
    "            ce_x = K.sqrt(K.sum(K.square(inputs_expanded - K.expand_dims(inputs[:, :-1], axis=2)), axis=-1))  # shape (n_samples, timesteps)\n",
    "            ce_w = K.sqrt(K.sum(K.square(clusters_expanded - K.expand_dims(self.clusters[:, :-1], axis=1)), axis=-1))  # shape (n_clusters, timesteps)\n",
    "            ce = K.maximum(K.expand_dims(ce_x, axis=1), ce_w) / K.minimum(K.expand_dims(ce_x, axis=1), ce_w)  # shape (n_samples, n_clusters, timesteps)\n",
    "            ed = K.sqrt(K.sum(K.square(inputs_expanded - clusters_expanded), axis=-1))  # shape (n_samples, n_clusters)\n",
    "            distance = K.sum(ed * ce, axis=-1)  # shape (n_samples, n_clusters)\n",
    "        elif self.dist_metric == 'cor':\n",
    "            inputs_norm = (inputs - K.mean(inputs, axis=1, keepdims=True)) / K.std(inputs, axis=1, keepdims=True)  # shape (n_samples, n_features)\n",
    "            clusters_norm = (self.clusters - K.mean(self.clusters, axis=1, keepdims=True)) / K.std(self.clusters, axis=1, keepdims=True)  # shape (n_clusters, n_features)\n",
    "            pcc = K.mean(K.expand_dims(inputs_norm, axis=1) * K.expand_dims(clusters_norm, axis=0), axis=-1)  # Pearson correlation coefficients\n",
    "            distance = K.sqrt(2.0 * (1.0 - pcc))  # correlation-based similarities, shape (n_samples, n_clusters)\n",
    "        elif self.dist_metric == 'acf':\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise ValueError('Available distances are eucl, cid, cor and acf!')\n",
    "\n",
    "        q = 1.0 / (1.0 + K.square(distance) / self.alpha)\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 3\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters, 'dist_metric': self.dist_metric}\n",
    "        base_config = super(TSClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c06d6859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:37.733105Z",
     "iopub.status.busy": "2024-09-20T18:16:37.731942Z",
     "iopub.status.idle": "2024-09-20T18:16:37.871595Z",
     "shell.execute_reply": "2024-09-20T18:16:37.870047Z"
    },
    "papermill": {
     "duration": 0.16141,
     "end_time": "2024-09-20T18:16:37.874821",
     "exception": false,
     "start_time": "2024-09-20T18:16:37.713411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Series</th>\n",
       "      <th>N</th>\n",
       "      <th>NF</th>\n",
       "      <th>Category</th>\n",
       "      <th>Starting Year</th>\n",
       "      <th>Starting Month</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>...</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N1402</td>\n",
       "      <td>68</td>\n",
       "      <td>18</td>\n",
       "      <td>MICRO</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N1403</td>\n",
       "      <td>68</td>\n",
       "      <td>18</td>\n",
       "      <td>MICRO</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>1920.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N1404</td>\n",
       "      <td>68</td>\n",
       "      <td>18</td>\n",
       "      <td>MICRO</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>4860.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N1405</td>\n",
       "      <td>68</td>\n",
       "      <td>18</td>\n",
       "      <td>MICRO</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>180.0</td>\n",
       "      <td>940.0</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N1406</td>\n",
       "      <td>68</td>\n",
       "      <td>18</td>\n",
       "      <td>MICRO</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1550.0</td>\n",
       "      <td>4450.0</td>\n",
       "      <td>3050.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Series   N  NF      Category  Starting Year  Starting Month       1       2  \\\n",
       "0  N1402  68  18  MICRO                  1990               1  2640.0  2640.0   \n",
       "1  N1403  68  18  MICRO                  1990               1  1680.0  1920.0   \n",
       "2  N1404  68  18  MICRO                  1990               1  1140.0   720.0   \n",
       "3  N1405  68  18  MICRO                  1990               1   180.0   940.0   \n",
       "4  N1406  68  18  MICRO                  1990               1  2000.0  1550.0   \n",
       "\n",
       "        3       4  ...  135  136  137  138  139  140  141  142  143  144  \n",
       "0  2160.0  4200.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1   120.0  1080.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "2  4860.0  1200.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "3  2040.0   800.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "4  4450.0  3050.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 150 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3 = pd.read_csv(\"/kaggle/input/m3monthdataset/M3Month.csv\")\n",
    "m3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b8f5330",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:37.912244Z",
     "iopub.status.busy": "2024-09-20T18:16:37.911158Z",
     "iopub.status.idle": "2024-09-20T18:16:37.937434Z",
     "shell.execute_reply": "2024-09-20T18:16:37.935908Z"
    },
    "papermill": {
     "duration": 0.048099,
     "end_time": "2024-09-20T18:16:37.940431",
     "exception": false,
     "start_time": "2024-09-20T18:16:37.892332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MICRO       \n",
      "INDUSTRY    \n",
      "MACRO       \n",
      "FINANCE     \n",
      "DEMOGRAPHIC \n",
      "OTHER       \n"
     ]
    }
   ],
   "source": [
    "class_dataframes = {}  # A dictionary to store DataFrames for each class\n",
    "\n",
    "# Iterate over unique class labels in the 'category' column\n",
    "for class_label in m3['Category'].unique():\n",
    "    print(class_label)\n",
    "    # Filter the original DataFrame for rows with the current class label\n",
    "    class_df = m3[m3['Category'] == class_label]\n",
    "    class_label = class_label.replace(\" \", \"\")\n",
    "    # Store the class-specific DataFrame in the dictionary with the class label as the key\n",
    "    class_dataframes[class_label] = class_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7725354b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:37.977323Z",
     "iopub.status.busy": "2024-09-20T18:16:37.976093Z",
     "iopub.status.idle": "2024-09-20T18:16:37.985331Z",
     "shell.execute_reply": "2024-09-20T18:16:37.983841Z"
    },
    "papermill": {
     "duration": 0.030823,
     "end_time": "2024-09-20T18:16:37.988291",
     "exception": false,
     "start_time": "2024-09-20T18:16:37.957468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    ts_train = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        temp=np.array(list(data.iloc[i][6:].dropna()))\n",
    "        temp = temp[:-18]\n",
    "        temp=temp.reshape(1,len(temp),1)\n",
    "        temp2 = TimeSeriesScalerMeanVariance().fit_transform(temp)\n",
    "        ts_train.append(temp2.reshape(-1,1))\n",
    "\n",
    "    return ts_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e3bf8be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:38.025216Z",
     "iopub.status.busy": "2024-09-20T18:16:38.024693Z",
     "iopub.status.idle": "2024-09-20T18:16:38.110792Z",
     "shell.execute_reply": "2024-09-20T18:16:38.109460Z"
    },
    "papermill": {
     "duration": 0.108864,
     "end_time": "2024-09-20T18:16:38.114012",
     "exception": false,
     "start_time": "2024-09-20T18:16:38.005148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DTC:\n",
    "    def __init__(self, n_clusters, input_dim, timesteps,max_dim=None,dataset_name=None,\n",
    "                 alpha=1.0, dist_metric='eucl', cluster_init='kmeans', heatmap=False):\n",
    "        self.n_clusters = 2\n",
    "        self.input_dim = None\n",
    "        self.max_dim=max_dim\n",
    "        self.dataset_name=dataset_name\n",
    "        self.timesteps = 791\n",
    "        self.latent_shape = 16\n",
    "        self.alpha = alpha\n",
    "        self.dist_metric = dist_metric\n",
    "        self.cluster_init = cluster_init\n",
    "        self.heatmap = heatmap\n",
    "        self.pretrained = False\n",
    "        self.alpha2 = 0.8\n",
    "        self.learning_rate = None\n",
    "        self.optimizer = keras.optimizers.Adam()\n",
    "        self.model = self.autoencoder = self.encoder = self.decoder = self.predmodel =  None\n",
    "        self.autoencoderNeighbour = self.encoderNeighbour = self.decoderNeighbour =  None\n",
    "        self.FeatureWeight = None\n",
    "        self.a = 1.0\n",
    "        self.b = 1.0\n",
    "        self.initial_weights = None\n",
    "        self.a_b = [1.93, 0.79]\n",
    "        self.KNN = None\n",
    "        self.b1 = -0.9\n",
    "        self.b2 = self.b1/2\n",
    "\n",
    "        \n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Create DTC model\n",
    "        \"\"\"\n",
    "        # Create AE models\n",
    "        self.FeatureWeight = np.ones((self.n_clusters, self.latent_shape),dtype='float32')/self.latent_shape\n",
    "        self.autoencoder, self.encoder, self.decoder = autoencoder(series_len=self.max_dim)\n",
    "        self.autoencoderNeighbour , self.encoderNeighbour , self.decoderNeighbour = autoencoderNeighbour(series_len=self.max_dim)\n",
    "        clustering_layer = TSClusteringLayer(self.n_clusters,\n",
    "                                             alpha=self.alpha,\n",
    "                                             dist_metric=self.dist_metric,\n",
    "                                             name='TSClustering')(self.encoderNeighbour.output)\n",
    "        # Create DTC model\n",
    "        self.model = Model(inputs=[self.autoencoderNeighbour.input],\n",
    "                               outputs=[self.autoencoderNeighbour.output, clustering_layer])\n",
    "        self.KNN = NearestNeighbors(n_neighbors=3,metric='euclidean')\n",
    "\n",
    "    @property\n",
    "    def cluster_centers_(self):\n",
    "        \"\"\"\n",
    "        Returns cluster centers\n",
    "        \"\"\"\n",
    "        return self.model.get_layer(name='TSClustering').get_weights()[0]\n",
    "    \n",
    "    def reconstruction_loss(self, data, reconstruction):\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.mean_squared_error(data, reconstruction), axis=(0, 1)\n",
    "                )\n",
    "            )\n",
    "        return reconstruction_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def weighted_kld(loss_weight):\n",
    "        \"\"\"\n",
    "        Custom KL-divergence loss with a variable weight parameter\n",
    "        \"\"\"\n",
    "        def loss(y_true, y_pred):\n",
    "            return loss_weight * kullback_leibler_divergence(y_true, y_pred)\n",
    "        return loss\n",
    "    \n",
    "    def loss_kld(self, y_true, y_pred):\n",
    "        return kullback_leibler_divergence(y_true, y_pred)\n",
    "    \n",
    "    \n",
    "    def loss_kld(self, y_true, y_pred):\n",
    "        return kullback_leibler_divergence(y_true, y_pred)\n",
    "    \n",
    "    def categorical_cross_entropy(self, y_true, y_pred):\n",
    "        cce = CategoricalCrossentropy()\n",
    "        return cce(y_true, y_pred)\n",
    "        \n",
    "    def on_epoch_end(self, epoch):\n",
    "        \"\"\"\n",
    "        Update heatmap loss weight on epoch end\n",
    "        \"\"\"\n",
    "        if epoch > self.finetune_heatmap_at_epoch:\n",
    "            K.set_value(self.heatmap_loss_weight, self.final_heatmap_loss_weight)\n",
    "\n",
    "    def compile(self, gamma, optimizer, initial_heatmap_loss_weight=None, final_heatmap_loss_weight=None):\n",
    "        \"\"\"\n",
    "        Compile DTC model\n",
    "\n",
    "        # Arguments\n",
    "            gamma: coefficient of TS clustering loss\n",
    "            optimizer: optimization algorithm\n",
    "            initial_heatmap_loss_weight (optional): initial weight of heatmap loss vs clustering loss\n",
    "            final_heatmap_loss_weight (optional): final weight of heatmap loss vs clustering loss (heatmap finetuning)\n",
    "        \"\"\"\n",
    "        if self.heatmap:\n",
    "            self.initial_heatmap_loss_weight = initial_heatmap_loss_weight\n",
    "            self.final_heatmap_loss_weight = final_heatmap_loss_weight\n",
    "            self.heatmap_loss_weight = K.variable(self.initial_heatmap_loss_weight)\n",
    "            self.model.compile(loss=['mse', DTC.weighted_kld(1.0 - self.heatmap_loss_weight), DTC.weighted_kld(self.heatmap_loss_weight)],\n",
    "                               loss_weights=[1.0, gamma, gamma],\n",
    "                               optimizer=optimizer)\n",
    "        else:\n",
    "            self.model.compile(loss=['mse', 'CategoricalCrossentropy'],\n",
    "                               loss_weights=[1.0, gamma],\n",
    "                               optimizer=optimizer)\n",
    "\n",
    "    def load_weights(self, weights_path):\n",
    "        \"\"\"\n",
    "        Load pre-trained weights of DTC model\n",
    "\n",
    "        # Arguments\n",
    "            weight_path: path to weights file (.h5)\n",
    "        \"\"\"\n",
    "        self.model.load_weights(weights_path)\n",
    "        self.pretrained = True\n",
    "\n",
    "    def load_ae_weights(self, ae_weights_path):\n",
    "        \"\"\"\n",
    "        Load pre-trained weights of AE\n",
    "\n",
    "        # Arguments\n",
    "            ae_weight_path: path to weights file (.h5)\n",
    "        \"\"\"\n",
    "        self.autoencoder.load_weights(ae_weights_path)\n",
    "        self.pretrained = True\n",
    "        \n",
    "    def load_ae_neighbour_weights(self):\n",
    "        input = self.autoencoder.get_layer(name='input').get_weights()\n",
    "        encoder_0 = self.autoencoder.get_layer(name='encoded_lstm1').get_weights()\n",
    "        encoder_1 = self.autoencoder.get_layer(name='encoded_lstm2').get_weights()\n",
    "        encoder_2 = self.autoencoder.get_layer(name='encoded_lstm3').get_weights()\n",
    "        atten_encode = self.autoencoder.get_layer(name='encoded_atten').get_weights()\n",
    "        dense_encode = self.autoencoder.get_layer(name=\"encoded_dense\").get_weights()\n",
    "        decoder_0 = self.autoencoder.get_layer(name='decoded_lstm1').get_weights()\n",
    "        decoder_1 = self.autoencoder.get_layer(name='decoded_lstm2').get_weights()\n",
    "        decoder_2 = self.autoencoder.get_layer(name='decoded_lstm3').get_weights()\n",
    "        decoder_atten = self.autoencoder.get_layer(name='decoded_atten').get_weights()\n",
    "        decoder_timeD = self.autoencoder.get_layer(name=\"decoded_timeD\").get_weights()\n",
    "\n",
    "        # Image weight\n",
    "        self.autoencoderNeighbour.get_layer(name='input').set_weights(input)\n",
    "        self.autoencoderNeighbour.get_layer(name='lstm_input_256').set_weights(encoder_0)\n",
    "        self.autoencoderNeighbour.get_layer(name='lstm_input_128').set_weights(encoder_1)\n",
    "        self.autoencoderNeighbour.get_layer(name='lstm_input_64').set_weights(encoder_2)\n",
    "        self.autoencoderNeighbour.get_layer(name='attention_input').set_weights(atten_encode)\n",
    "        self.autoencoderNeighbour.get_layer(name='dense_input').set_weights(dense_encode)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        self.autoencoderNeighbour.get_layer(name='decoded_lstm1').set_weights(decoder_0)\n",
    "        self.autoencoderNeighbour.get_layer(name='decoded_lstm2').set_weights(decoder_1)\n",
    "        self.autoencoderNeighbour.get_layer(name='decoded_lstm3').set_weights(decoder_2)\n",
    "        self.autoencoderNeighbour.get_layer(name='decoded_atten').set_weights(decoder_atten)\n",
    "\n",
    "        self.autoencoderNeighbour.get_layer(name='lstm_Neighbour1_256').set_weights(encoder_0)\n",
    "        self.autoencoderNeighbour.get_layer(name='lstm_Neighbour1_128').set_weights(encoder_1)\n",
    "        self.autoencoderNeighbour.get_layer(name='lstm_Neighbour1_64').set_weights(encoder_2)\n",
    "        self.autoencoderNeighbour.get_layer(name='attention_Neighbour1').set_weights(decoder_atten)\n",
    "        \n",
    "        self.autoencoderNeighbour.get_layer(name='lstm_Neighbour2_256').set_weights(encoder_0)\n",
    "        self.autoencoderNeighbour.get_layer(name='lstm_Neighbour2_128').set_weights(encoder_1)\n",
    "        self.autoencoderNeighbour.get_layer(name='lstm_Neighbour2_64').set_weights(encoder_2)\n",
    "        self.autoencoderNeighbour.get_layer(name='attention_Neighbour2').set_weights(decoder_atten)\n",
    "\n",
    "        self.autoencoderNeighbour.get_layer(name='Neighbour1_input').set_weights(input)\n",
    "        self.autoencoderNeighbour.get_layer(name='Neighbour2_input').set_weights(input)\n",
    "\n",
    "        # Image weight (encoder)\n",
    "        self.encoderNeighbour.get_layer(name='input').set_weights(input)\n",
    "        self.encoderNeighbour.get_layer(name='lstm_input_256').set_weights(encoder_0)\n",
    "        self.encoderNeighbour.get_layer(name='lstm_input_128').set_weights(encoder_1)\n",
    "        self.encoderNeighbour.get_layer(name='lstm_input_64').set_weights(encoder_2)\n",
    "        self.encoderNeighbour.get_layer(name='attention_input').set_weights(atten_encode)\n",
    "        \n",
    "        self.encoderNeighbour.get_layer(name='lstm_Neighbour1_256').set_weights(encoder_0)\n",
    "        self.encoderNeighbour.get_layer(name='lstm_Neighbour1_128').set_weights(encoder_1)\n",
    "        self.encoderNeighbour.get_layer(name='lstm_Neighbour1_64').set_weights(encoder_2)\n",
    "        self.encoderNeighbour.get_layer(name='attention_Neighbour1').set_weights(decoder_atten)\n",
    "        \n",
    "        self.encoderNeighbour.get_layer(name='lstm_Neighbour2_256').set_weights(encoder_0)\n",
    "        self.encoderNeighbour.get_layer(name='lstm_Neighbour2_128').set_weights(encoder_1)\n",
    "        self.encoderNeighbour.get_layer(name='lstm_Neighbour2_64').set_weights(encoder_2)\n",
    "        self.encoderNeighbour.get_layer(name='attention_Neighbour2').set_weights(decoder_atten)\n",
    "\n",
    "        self.encoderNeighbour.get_layer(name='Neighbour1_input').set_weights(input)\n",
    "        self.encoderNeighbour.get_layer(name='Neighbour2_input').set_weights(input)\n",
    "\n",
    "    def dist(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Compute distance between two multivariate time series using chosen distance metric\n",
    "\n",
    "        # Arguments\n",
    "            x1: first input (np array)\n",
    "            x2: second input (np array)\n",
    "        # Return\n",
    "            distance\n",
    "        \"\"\"\n",
    "        if self.dist_metric == 'eucl':\n",
    "            return tsdistances.eucl(x1, x2)\n",
    "        elif self.dist_metric == 'cid':\n",
    "            return tsdistances.cid(x1, x2)\n",
    "        elif self.dist_metric == 'cor':\n",
    "            return tsdistances.cor(x1, x2)\n",
    "        elif self.dist_metric == 'acf':\n",
    "            return tsdistances.acf(x1, x2)\n",
    "        else:\n",
    "            raise ValueError('Available distances are eucl, cid, cor and acf!')\n",
    "\n",
    "    def init_cluster_weights(self, X):\n",
    "        \"\"\"\n",
    "        Initialize with complete-linkage hierarchical clustering or k-means.\n",
    "\n",
    "        # Arguments\n",
    "            X: numpy array containing training set or batch\n",
    "        \"\"\"\n",
    "        assert(self.cluster_init in ['hierarchical', 'kmeans'])\n",
    "        print('Initializing cluster...')\n",
    "\n",
    "        features = self.encode(X)\n",
    "\n",
    "        if self.cluster_init == 'hierarchical':\n",
    "            if self.dist_metric == 'eucl':  # use AgglomerativeClustering off-the-shelf\n",
    "                hc = AgglomerativeClustering(n_clusters=self.n_clusters,\n",
    "                                             affinity='euclidean',\n",
    "                                             linkage='complete').fit(features.reshape(features.shape[0], -1))\n",
    "            else:  # compute distance matrix using dist\n",
    "                d = np.zeros((features.shape[0], features.shape[0]))\n",
    "                for i in range(features.shape[0]):\n",
    "                    for j in range(i):\n",
    "                        d[i, j] = d[j, i] = self.dist(features[i], features[j])\n",
    "                hc = AgglomerativeClustering(n_clusters=self.n_clusters,\n",
    "                                             affinity='precomputed',\n",
    "                                             linkage='complete').fit(d)\n",
    "            # compute centroid\n",
    "            cluster_centers = np.array([features[hc.labels_ == c].mean(axis=0) for c in range(self.n_clusters)])\n",
    "        elif self.cluster_init == 'kmeans':\n",
    "            # fit k-means on flattened features\n",
    "            km = KMeans(n_clusters=self.n_clusters, n_init=10).fit(features.reshape(features.shape[0], -1))\n",
    "            cluster_centers = km.cluster_centers_.reshape(self.n_clusters, features.shape[1])\n",
    "\n",
    "        self.model.get_layer(name='TSClustering').set_weights([cluster_centers])\n",
    "        print('Done!')\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encoding function. Extract latent features from hidden layer\n",
    "\n",
    "        # Arguments\n",
    "            x: data point\n",
    "        # Return\n",
    "            encoded (latent) data point\n",
    "        \"\"\"\n",
    "        return self.encoder.predict(x)\n",
    "\n",
    "    def decode(self, x):\n",
    "        \"\"\"\n",
    "        Decoding function. Decodes encoded sequence from latent space.\n",
    "\n",
    "        # Arguments\n",
    "            x: encoded (latent) data point\n",
    "        # Return\n",
    "            decoded data point\n",
    "        \"\"\"\n",
    "        return self.decoder.predict(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict cluster assignment.\n",
    "\n",
    "        \"\"\"\n",
    "        q = self.model.predict(x, verbose=0)[1]\n",
    "        return q.argmax(axis=1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def target_distribution(q):  # target distribution p which enhances the discrimination of soft label q\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def predict_heatmap(self, x):\n",
    "        \"\"\"\n",
    "        Produces TS clustering heatmap from input sequence.\n",
    "\n",
    "        # Arguments\n",
    "            x: data point\n",
    "        # Return\n",
    "            heatmap\n",
    "        \"\"\"\n",
    "        return self.heatmap_model.predict(x, verbose=0)\n",
    "\n",
    "    def pretrain(self, X,\n",
    "                 optimizer='adam',\n",
    "                 epochs=10,\n",
    "                 batch_size=64,\n",
    "                 save_dir='results/tmp',\n",
    "                 verbose=1):\n",
    "        \"\"\"\n",
    "        Pre-train the autoencoder using only MSE reconstruction loss\n",
    "        Saves weights in h5 format.\n",
    "\n",
    "        # Arguments\n",
    "            X: training set\n",
    "            optimizer: optimization algorithm\n",
    "            epochs: number of pre-training epochs\n",
    "            batch_size: training batch size\n",
    "            save_dir: path to existing directory where weights will be saved\n",
    "        \"\"\"\n",
    "        print('Pretraining...')\n",
    "        self.autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        # Begin pretraining\n",
    "        #t0 = time()\n",
    "        self.autoencoder.fit(X, X, batch_size=batch_size, epochs=epochs)\n",
    "        #print('Pretraining time: ', time() - t0)\n",
    "        self.autoencoder.save_weights('{}/ae_weights-epoch{}.h5'.format(save_dir, epochs))\n",
    "        print('Pretrained weights are saved to {}/ae_weights-epoch{}.h5'.format(save_dir, epochs))\n",
    "        self.pretrained = True\n",
    "        \n",
    "    def mutate_clustering(self, clustering, mutation_rate=0.3):\n",
    "        new_clustering = clustering.copy()\n",
    "        n_samples = len(clustering)\n",
    "        for i in range(n_samples):\n",
    "            if np.random.rand() < mutation_rate:\n",
    "                new_clustering[i] = 1 - new_clustering[i]\n",
    "        return new_clustering\n",
    "\n",
    "    def crossover_clustering(self, clustering1, clustering2, crossover_rate=0.3):\n",
    "        n_samples = len(clustering1)\n",
    "        new_clustering = clustering1.copy()\n",
    "        for i in range(n_samples):\n",
    "            if np.random.rand() < crossover_rate:\n",
    "                new_clustering[i] = clustering2[i]\n",
    "        return new_clustering\n",
    "    \n",
    "    def save_metrics(self, metrics, file_path='metrics.csv'):\n",
    "        \"\"\"\n",
    "        Save training metrics to a CSV file.\n",
    "\n",
    "        # Arguments\n",
    "            metrics: List of dictionaries containing metrics for each epoch\n",
    "            file_path: Path to the CSV file where metrics will be saved\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(metrics)\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f'Metrics saved to {file_path}')\n",
    "    \n",
    "    def fit(self, X_train, y_train=None, X_val=None, y_val=None,\n",
    "        epochs=10,\n",
    "        eval_epochs=10,\n",
    "        save_epochs=10,\n",
    "        batch_size=64,\n",
    "        tol=0.001,\n",
    "        patience=5,\n",
    "        finetune_heatmap_at_epoch=8,\n",
    "        save_dir='results/tmp',\n",
    "        mutation_rate=0.01,\n",
    "        crossover_rate=0.5,\n",
    "        num_iterations=5):\n",
    "        \n",
    "    \n",
    "            if not self.pretrained:\n",
    "                print('Autoencoder was not pre-trained!')\n",
    "\n",
    "\n",
    "            patience_cnt = 0\n",
    "            index = 0\n",
    "            print('Training for {} epochs.\\nEvaluating every {} and saving model every {} epochs.'.format(epochs, eval_epochs, save_epochs))\n",
    "            index_array = np.arange(X_train.shape[0])\n",
    "            for epoch in range(epochs):\n",
    "                # Initial clustering\n",
    "                if epoch == 0:\n",
    "                    q = self.model.predict([X_train, X_train, X_train])[1]\n",
    "                    extract = Model(inputs = self.model.input, outputs = self.model.get_layer('add_inputs').output)\n",
    "                    z = extract.predict([X_train, X_train, X_train])\n",
    "                else:\n",
    "                    self.KNN.fit(z)\n",
    "                    _,Neighbors_list=self.KNN.kneighbors(z)\n",
    "                    q = self.model.predict([X_train,X_train[Neighbors_list[:,1]], X_train[Neighbors_list[:,2]]])[1]\n",
    "\n",
    "                p = DTC.target_distribution(q)\n",
    "                p_pred = p.argmax(axis=1)\n",
    "                reliableindex, _ = np.unique(np.unique(np.where(np.sort(q,axis=1)[:,-1] - np.sort(q,axis=1)[:,-2]>=self.b2), np.where(np.max(q,axis=1)>=self.b1)),\n",
    "                           np.unique(np.where(np.sort(p,axis=1)[:,-1] - np.sort(p,axis=1)[:,-2]>=self.b2), np.where(np.max(p,axis=1)>=self.b1)))\n",
    "\n",
    "                print('Number of reliable samples:', len(q[reliableindex]) )\n",
    "                print('Number of unreliable samples:', len(q)-len(q[reliableindex]) )\n",
    "#                 print(\"my\",new_clustering)\n",
    "                Number_Unreliable = len(q)-len(q[reliableindex])\n",
    "                Number_reliable = len(q[reliableindex])\n",
    "                with tf.GradientTape() as tape:\n",
    "                    idx = index_array[index:reliableindex.shape[0]]\n",
    "                    self.KNN.fit(z[reliableindex[idx]])\n",
    "                    _,Neighbors_list=self.KNN.kneighbors(z[reliableindex[idx]])\n",
    "                    outputs = self.model([X_train[reliableindex[idx]], X_train[Neighbors_list[:,1]], X_train[Neighbors_list[:,2]]], training=True)\n",
    "                    reconstruction_output, clustering_output = outputs\n",
    "                    reconstruction_loss = self.reconstruction_loss(X_train[reliableindex[idx]], reconstruction_output)\n",
    "                    clustering_loss = self.categorical_cross_entropy(p[reliableindex[idx]], clustering_output)\n",
    "                    total_loss = tf.reduce_sum(reconstruction_loss) + tf.reduce_sum(clustering_loss)\n",
    "                    total_loss = tf.reduce_sum(total_loss)\n",
    "                    print(\"reconstruction_loss: \",tf.reduce_sum(reconstruction_loss))\n",
    "                    print(\"clustering_loss: \",tf.reduce_sum(clustering_loss))\n",
    "\n",
    "                            \n",
    "                grads = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "                num_elements = len(reliableindex[idx])\n",
    "                n1 = Neighbors_list[:,1]\n",
    "                n1 = n1[:num_elements]\n",
    "                n2 = Neighbors_list[:,2]\n",
    "                n2 = n2[:num_elements]\n",
    "                z[idx] = extract.predict([X_train[reliableindex[idx]], X_train[n1], X_train[n2]])\n",
    "                print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "                print(\"Total Loss: \", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a452a14e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:38.150547Z",
     "iopub.status.busy": "2024-09-20T18:16:38.150015Z",
     "iopub.status.idle": "2024-09-20T18:16:40.350242Z",
     "shell.execute_reply": "2024-09-20T18:16:40.348587Z"
    },
    "papermill": {
     "duration": 2.222288,
     "end_time": "2024-09-20T18:16:40.353449",
     "exception": false,
     "start_time": "2024-09-20T18:16:38.131161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_MICRO = preprocessing(class_dataframes['MICRO'])\n",
    "dataset_INDUSTRY = preprocessing(class_dataframes['INDUSTRY'])\n",
    "dataset_MACRO = preprocessing(class_dataframes['MACRO'])\n",
    "dataset_FINANCE = preprocessing(class_dataframes['FINANCE'])\n",
    "dataset_DEMOGRAPHIC = preprocessing(class_dataframes['DEMOGRAPHIC'])\n",
    "dataset_OTHER = preprocessing(class_dataframes['OTHER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f33f19a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:40.391430Z",
     "iopub.status.busy": "2024-09-20T18:16:40.390723Z",
     "iopub.status.idle": "2024-09-20T18:16:40.400159Z",
     "shell.execute_reply": "2024-09-20T18:16:40.398516Z"
    },
    "papermill": {
     "duration": 0.032042,
     "end_time": "2024-09-20T18:16:40.403478",
     "exception": false,
     "start_time": "2024-09-20T18:16:40.371436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_seq_length_MICRO = max(len(seq) for seq in dataset_MICRO)\n",
    "max_seq_length_INDUSTRY = max(len(seq) for seq in dataset_INDUSTRY)\n",
    "max_seq_length_MACRO = max(len(seq) for seq in dataset_MACRO)\n",
    "max_seq_length_FINANCE = max(len(seq) for seq in dataset_FINANCE)\n",
    "max_seq_length_DEMOGRAPHIC = max(len(seq) for seq in dataset_DEMOGRAPHIC)\n",
    "max_seq_length_OTHER = max(len(seq) for seq in dataset_OTHER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f2ce817",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:40.441707Z",
     "iopub.status.busy": "2024-09-20T18:16:40.441200Z",
     "iopub.status.idle": "2024-09-20T18:16:40.460830Z",
     "shell.execute_reply": "2024-09-20T18:16:40.459463Z"
    },
    "papermill": {
     "duration": 0.043163,
     "end_time": "2024-09-20T18:16:40.464541",
     "exception": false,
     "start_time": "2024-09-20T18:16:40.421378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "padded_sequences_MICRO = tf.keras.preprocessing.sequence.pad_sequences(dataset_MICRO, maxlen=max_seq_length_MICRO, padding='post', dtype='float32')\n",
    "padded_sequences_INDUSTRY = tf.keras.preprocessing.sequence.pad_sequences(dataset_INDUSTRY, maxlen=max_seq_length_INDUSTRY, padding='post', dtype='float32')\n",
    "padded_sequences_MACRO = tf.keras.preprocessing.sequence.pad_sequences(dataset_MACRO, maxlen=max_seq_length_MACRO, padding='post', dtype='float32')\n",
    "padded_sequences_FINANCE = tf.keras.preprocessing.sequence.pad_sequences(dataset_FINANCE, maxlen=max_seq_length_FINANCE, padding='post', dtype='float32')\n",
    "padded_sequences_DEMOGRAPHIC = tf.keras.preprocessing.sequence.pad_sequences(dataset_DEMOGRAPHIC, maxlen=max_seq_length_DEMOGRAPHIC, padding='post', dtype='float32')\n",
    "padded_sequences_OTHER = tf.keras.preprocessing.sequence.pad_sequences(dataset_OTHER, maxlen=max_seq_length_OTHER, padding='post', dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b026d02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:40.604954Z",
     "iopub.status.busy": "2024-09-20T18:16:40.603450Z",
     "iopub.status.idle": "2024-09-20T18:16:40.613219Z",
     "shell.execute_reply": "2024-09-20T18:16:40.611884Z"
    },
    "papermill": {
     "duration": 0.032476,
     "end_time": "2024-09-20T18:16:40.616008",
     "exception": false,
     "start_time": "2024-09-20T18:16:40.583532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reshaped_array_MICRO = padded_sequences_MICRO.reshape(padded_sequences_MICRO.shape[0], 1, padded_sequences_MICRO.shape[1])\n",
    "reshaped_array_INDUSTRY = padded_sequences_INDUSTRY.reshape(padded_sequences_INDUSTRY.shape[0], 1, padded_sequences_INDUSTRY.shape[1])\n",
    "reshaped_array_MACRO = padded_sequences_MACRO.reshape(padded_sequences_MACRO.shape[0], 1, padded_sequences_MACRO.shape[1])\n",
    "reshaped_array_FINANCE = padded_sequences_FINANCE.reshape(padded_sequences_FINANCE.shape[0], 1, padded_sequences_FINANCE.shape[1])\n",
    "reshaped_array_DEMOGRAPHIC = padded_sequences_DEMOGRAPHIC.reshape(padded_sequences_DEMOGRAPHIC.shape[0], 1, padded_sequences_DEMOGRAPHIC.shape[1])\n",
    "reshaped_array_OTHER = padded_sequences_OTHER.reshape(padded_sequences_OTHER.shape[0], 1, padded_sequences_OTHER.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57955e4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:40.653279Z",
     "iopub.status.busy": "2024-09-20T18:16:40.652458Z",
     "iopub.status.idle": "2024-09-20T18:16:40.661146Z",
     "shell.execute_reply": "2024-09-20T18:16:40.659339Z"
    },
    "papermill": {
     "duration": 0.03089,
     "end_time": "2024-09-20T18:16:40.664354",
     "exception": false,
     "start_time": "2024-09-20T18:16:40.633464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(474, 1, 108)\n",
      "(334, 1, 126)\n",
      "(312, 1, 126)\n",
      "(145, 1, 126)\n",
      "(111, 1, 120)\n",
      "(52, 1, 102)\n"
     ]
    }
   ],
   "source": [
    "print(reshaped_array_MICRO.shape)\n",
    "print(reshaped_array_INDUSTRY.shape)\n",
    "print(reshaped_array_MACRO.shape)\n",
    "print(reshaped_array_FINANCE.shape)\n",
    "print(reshaped_array_DEMOGRAPHIC.shape)\n",
    "print(reshaped_array_OTHER.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8610cc4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:40.702747Z",
     "iopub.status.busy": "2024-09-20T18:16:40.702237Z",
     "iopub.status.idle": "2024-09-20T18:16:40.708267Z",
     "shell.execute_reply": "2024-09-20T18:16:40.706743Z"
    },
    "papermill": {
     "duration": 0.028894,
     "end_time": "2024-09-20T18:16:40.711424",
     "exception": false,
     "start_time": "2024-09-20T18:16:40.682530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "# dtc = DTC(n_clusters=2,input_dim=reshaped_array_DEMOGRAPHIC.shape[-1], timesteps=reshaped_array_DEMOGRAPHIC.shape[1], max_dim=max_seq_length_DEMOGRAPHIC,dataset_name='m3-demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11c34a97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:40.751843Z",
     "iopub.status.busy": "2024-09-20T18:16:40.751417Z",
     "iopub.status.idle": "2024-09-20T18:16:40.757535Z",
     "shell.execute_reply": "2024-09-20T18:16:40.755959Z"
    },
    "papermill": {
     "duration": 0.029764,
     "end_time": "2024-09-20T18:16:40.760783",
     "exception": false,
     "start_time": "2024-09-20T18:16:40.731019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "# dtc = DTC(n_clusters=2,input_dim=reshaped_array_cif012.shape[-1], timesteps=reshaped_array_cif012.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4c91010",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:40.800017Z",
     "iopub.status.busy": "2024-09-20T18:16:40.799481Z",
     "iopub.status.idle": "2024-09-20T18:16:40.808362Z",
     "shell.execute_reply": "2024-09-20T18:16:40.806664Z"
    },
    "papermill": {
     "duration": 0.032118,
     "end_time": "2024-09-20T18:16:40.811307",
     "exception": false,
     "start_time": "2024-09-20T18:16:40.779189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocessing_tourism_hospital(data):\n",
    "    ts_train = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        temp=np.array(list(data.iloc[i][:].dropna()))\n",
    "#         temp = temp[:-18]\n",
    "        temp=temp.reshape(1,len(temp),1)\n",
    "        temp2 = TimeSeriesScalerMeanVariance().fit_transform(temp)\n",
    "        ts_train.append(temp2.reshape(-1,1))\n",
    "\n",
    "    return ts_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63042519",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:40.848499Z",
     "iopub.status.busy": "2024-09-20T18:16:40.847941Z",
     "iopub.status.idle": "2024-09-20T18:16:43.188878Z",
     "shell.execute_reply": "2024-09-20T18:16:43.187378Z"
    },
    "papermill": {
     "duration": 2.362907,
     "end_time": "2024-09-20T18:16:43.191742",
     "exception": false,
     "start_time": "2024-09-20T18:16:40.828835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(427, 1, 122)\n"
     ]
    }
   ],
   "source": [
    "dataset_tourism = pd.read_excel(\"/kaggle/input/newtsdatasets/Tourism-new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "\n",
    "dataset_tourism=dataset_tourism.iloc[:,:-8]\n",
    "dataset_tourism = preprocessing_tourism_hospital(dataset_tourism)\n",
    "max_seq_length_tourism= max(len(seq) for seq in dataset_tourism)\n",
    "padded_sequences_tourism = tf.keras.preprocessing.sequence.pad_sequences(dataset_tourism, maxlen=max_seq_length_tourism, padding='post', dtype='float32')\n",
    "reshaped_array_tourism = padded_sequences_tourism.reshape(padded_sequences_tourism.shape[0], 1, padded_sequences_tourism.shape[1])\n",
    "print(reshaped_array_tourism.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52d4f138",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:43.231043Z",
     "iopub.status.busy": "2024-09-20T18:16:43.229575Z",
     "iopub.status.idle": "2024-09-20T18:16:45.513096Z",
     "shell.execute_reply": "2024-09-20T18:16:45.511205Z"
    },
    "papermill": {
     "duration": 2.306661,
     "end_time": "2024-09-20T18:16:45.516345",
     "exception": false,
     "start_time": "2024-09-20T18:16:43.209684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(767, 1, 72)\n"
     ]
    }
   ],
   "source": [
    "dataset_hospital = pd.read_excel(\"/kaggle/input/newtsdatasets/Hospital_new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "\n",
    "dataset_hospital=dataset_hospital.iloc[:,:-12]\n",
    "dataset_hospital = preprocessing_tourism_hospital(dataset_hospital)\n",
    "max_seq_length_hospital= max(len(seq) for seq in dataset_hospital)\n",
    "padded_sequences_hospital = tf.keras.preprocessing.sequence.pad_sequences(dataset_hospital, maxlen=max_seq_length_hospital, padding='post', dtype='float32')\n",
    "reshaped_array_hospital= padded_sequences_hospital.reshape(padded_sequences_hospital.shape[0], 1, padded_sequences_hospital.shape[1])\n",
    "print(reshaped_array_hospital.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d09a9bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:45.553945Z",
     "iopub.status.busy": "2024-09-20T18:16:45.553435Z",
     "iopub.status.idle": "2024-09-20T18:16:45.801444Z",
     "shell.execute_reply": "2024-09-20T18:16:45.799895Z"
    },
    "papermill": {
     "duration": 0.269949,
     "end_time": "2024-09-20T18:16:45.804208",
     "exception": false,
     "start_time": "2024-09-20T18:16:45.534259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 1, 108)\n"
     ]
    }
   ],
   "source": [
    "dataset_cif = pd.read_excel(\"/kaggle/input/cifnewdataset/12.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "\n",
    "dataset_cif=dataset_cif.iloc[:,:-12]\n",
    "dataset_cif = preprocessing_tourism_hospital(dataset_cif)\n",
    "max_seq_length_cif= max(len(seq) for seq in dataset_cif)\n",
    "padded_sequences_cif = tf.keras.preprocessing.sequence.pad_sequences(dataset_cif, maxlen=max_seq_length_cif, padding='post', dtype='float32')\n",
    "reshaped_array_cif= padded_sequences_cif.reshape(padded_sequences_cif.shape[0], 1, padded_sequences_cif.shape[1])\n",
    "print(reshaped_array_cif.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ac3e1b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:45.842094Z",
     "iopub.status.busy": "2024-09-20T18:16:45.841534Z",
     "iopub.status.idle": "2024-09-20T18:16:45.847535Z",
     "shell.execute_reply": "2024-09-20T18:16:45.846114Z"
    },
    "papermill": {
     "duration": 0.027788,
     "end_time": "2024-09-20T18:16:45.850126",
     "exception": false,
     "start_time": "2024-09-20T18:16:45.822338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name='m3-micro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25e14c9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:45.891226Z",
     "iopub.status.busy": "2024-09-20T18:16:45.890657Z",
     "iopub.status.idle": "2024-09-20T18:16:45.902972Z",
     "shell.execute_reply": "2024-09-20T18:16:45.901666Z"
    },
    "papermill": {
     "duration": 0.037513,
     "end_time": "2024-09-20T18:16:45.905974",
     "exception": false,
     "start_time": "2024-09-20T18:16:45.868461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset_name=='cif-12':\n",
    "    padded_seq=padded_sequences_cif\n",
    "    reshaped_array=reshaped_array_cif\n",
    "    max_dim=max_seq_length_cif\n",
    "elif dataset_name=='hospital':\n",
    "    padded_seq=padded_sequences_hospital\n",
    "    reshaped_array=reshaped_array_hospital\n",
    "    max_dim=max_seq_length_hospital\n",
    "elif dataset_name=='tourism':\n",
    "    padded_seq=padded_sequences_tourism\n",
    "    reshaped_array=reshaped_array_tourism\n",
    "    max_dim=max_seq_length_tourism\n",
    "elif dataset_name=='m3-demo':\n",
    "    padded_seq=padded_sequences_DEMOGRAPHIC\n",
    "    reshaped_array=reshaped_array_DEMOGRAPHIC\n",
    "    max_dim=max_seq_length_DEMOGRAPHIC\n",
    "elif dataset_name=='m3-finance':\n",
    "    padded_seq=padded_sequences_FINANCE\n",
    "    reshaped_array=reshaped_array_FINANCE\n",
    "    max_dim=max_seq_length_FINANCE\n",
    "elif dataset_name=='m3-industry':\n",
    "    padded_seq=padded_sequences_INDUSTRY\n",
    "    reshaped_array=reshaped_array_INDUSTRY\n",
    "    max_dim=max_seq_length_INDUSTRY\n",
    "\n",
    "elif dataset_name=='m3-macro':\n",
    "    padded_seq=padded_sequences_MACRO\n",
    "    reshaped_array=reshaped_array_MACRO\n",
    "    max_dim=max_seq_length_MACRO\n",
    "elif dataset_name=='m3-micro':\n",
    "    padded_seq=padded_sequences_MICRO\n",
    "    reshaped_array=reshaped_array_MICRO\n",
    "    max_dim=max_seq_length_MICRO\n",
    "else:\n",
    "    padded_seq=padded_sequences_OTHER\n",
    "    reshaped_array=reshaped_array_OTHER\n",
    "    max_dim=max_seq_length_OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0cf81c5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:45.943339Z",
     "iopub.status.busy": "2024-09-20T18:16:45.942833Z",
     "iopub.status.idle": "2024-09-20T18:16:45.957529Z",
     "shell.execute_reply": "2024-09-20T18:16:45.956148Z"
    },
    "papermill": {
     "duration": 0.037227,
     "end_time": "2024-09-20T18:16:45.960758",
     "exception": false,
     "start_time": "2024-09-20T18:16:45.923531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtc = DTC(n_clusters=2,input_dim=reshaped_array.shape[-1], timesteps=padded_seq.shape[1], max_dim=max_dim,dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b5b07c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:45.999433Z",
     "iopub.status.busy": "2024-09-20T18:16:45.998956Z",
     "iopub.status.idle": "2024-09-20T18:16:55.777018Z",
     "shell.execute_reply": "2024-09-20T18:16:55.767113Z"
    },
    "papermill": {
     "duration": 9.805621,
     "end_time": "2024-09-20T18:16:55.784505",
     "exception": false,
     "start_time": "2024-09-20T18:16:45.978884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Neighbour1_input (InputLayer)  [(None, None, 108)]  0           []                               \n",
      "                                                                                                  \n",
      " Neighbour2_input (InputLayer)  [(None, None, 108)]  0           []                               \n",
      "                                                                                                  \n",
      " input (InputLayer)             [(None, None, 108)]  0           []                               \n",
      "                                                                                                  \n",
      " lstm_Neighbour1_256 (LSTM)     (None, None, 256)    373760      ['Neighbour1_input[0][0]']       \n",
      "                                                                                                  \n",
      " lstm_Neighbour2_256 (LSTM)     (None, None, 256)    373760      ['Neighbour2_input[0][0]']       \n",
      "                                                                                                  \n",
      " lstm_input_256 (LSTM)          (None, None, 256)    373760      ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " lstm_Neighbour1_128 (LSTM)     (None, None, 128)    197120      ['lstm_Neighbour1_256[0][0]']    \n",
      "                                                                                                  \n",
      " lstm_Neighbour2_128 (LSTM)     (None, None, 128)    197120      ['lstm_Neighbour2_256[0][0]']    \n",
      "                                                                                                  \n",
      " lstm_input_128 (LSTM)          (None, None, 128)    197120      ['lstm_input_256[0][0]']         \n",
      "                                                                                                  \n",
      " attention_Neighbour1 (Attentio  (None, None, 128)   0           ['lstm_Neighbour1_128[0][0]',    \n",
      " n)                                                               'lstm_Neighbour2_128[0][0]',    \n",
      "                                                                  'lstm_Neighbour2_128[0][0]']    \n",
      "                                                                                                  \n",
      " attention_Neighbour2 (Attentio  (None, None, 128)   0           ['lstm_Neighbour2_128[0][0]',    \n",
      " n)                                                               'lstm_Neighbour1_128[0][0]',    \n",
      "                                                                  'lstm_Neighbour1_128[0][0]']    \n",
      "                                                                                                  \n",
      " attention_input (Attention)    (None, None, 128)    0           ['lstm_input_128[0][0]',         \n",
      "                                                                  'lstm_input_128[0][0]',         \n",
      "                                                                  'lstm_input_128[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, None, 256)    0           ['lstm_Neighbour1_128[0][0]',    \n",
      "                                                                  'attention_Neighbour1[0][0]']   \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, None, 256)    0           ['lstm_Neighbour2_128[0][0]',    \n",
      "                                                                  'attention_Neighbour2[0][0]']   \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, None, 256)    0           ['lstm_input_128[0][0]',         \n",
      "                                                                  'attention_input[0][0]']        \n",
      "                                                                                                  \n",
      " lstm_Neighbour1_64 (LSTM)      (None, 64)           82176       ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_Neighbour2_64 (LSTM)      (None, 64)           82176       ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_input_64 (LSTM)           (None, 64)           82176       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_Neighbour1 (Dense)       (None, 16)           1040        ['lstm_Neighbour1_64[0][0]']     \n",
      "                                                                                                  \n",
      " dense_Neighbour2 (Dense)       (None, 16)           1040        ['lstm_Neighbour2_64[0][0]']     \n",
      "                                                                                                  \n",
      " dense_input (Dense)            (None, 16)           1040        ['lstm_input_64[0][0]']          \n",
      "                                                                                                  \n",
      " maximum (Maximum)              (None, 16)           0           ['dense_Neighbour1[0][0]',       \n",
      "                                                                  'dense_Neighbour2[0][0]']       \n",
      "                                                                                                  \n",
      " add_inputs (Add)               (None, 16)           0           ['dense_input[0][0]',            \n",
      "                                                                  'maximum[0][0]']                \n",
      "                                                                                                  \n",
      " repeat_vector_2 (RepeatVector)  (None, 1, 16)       0           ['add_inputs[0][0]']             \n",
      "                                                                                                  \n",
      " decoded_lstm1 (LSTM)           (None, 1, 64)        20736       ['repeat_vector_2[0][0]']        \n",
      "                                                                                                  \n",
      " decoded_lstm2 (LSTM)           (None, 1, 128)       98816       ['decoded_lstm1[0][0]']          \n",
      "                                                                                                  \n",
      " decoded_lstm3 (LSTM)           (None, 1, 256)       394240      ['decoded_lstm2[0][0]']          \n",
      "                                                                                                  \n",
      " decoded_atten (Attention)      (None, 1, 256)       0           ['decoded_lstm3[0][0]',          \n",
      "                                                                  'decoded_lstm3[0][0]',          \n",
      "                                                                  'decoded_lstm3[0][0]']          \n",
      "                                                                                                  \n",
      " decoded_concat (Concatenate)   (None, 1, 512)       0           ['decoded_lstm3[0][0]',          \n",
      "                                                                  'decoded_atten[0][0]']          \n",
      "                                                                                                  \n",
      " decoded_timeD (TimeDistributed  (None, 1, 108)      55404       ['decoded_concat[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " TSClustering (TSClusteringLaye  (None, 2)           32          ['add_inputs[0][0]']             \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,531,516\n",
      "Trainable params: 2,531,516\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = 'adam'\n",
    "dtc.initialize()\n",
    "dtc.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3503238",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:55.841686Z",
     "iopub.status.busy": "2024-09-20T18:16:55.841158Z",
     "iopub.status.idle": "2024-09-20T18:16:55.878878Z",
     "shell.execute_reply": "2024-09-20T18:16:55.877037Z"
    },
    "papermill": {
     "duration": 0.071029,
     "end_time": "2024-09-20T18:16:55.883311",
     "exception": false,
     "start_time": "2024-09-20T18:16:55.812282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtc.compile(gamma=1.0, optimizer=optimizer, initial_heatmap_loss_weight=0.1,\n",
    "                final_heatmap_loss_weight=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f20e2df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:16:55.943113Z",
     "iopub.status.busy": "2024-09-20T18:16:55.942580Z",
     "iopub.status.idle": "2024-09-20T18:24:24.685170Z",
     "shell.execute_reply": "2024-09-20T18:24:24.683343Z"
    },
    "papermill": {
     "duration": 448.777194,
     "end_time": "2024-09-20T18:24:24.688547",
     "exception": false,
     "start_time": "2024-09-20T18:16:55.911353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining...\n",
      "Epoch 1/50\n",
      "48/48 [==============================] - 9s 187ms/step - loss: 0.6233\n",
      "Epoch 2/50\n",
      "48/48 [==============================] - 9s 188ms/step - loss: 0.4904\n",
      "Epoch 3/50\n",
      "48/48 [==============================] - 9s 190ms/step - loss: 0.4706\n",
      "Epoch 4/50\n",
      "48/48 [==============================] - 9s 190ms/step - loss: 0.4562\n",
      "Epoch 5/50\n",
      "48/48 [==============================] - 9s 186ms/step - loss: 0.4453\n",
      "Epoch 6/50\n",
      "48/48 [==============================] - 9s 186ms/step - loss: 0.4315\n",
      "Epoch 7/50\n",
      "48/48 [==============================] - 9s 188ms/step - loss: 0.4218\n",
      "Epoch 8/50\n",
      "48/48 [==============================] - 9s 189ms/step - loss: 0.4107\n",
      "Epoch 9/50\n",
      "48/48 [==============================] - 9s 186ms/step - loss: 0.4032\n",
      "Epoch 10/50\n",
      "48/48 [==============================] - 9s 190ms/step - loss: 0.3767\n",
      "Epoch 11/50\n",
      "48/48 [==============================] - 9s 192ms/step - loss: 0.3555\n",
      "Epoch 12/50\n",
      "48/48 [==============================] - 9s 194ms/step - loss: 0.3481\n",
      "Epoch 13/50\n",
      "48/48 [==============================] - 9s 190ms/step - loss: 0.3427\n",
      "Epoch 14/50\n",
      "48/48 [==============================] - 9s 192ms/step - loss: 0.3410\n",
      "Epoch 15/50\n",
      "48/48 [==============================] - 9s 190ms/step - loss: 0.3348\n",
      "Epoch 16/50\n",
      "48/48 [==============================] - 9s 188ms/step - loss: 0.3316\n",
      "Epoch 17/50\n",
      "48/48 [==============================] - 9s 190ms/step - loss: 0.3287\n",
      "Epoch 18/50\n",
      "48/48 [==============================] - 9s 188ms/step - loss: 0.3263\n",
      "Epoch 19/50\n",
      "48/48 [==============================] - 9s 188ms/step - loss: 0.3251\n",
      "Epoch 20/50\n",
      "48/48 [==============================] - 9s 188ms/step - loss: 0.3238\n",
      "Epoch 21/50\n",
      "48/48 [==============================] - 9s 190ms/step - loss: 0.3232\n",
      "Epoch 22/50\n",
      "48/48 [==============================] - 9s 188ms/step - loss: 0.3221\n",
      "Epoch 23/50\n",
      "48/48 [==============================] - 9s 187ms/step - loss: 0.3210\n",
      "Epoch 24/50\n",
      "48/48 [==============================] - 9s 187ms/step - loss: 0.3196\n",
      "Epoch 25/50\n",
      "48/48 [==============================] - 9s 187ms/step - loss: 0.3202\n",
      "Epoch 26/50\n",
      "48/48 [==============================] - 9s 181ms/step - loss: 0.3189\n",
      "Epoch 27/50\n",
      "48/48 [==============================] - 9s 183ms/step - loss: 0.3182\n",
      "Epoch 28/50\n",
      "48/48 [==============================] - 9s 184ms/step - loss: 0.3195\n",
      "Epoch 29/50\n",
      "48/48 [==============================] - 9s 183ms/step - loss: 0.3167\n",
      "Epoch 30/50\n",
      "48/48 [==============================] - 9s 184ms/step - loss: 0.3157\n",
      "Epoch 31/50\n",
      "48/48 [==============================] - 9s 184ms/step - loss: 0.3157\n",
      "Epoch 32/50\n",
      "48/48 [==============================] - 9s 187ms/step - loss: 0.3137\n",
      "Epoch 33/50\n",
      "48/48 [==============================] - 9s 183ms/step - loss: 0.3134\n",
      "Epoch 34/50\n",
      "48/48 [==============================] - 9s 185ms/step - loss: 0.3150\n",
      "Epoch 35/50\n",
      "48/48 [==============================] - 9s 184ms/step - loss: 0.3130\n",
      "Epoch 36/50\n",
      "48/48 [==============================] - 9s 183ms/step - loss: 0.3115\n",
      "Epoch 37/50\n",
      "48/48 [==============================] - 9s 182ms/step - loss: 0.3105\n",
      "Epoch 38/50\n",
      "48/48 [==============================] - 9s 186ms/step - loss: 0.3088\n",
      "Epoch 39/50\n",
      "48/48 [==============================] - 9s 184ms/step - loss: 0.3083\n",
      "Epoch 40/50\n",
      "48/48 [==============================] - 9s 182ms/step - loss: 0.3056\n",
      "Epoch 41/50\n",
      "48/48 [==============================] - 9s 185ms/step - loss: 0.3027\n",
      "Epoch 42/50\n",
      "48/48 [==============================] - 9s 185ms/step - loss: 0.2991\n",
      "Epoch 43/50\n",
      "48/48 [==============================] - 9s 184ms/step - loss: 0.2963\n",
      "Epoch 44/50\n",
      "48/48 [==============================] - 9s 184ms/step - loss: 0.2930\n",
      "Epoch 45/50\n",
      "48/48 [==============================] - 9s 181ms/step - loss: 0.2907\n",
      "Epoch 46/50\n",
      "48/48 [==============================] - 9s 184ms/step - loss: 0.2885\n",
      "Epoch 47/50\n",
      "48/48 [==============================] - 9s 186ms/step - loss: 0.2877\n",
      "Epoch 48/50\n",
      "48/48 [==============================] - 9s 188ms/step - loss: 0.2850\n",
      "Epoch 49/50\n",
      "48/48 [==============================] - 9s 189ms/step - loss: 0.2829\n",
      "Epoch 50/50\n",
      "48/48 [==============================] - 9s 188ms/step - loss: 0.2825\n",
      "Pretrained weights are saved to /kaggle/working//ae_weights-epoch50.h5\n"
     ]
    }
   ],
   "source": [
    "dtc.pretrain(X=reshaped_array, optimizer=optimizer, epochs=50, batch_size=10,save_dir=\"/kaggle/working/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d11b5cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:24:25.212203Z",
     "iopub.status.busy": "2024-09-20T18:24:25.211729Z",
     "iopub.status.idle": "2024-09-20T18:24:25.268106Z",
     "shell.execute_reply": "2024-09-20T18:24:25.266566Z"
    },
    "papermill": {
     "duration": 0.323279,
     "end_time": "2024-09-20T18:24:25.271360",
     "exception": false,
     "start_time": "2024-09-20T18:24:24.948081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtc.load_ae_neighbour_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a1b8c57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:24:25.783764Z",
     "iopub.status.busy": "2024-09-20T18:24:25.783324Z",
     "iopub.status.idle": "2024-09-20T18:24:26.643332Z",
     "shell.execute_reply": "2024-09-20T18:24:26.642397Z"
    },
    "papermill": {
     "duration": 1.117777,
     "end_time": "2024-09-20T18:24:26.646166",
     "exception": false,
     "start_time": "2024-09-20T18:24:25.528389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing cluster...\n",
      "15/15 [==============================] - 1s 38ms/step\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Initialize clusters\n",
    "dtc.init_cluster_weights(reshaped_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f1fbf57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:24:27.236841Z",
     "iopub.status.busy": "2024-09-20T18:24:27.235403Z",
     "iopub.status.idle": "2024-09-20T18:25:15.503151Z",
     "shell.execute_reply": "2024-09-20T18:25:15.501478Z"
    },
    "papermill": {
     "duration": 48.524011,
     "end_time": "2024-09-20T18:25:15.505946",
     "exception": false,
     "start_time": "2024-09-20T18:24:26.981935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10 epochs.\n",
      "Evaluating every 10 and saving model every 10 epochs.\n",
      "15/15 [==============================] - 2s 145ms/step\n",
      "15/15 [==============================] - 2s 108ms/step\n",
      "Number of reliable samples: 474\n",
      "Number of unreliable samples: 0\n",
      "reconstruction_loss:  tf.Tensor(335.2123, shape=(), dtype=float32)\n",
      "clustering_loss:  tf.Tensor(0.25831142, shape=(), dtype=float32)\n",
      "15/15 [==============================] - 2s 109ms/step\n",
      "Epoch: 1/10\n",
      "Total Loss:  tf.Tensor(335.4706, shape=(), dtype=float32)\n",
      "15/15 [==============================] - 2s 144ms/step\n",
      "Number of reliable samples: 474\n",
      "Number of unreliable samples: 0\n",
      "reconstruction_loss:  tf.Tensor(325.27667, shape=(), dtype=float32)\n",
      "clustering_loss:  tf.Tensor(0.2517398, shape=(), dtype=float32)\n",
      "15/15 [==============================] - 2s 109ms/step\n",
      "Epoch: 2/10\n",
      "Total Loss:  tf.Tensor(325.5284, shape=(), dtype=float32)\n",
      "15/15 [==============================] - 2s 148ms/step\n",
      "Number of reliable samples: 474\n",
      "Number of unreliable samples: 0\n",
      "reconstruction_loss:  tf.Tensor(316.2582, shape=(), dtype=float32)\n",
      "clustering_loss:  tf.Tensor(0.24267033, shape=(), dtype=float32)\n",
      "15/15 [==============================] - 2s 110ms/step\n",
      "Epoch: 3/10\n",
      "Total Loss:  tf.Tensor(316.5009, shape=(), dtype=float32)\n",
      "15/15 [==============================] - 2s 152ms/step\n",
      "Number of reliable samples: 474\n",
      "Number of unreliable samples: 0\n",
      "reconstruction_loss:  tf.Tensor(307.35757, shape=(), dtype=float32)\n",
      "clustering_loss:  tf.Tensor(0.23248143, shape=(), dtype=float32)\n",
      "15/15 [==============================] - 2s 111ms/step\n",
      "Epoch: 4/10\n",
      "Total Loss:  tf.Tensor(307.59006, shape=(), dtype=float32)\n",
      "15/15 [==============================] - 2s 149ms/step\n",
      "Number of reliable samples: 474\n",
      "Number of unreliable samples: 0\n",
      "reconstruction_loss:  tf.Tensor(298.0845, shape=(), dtype=float32)\n",
      "clustering_loss:  tf.Tensor(0.2240695, shape=(), dtype=float32)\n",
      "15/15 [==============================] - 2s 113ms/step\n",
      "Epoch: 5/10\n",
      "Total Loss:  tf.Tensor(298.30856, shape=(), dtype=float32)\n",
      "15/15 [==============================] - 2s 150ms/step\n",
      "Number of reliable samples: 474\n",
      "Number of unreliable samples: 0\n",
      "reconstruction_loss:  tf.Tensor(288.29202, shape=(), dtype=float32)\n",
      "clustering_loss:  tf.Tensor(0.22053334, shape=(), dtype=float32)\n",
      "15/15 [==============================] - 2s 109ms/step\n",
      "Epoch: 6/10\n",
      "Total Loss:  tf.Tensor(288.51254, shape=(), dtype=float32)\n",
      "15/15 [==============================] - 2s 152ms/step\n",
      "Number of reliable samples: 474\n",
      "Number of unreliable samples: 0\n",
      "reconstruction_loss:  tf.Tensor(278.01752, shape=(), dtype=float32)\n",
      "clustering_loss:  tf.Tensor(0.22186078, shape=(), dtype=float32)\n",
      "15/15 [==============================] - 2s 117ms/step\n",
      "Epoch: 7/10\n",
      "Total Loss:  tf.Tensor(278.23938, shape=(), dtype=float32)\n",
      "15/15 [==============================] - 2s 156ms/step\n",
      "Number of reliable samples: 474\n",
      "Number of unreliable samples: 0\n",
      "reconstruction_loss:  tf.Tensor(267.42764, shape=(), dtype=float32)\n",
      "clustering_loss:  tf.Tensor(0.22743346, shape=(), dtype=float32)\n",
      "15/15 [==============================] - 2s 112ms/step\n",
      "Epoch: 8/10\n",
      "Total Loss:  tf.Tensor(267.6551, shape=(), dtype=float32)\n",
      "15/15 [==============================] - 2s 148ms/step\n",
      "Number of reliable samples: 474\n",
      "Number of unreliable samples: 0\n",
      "reconstruction_loss:  tf.Tensor(256.68512, shape=(), dtype=float32)\n",
      "clustering_loss:  tf.Tensor(0.23695943, shape=(), dtype=float32)\n",
      "15/15 [==============================] - 2s 110ms/step\n",
      "Epoch: 9/10\n",
      "Total Loss:  tf.Tensor(256.9221, shape=(), dtype=float32)\n",
      "15/15 [==============================] - 2s 150ms/step\n",
      "Number of reliable samples: 474\n",
      "Number of unreliable samples: 0\n",
      "reconstruction_loss:  tf.Tensor(246.03604, shape=(), dtype=float32)\n",
      "clustering_loss:  tf.Tensor(0.2489418, shape=(), dtype=float32)\n",
      "15/15 [==============================] - 2s 113ms/step\n",
      "Epoch: 10/10\n",
      "Total Loss:  tf.Tensor(246.28499, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "dtc.fit(reshaped_array, y_train=None, X_val=None, y_val=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e148783",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:25:16.107093Z",
     "iopub.status.busy": "2024-09-20T18:25:16.106582Z",
     "iopub.status.idle": "2024-09-20T18:25:18.499486Z",
     "shell.execute_reply": "2024-09-20T18:25:18.497887Z"
    },
    "papermill": {
     "duration": 2.691783,
     "end_time": "2024-09-20T18:25:18.503048",
     "exception": false,
     "start_time": "2024-09-20T18:25:15.811265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance (TRAIN)\n",
      "15/15 [==============================] - 2s 152ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "print('Performance (TRAIN)')\n",
    "results = {}\n",
    "q = dtc.model.predict([reshaped_array, reshaped_array, reshaped_array])[1]\n",
    "p = dtc.target_distribution(q)\n",
    "p_pred = p.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37502303",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:25:19.137144Z",
     "iopub.status.busy": "2024-09-20T18:25:19.135995Z",
     "iopub.status.idle": "2024-09-20T18:25:19.143810Z",
     "shell.execute_reply": "2024-09-20T18:25:19.142253Z"
    },
    "papermill": {
     "duration": 0.316785,
     "end_time": "2024-09-20T18:25:19.146771",
     "exception": false,
     "start_time": "2024-09-20T18:25:18.829986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save(dataset_name+'_Labels.npy', p_pred) # save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07616e2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:25:19.732437Z",
     "iopub.status.busy": "2024-09-20T18:25:19.731942Z",
     "iopub.status.idle": "2024-09-20T18:25:19.743387Z",
     "shell.execute_reply": "2024-09-20T18:25:19.742004Z"
    },
    "papermill": {
     "duration": 0.311472,
     "end_time": "2024-09-20T18:25:19.746087",
     "exception": false,
     "start_time": "2024-09-20T18:25:19.434615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81a044f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T18:25:20.327399Z",
     "iopub.status.busy": "2024-09-20T18:25:20.326948Z",
     "iopub.status.idle": "2024-09-20T18:25:20.336652Z",
     "shell.execute_reply": "2024-09-20T18:25:20.335066Z"
    },
    "papermill": {
     "duration": 0.303017,
     "end_time": "2024-09-20T18:25:20.339939",
     "exception": false,
     "start_time": "2024-09-20T18:25:20.036922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([180, 294]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(p_pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f13ba8f",
   "metadata": {
    "papermill": {
     "duration": 0.294372,
     "end_time": "2024-09-20T18:25:21.036124",
     "exception": false,
     "start_time": "2024-09-20T18:25:20.741752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3743351,
     "sourceId": 6479400,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3792584,
     "sourceId": 6564428,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4520850,
     "sourceId": 7735771,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4454423,
     "sourceId": 9326749,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 146550165,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 146550185,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 166856298,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 600.751338,
   "end_time": "2024-09-20T18:25:24.252450",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-20T18:15:23.501112",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
