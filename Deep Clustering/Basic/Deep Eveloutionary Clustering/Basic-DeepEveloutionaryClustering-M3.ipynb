{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "491d925f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:24:28.502359Z",
     "iopub.status.busy": "2024-10-17T06:24:28.502044Z",
     "iopub.status.idle": "2024-10-17T06:24:41.589949Z",
     "shell.execute_reply": "2024-10-17T06:24:41.588995Z"
    },
    "papermill": {
     "duration": 13.110388,
     "end_time": "2024-10-17T06:24:41.592343",
     "exception": false,
     "start_time": "2024-10-17T06:24:28.481955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tslearn\r\n",
      "  Downloading tslearn-0.6.3-py3-none-any.whl (374 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tslearn) (1.23.5)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from tslearn) (1.11.2)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from tslearn) (1.2.2)\r\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from tslearn) (0.57.1)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from tslearn) (1.3.2)\r\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->tslearn) (0.40.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->tslearn) (3.1.0)\r\n",
      "Installing collected packages: tslearn\r\n",
      "Successfully installed tslearn-0.6.3\r\n"
     ]
    }
   ],
   "source": [
    "! pip install tslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04d14819",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:24:41.631719Z",
     "iopub.status.busy": "2024-10-17T06:24:41.631366Z",
     "iopub.status.idle": "2024-10-17T06:24:53.715628Z",
     "shell.execute_reply": "2024-10-17T06:24:53.714713Z"
    },
    "papermill": {
     "duration": 12.106495,
     "end_time": "2024-10-17T06:24:53.718166",
     "exception": false,
     "start_time": "2024-10-17T06:24:41.611671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn-extra\r\n",
      "  Downloading scikit_learn_extra-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn-extra) (1.23.5)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn-extra) (1.11.2)\r\n",
      "Requirement already satisfied: scikit-learn>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn-extra) (1.2.2)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.1.0)\r\n",
      "Installing collected packages: scikit-learn-extra\r\n",
      "Successfully installed scikit-learn-extra-0.3.0\r\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-learn-extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03a57aab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:24:53.759294Z",
     "iopub.status.busy": "2024-10-17T06:24:53.758944Z",
     "iopub.status.idle": "2024-10-17T06:25:05.741382Z",
     "shell.execute_reply": "2024-10-17T06:25:05.740408Z"
    },
    "papermill": {
     "duration": 12.005277,
     "end_time": "2024-10-17T06:25:05.743755",
     "exception": false,
     "start_time": "2024-10-17T06:24:53.738478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rstl\r\n",
      "  Downloading rstl-0.1.3-py3-none-any.whl (5.2 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rstl) (1.23.5)\r\n",
      "Installing collected packages: rstl\r\n",
      "Successfully installed rstl-0.1.3\r\n"
     ]
    }
   ],
   "source": [
    "! pip install rstl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29378545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:25:05.785709Z",
     "iopub.status.busy": "2024-10-17T06:25:05.785320Z",
     "iopub.status.idle": "2024-10-17T06:25:17.742231Z",
     "shell.execute_reply": "2024-10-17T06:25:17.741239Z"
    },
    "papermill": {
     "duration": 11.980105,
     "end_time": "2024-10-17T06:25:17.744523",
     "exception": false,
     "start_time": "2024-10-17T06:25:05.764418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-tcn\r\n",
      "  Downloading keras_tcn-3.5.0-py3-none-any.whl (13 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras-tcn) (1.23.5)\r\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (from keras-tcn) (2.12.0)\r\n",
      "Requirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.10/site-packages (from keras-tcn) (0.21.0)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (1.4.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (23.5.26)\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (0.4.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (0.2.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (1.51.1)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (3.9.0)\r\n",
      "Requirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (0.4.13)\r\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (2.12.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (16.0.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (3.3.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (21.3)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (3.20.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (68.0.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (1.16.0)\r\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (2.12.3)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (2.12.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (2.3.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (4.6.3)\r\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (1.14.1)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (0.32.0)\r\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /opt/conda/lib/python3.10/site-packages (from tensorflow-addons->keras-tcn) (2.13.3)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow->keras-tcn) (0.40.0)\r\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow->keras-tcn) (0.2.0)\r\n",
      "Requirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow->keras-tcn) (1.11.2)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (2.20.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (1.0.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (3.4.3)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (2.31.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (0.7.1)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (2.3.7)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow->keras-tcn) (3.0.9)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (4.2.4)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (0.2.7)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (4.9)\r\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (1.26.15)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (1.3.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (2023.7.22)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (2.1.3)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (3.2.2)\r\n",
      "Installing collected packages: keras-tcn\r\n",
      "Successfully installed keras-tcn-3.5.0\r\n"
     ]
    }
   ],
   "source": [
    "! pip install keras-tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03b1faf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:25:17.787791Z",
     "iopub.status.busy": "2024-10-17T06:25:17.787452Z",
     "iopub.status.idle": "2024-10-17T06:25:29.980218Z",
     "shell.execute_reply": "2024-10-17T06:25:29.979006Z"
    },
    "papermill": {
     "duration": 12.21766,
     "end_time": "2024-10-17T06:25:29.983027",
     "exception": false,
     "start_time": "2024-10-17T06:25:17.765367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pmdarima\r\n",
      "  Downloading pmdarima-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.10/site-packages (from pmdarima) (1.3.2)\r\n",
      "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /opt/conda/lib/python3.10/site-packages (from pmdarima) (0.29.35)\r\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from pmdarima) (1.23.5)\r\n",
      "Requirement already satisfied: pandas>=0.19 in /opt/conda/lib/python3.10/site-packages (from pmdarima) (2.0.2)\r\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /opt/conda/lib/python3.10/site-packages (from pmdarima) (1.2.2)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from pmdarima) (1.11.2)\r\n",
      "Requirement already satisfied: statsmodels>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from pmdarima) (0.14.0)\r\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from pmdarima) (1.26.15)\r\n",
      "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /opt/conda/lib/python3.10/site-packages (from pmdarima) (68.0.0)\r\n",
      "Requirement already satisfied: packaging>=17.1 in /opt/conda/lib/python3.10/site-packages (from pmdarima) (21.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.1->pmdarima) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.19->pmdarima) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.19->pmdarima) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.19->pmdarima) (2023.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22->pmdarima) (3.1.0)\r\n",
      "Requirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.10/site-packages (from statsmodels>=0.13.2->pmdarima) (0.5.3)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.2->statsmodels>=0.13.2->pmdarima) (1.16.0)\r\n",
      "Installing collected packages: pmdarima\r\n",
      "Successfully installed pmdarima-2.0.4\r\n"
     ]
    }
   ],
   "source": [
    "! pip install pmdarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef82c2da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:25:30.034091Z",
     "iopub.status.busy": "2024-10-17T06:25:30.033616Z",
     "iopub.status.idle": "2024-10-17T06:25:41.619098Z",
     "shell.execute_reply": "2024-10-17T06:25:41.617996Z"
    },
    "papermill": {
     "duration": 11.612384,
     "end_time": "2024-10-17T06:25:41.621402",
     "exception": false,
     "start_time": "2024-10-17T06:25:30.009018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in /opt/conda/lib/python3.10/site-packages (0.14.0)\r\n",
      "Requirement already satisfied: numpy>=1.18 in /opt/conda/lib/python3.10/site-packages (from statsmodels) (1.23.5)\r\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.4 in /opt/conda/lib/python3.10/site-packages (from statsmodels) (1.11.2)\r\n",
      "Requirement already satisfied: pandas>=1.0 in /opt/conda/lib/python3.10/site-packages (from statsmodels) (2.0.2)\r\n",
      "Requirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.10/site-packages (from statsmodels) (0.5.3)\r\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from statsmodels) (21.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21.3->statsmodels) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0->statsmodels) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0->statsmodels) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0->statsmodels) (2023.3)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.2->statsmodels) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae862183",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:25:41.666621Z",
     "iopub.status.busy": "2024-10-17T06:25:41.666267Z",
     "iopub.status.idle": "2024-10-17T06:25:55.271295Z",
     "shell.execute_reply": "2024-10-17T06:25:55.270374Z"
    },
    "papermill": {
     "duration": 13.630308,
     "end_time": "2024-10-17T06:25:55.273758",
     "exception": false,
     "start_time": "2024-10-17T06:25:41.643450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting statsforecast\r\n",
      "  Downloading statsforecast-1.7.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (314 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from statsforecast) (2.2.1)\r\n",
      "Collecting coreforecast>=0.0.12 (from statsforecast)\r\n",
      "  Downloading coreforecast-0.0.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (271 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.4/271.4 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numba>=0.55.0 in /opt/conda/lib/python3.10/site-packages (from statsforecast) (0.57.1)\r\n",
      "Requirement already satisfied: numpy>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from statsforecast) (1.23.5)\r\n",
      "Requirement already satisfied: pandas>=1.3.5 in /opt/conda/lib/python3.10/site-packages (from statsforecast) (2.0.2)\r\n",
      "Requirement already satisfied: scipy>=1.7.3 in /opt/conda/lib/python3.10/site-packages (from statsforecast) (1.11.2)\r\n",
      "Requirement already satisfied: statsmodels>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from statsforecast) (0.14.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from statsforecast) (4.66.1)\r\n",
      "Collecting fugue>=0.8.1 (from statsforecast)\r\n",
      "  Downloading fugue-0.9.1-py3-none-any.whl (278 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting utilsforecast>=0.1.4 (from statsforecast)\r\n",
      "  Downloading utilsforecast-0.2.5-py3-none-any.whl (41 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=3 in /opt/conda/lib/python3.10/site-packages (from statsforecast) (3.1.0)\r\n",
      "Collecting triad>=0.9.7 (from fugue>=0.8.1->statsforecast)\r\n",
      "  Downloading triad-0.9.8-py3-none-any.whl (62 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting adagio>=0.2.4 (from fugue>=0.8.1->statsforecast)\r\n",
      "  Downloading adagio-0.2.6-py3-none-any.whl (19 kB)\r\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.55.0->statsforecast) (0.40.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3.5->statsforecast) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3.5->statsforecast) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3.5->statsforecast) (2023.3)\r\n",
      "Requirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.10/site-packages (from statsmodels>=0.13.2->statsforecast) (0.5.3)\r\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from statsmodels>=0.13.2->statsforecast) (21.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21.3->statsmodels>=0.13.2->statsforecast) (3.0.9)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.2->statsmodels>=0.13.2->statsforecast) (1.16.0)\r\n",
      "Requirement already satisfied: pyarrow>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from triad>=0.9.7->fugue>=0.8.1->statsforecast) (11.0.0)\r\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from triad>=0.9.7->fugue>=0.8.1->statsforecast) (2023.9.0)\r\n",
      "Collecting fs (from triad>=0.9.7->fugue>=0.8.1->statsforecast)\r\n",
      "  Downloading fs-2.4.16-py2.py3-none-any.whl (135 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: appdirs~=1.4.3 in /opt/conda/lib/python3.10/site-packages (from fs->triad>=0.9.7->fugue>=0.8.1->statsforecast) (1.4.4)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from fs->triad>=0.9.7->fugue>=0.8.1->statsforecast) (68.0.0)\r\n",
      "Installing collected packages: fs, coreforecast, utilsforecast, triad, adagio, fugue, statsforecast\r\n",
      "Successfully installed adagio-0.2.6 coreforecast-0.0.13.1 fs-2.4.16 fugue-0.9.1 statsforecast-1.7.8 triad-0.9.8 utilsforecast-0.2.5\r\n"
     ]
    }
   ],
   "source": [
    "! pip install statsforecast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f44811d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:25:55.322739Z",
     "iopub.status.busy": "2024-10-17T06:25:55.322365Z",
     "iopub.status.idle": "2024-10-17T06:25:57.639174Z",
     "shell.execute_reply": "2024-10-17T06:25:57.638367Z"
    },
    "papermill": {
     "duration": 2.34376,
     "end_time": "2024-10-17T06:25:57.641308",
     "exception": false,
     "start_time": "2024-10-17T06:25:55.297548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ba6d992",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:25:57.689574Z",
     "iopub.status.busy": "2024-10-17T06:25:57.689131Z",
     "iopub.status.idle": "2024-10-17T06:25:58.577249Z",
     "shell.execute_reply": "2024-10-17T06:25:58.576410Z"
    },
    "papermill": {
     "duration": 0.914941,
     "end_time": "2024-10-17T06:25:58.579652",
     "exception": false,
     "start_time": "2024-10-17T06:25:57.664711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pmdarima as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "546840ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:25:58.673721Z",
     "iopub.status.busy": "2024-10-17T06:25:58.672937Z",
     "iopub.status.idle": "2024-10-17T06:25:58.816275Z",
     "shell.execute_reply": "2024-10-17T06:25:58.815307Z"
    },
    "papermill": {
     "duration": 0.215352,
     "end_time": "2024-10-17T06:25:58.818826",
     "exception": false,
     "start_time": "2024-10-17T06:25:58.603474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ba96dc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:25:58.867279Z",
     "iopub.status.busy": "2024-10-17T06:25:58.866911Z",
     "iopub.status.idle": "2024-10-17T06:26:10.930565Z",
     "shell.execute_reply": "2024-10-17T06:26:10.929536Z"
    },
    "papermill": {
     "duration": 12.090431,
     "end_time": "2024-10-17T06:26:10.933028",
     "exception": false,
     "start_time": "2024-10-17T06:25:58.842597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import os\n",
    "import csv\n",
    "import argparse\n",
    "from time import time\n",
    "# Keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Reshape, UpSampling2D, Conv2DTranspose, GlobalAveragePooling1D, Softmax\n",
    "from keras.losses import kullback_leibler_divergence\n",
    "import keras.backend as K\n",
    "# scikit-learn\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "# Dataset helper function\n",
    "# DTC components\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, LeakyReLU, MaxPool1D, LSTM, Bidirectional, TimeDistributed, Dense, Reshape\n",
    "from keras.layers import UpSampling2D, Conv2DTranspose\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from statsmodels.tsa import stattools\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, Attention\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from sklearn.multioutput import MultiOutputRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b83ae45e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:10.981795Z",
     "iopub.status.busy": "2024-10-17T06:26:10.981182Z",
     "iopub.status.idle": "2024-10-17T06:26:11.121692Z",
     "shell.execute_reply": "2024-10-17T06:26:11.120774Z"
    },
    "papermill": {
     "duration": 0.166975,
     "end_time": "2024-10-17T06:26:11.124024",
     "exception": false,
     "start_time": "2024-10-17T06:26:10.957049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "import tensorflow_addons as tfa\n",
    "from math import pi, ceil\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0b87b0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:11.173133Z",
     "iopub.status.busy": "2024-10-17T06:26:11.172226Z",
     "iopub.status.idle": "2024-10-17T06:26:11.191320Z",
     "shell.execute_reply": "2024-10-17T06:26:11.190382Z"
    },
    "papermill": {
     "duration": 0.045604,
     "end_time": "2024-10-17T06:26:11.193288",
     "exception": false,
     "start_time": "2024-10-17T06:26:11.147684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import SpectralClustering\n",
    "# %tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import layers\n",
    "from keras.models import Sequential,Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import time\n",
    "print(tf.__version__)\n",
    "from keras.layers import MultiHeadAttention\n",
    "from keras.layers import Dense\n",
    "import gc\n",
    "from keras.layers import concatenate\n",
    "import csv\n",
    "import math\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# import xgboost as xgb\n",
    "warnings.filterwarnings('ignore')\n",
    "# import GPy, GPyOpt\n",
    "tfkl = tf.keras.layers\n",
    "tfk = tf.keras\n",
    "from tcn import TCN\n",
    "\n",
    "from rstl import STL\n",
    "from texttable import Texttable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7f9e870",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:11.240946Z",
     "iopub.status.busy": "2024-10-17T06:26:11.240661Z",
     "iopub.status.idle": "2024-10-17T06:26:14.401907Z",
     "shell.execute_reply": "2024-10-17T06:26:14.401150Z"
    },
    "papermill": {
     "duration": 3.187757,
     "end_time": "2024-10-17T06:26:14.404221",
     "exception": false,
     "start_time": "2024-10-17T06:26:11.216464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsforecast.models import AutoARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec41cdbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:14.452363Z",
     "iopub.status.busy": "2024-10-17T06:26:14.451754Z",
     "iopub.status.idle": "2024-10-17T06:26:14.456250Z",
     "shell.execute_reply": "2024-10-17T06:26:14.455366Z"
    },
    "papermill": {
     "duration": 0.030377,
     "end_time": "2024-10-17T06:26:14.458256",
     "exception": false,
     "start_time": "2024-10-17T06:26:14.427879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57f996be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:14.506039Z",
     "iopub.status.busy": "2024-10-17T06:26:14.505189Z",
     "iopub.status.idle": "2024-10-17T06:26:14.510623Z",
     "shell.execute_reply": "2024-10-17T06:26:14.509768Z"
    },
    "papermill": {
     "duration": 0.031472,
     "end_time": "2024-10-17T06:26:14.512605",
     "exception": false,
     "start_time": "2024-10-17T06:26:14.481133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import KLDivergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53ebb34e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:14.560191Z",
     "iopub.status.busy": "2024-10-17T06:26:14.559904Z",
     "iopub.status.idle": "2024-10-17T06:26:14.581179Z",
     "shell.execute_reply": "2024-10-17T06:26:14.580474Z"
    },
    "papermill": {
     "duration": 0.047992,
     "end_time": "2024-10-17T06:26:14.583572",
     "exception": false,
     "start_time": "2024-10-17T06:26:14.535580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset_params(dataset_name = 'cif-12'):\n",
    "    suilin_smape = False\n",
    "\n",
    "    #-------------------------------------------------- CIF 2016 ------------------------------------------------#\n",
    "    if dataset_name == 'cif-6':\n",
    "        dataset_path = '/kaggle/working'+'/'+'CIF2016'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/cifnewdataset/6.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "        features = None #pd.read_csv(\"/kaggle/input/ae-cif012-unsupervised-ae/Features_LSTM_cif012_32.csv\",sep=',', header=None) # CIF 2016 - Horizon 6\n",
    "#         features = pd.read_csv('/kaggle/input/cif-data-and-hynd-khs-feature/fs_hyndman_freqfind_cif12.csv',sep=',', header=0) # CIF 2016 - Horizon 6\n",
    "\n",
    "        lag = 15\n",
    "        look_forward = 6\n",
    "        batch_size = 3\n",
    "        epochs = 30\n",
    "        learning_rate = 0.0001\n",
    "        features=raw_data\n",
    "\n",
    "        suilin_smape = False\n",
    "        frequency = None\n",
    "\n",
    "    elif dataset_name == 'cif-12':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'CIF2016'\n",
    "\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/cifnewdataset/12.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "#         features = pd.read_csv(\"/kaggle/input/ae-cif012-unsupervised-ae/Features_LSTM_cif012_32.csv\",sep=',', header=None) # CIF 2016 - Horizon 6\n",
    "        #features = pd.read_csv('/kaggle/input/cif-data-and-hynd-khs-feature/fs_hyndman_freqfind_cif12.csv',sep=',', header=0) # CIF 2016 - Horizon 6\n",
    "#         raw_data = raw_data.iloc[:, :-6]\n",
    "        lag = 36\n",
    "        look_forward = 12\n",
    "        batch_size = 6\n",
    "        epochs = 25\n",
    "        learning_rate = 0.001\n",
    "\n",
    "        suilin_smape = False\n",
    "        frequency = 12\n",
    "#         frequency = None\n",
    "    \n",
    "    elif dataset_name == 'tourism':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'tourism'\n",
    "\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/newtsdatasets/Tourism-new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "\n",
    "        #         features = pd.read_csv('/kaggle/input/tourism/fs_hyndman_f-4_tourism.csv',sep=',', header=0) # CIF 2016 - Horizon 6\n",
    "        #     features = pd.read_csv('/kaggle/input/ae-tourism-unsupervised-cnn/Features_tourism_32.csv',sep=',', header=None) # CIF 2016 - Horizon 6\n",
    "\n",
    "\n",
    "        lag = 12\n",
    "        look_forward = 8\n",
    "        batch_size = 100\n",
    "        epochs =50\n",
    "        learning_rate = 0.001\n",
    "        suilin_smape = False\n",
    "        frequency = 4\n",
    "        # frequency = None   \n",
    "    elif dataset_name == 'hospital':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'hospital'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/newtsdatasets/Hospital_new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "\n",
    "#         raw_data = pd.read_csv(\"/kaggle/input/hospital-dataset/hospital_dataset.txt\",sep='delimeter', header=None) # Hospital - Horizon 12\n",
    "#         features = pd.read_csv(\"/kaggle/input/hospital-dataset/hospital_dataset-hyndman-features.csv\",sep=',', header=0) # CIF 2016 - Horizon 6\n",
    "#         features = pd.read_csv(\"/kaggle/input/ae-hospital-unsupervised-ae/Features_hospital_LSTM_8.csv\",sep=',', header=None) # CIF 2016 - Horizon 6\n",
    "\n",
    "        lag = 30\n",
    "        look_forward = 12\n",
    "        batch_size = 20\n",
    "        epochs = 50\n",
    "        learning_rate = 0.0001\n",
    "#         frequency = 12\n",
    "        frequency = None\n",
    "        \n",
    "    elif dataset_name == 'm3-demo':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'M3'\n",
    "\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/newtsdatasets/m3-demo-new2.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "        #features = pd.read_csv(\"/kaggle/input/ae-m3-unsupervised-attenlstm/Features_demo_32.csv\",sep=',', header=None)\n",
    "        \n",
    "        lag = 28#29\n",
    "        look_forward = 18\n",
    "        batch_size = 7\n",
    "        epochs = 20\n",
    "        learning_rate = 0.0001\n",
    "        frequency = 12\n",
    "        # frequency = None\n",
    "\n",
    "    elif dataset_name == 'm3-finance':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'M3'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/newtsdatasets/m3-finance-new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "        #features = pd.read_csv(\"/kaggle/input/ae-m3-unsupervised-attenlstm/Features_finance_32.csv\",sep=',', header=None)\n",
    "        lag = 27\n",
    "        look_forward = 18\n",
    "        batch_size = 9\n",
    "        epochs = 25\n",
    "        learning_rate = 0.0001\n",
    "        frequency = 12\n",
    "        # frequency = None\n",
    "\n",
    "    elif dataset_name == 'm3-industry':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'M3'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/newtsdatasets/m3-industry-new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "        #features = pd.read_csv(\"/kaggle/input/ae-m3-unsupervised-attenlstm/Features_industry_32.csv\",sep=',', header=None)\n",
    "        lag = 28\n",
    "        look_forward = 18\n",
    "        batch_size = 10\n",
    "        epochs = 25\n",
    "        learning_rate = 0.0001\n",
    "        frequency = 12\n",
    "        # frequency = None\n",
    "\n",
    "    elif dataset_name == 'm3-macro':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'M3'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/newtsdatasets/m3-macro-new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "        #features = pd.read_csv(\"/kaggle/input/ae-m3-unsupervised-attenlstm/Features_macro_32.csv\",sep=',', header=None) # CIF 2016 - Horizon 6\n",
    "\n",
    "        lag = 28\n",
    "        look_forward = 18\n",
    "        batch_size = 15\n",
    "        epochs = 25\n",
    "        learning_rate = 0.0001\n",
    "        frequency = 12\n",
    "        # frequency = None\n",
    "\n",
    "    elif dataset_name == 'm3-micro':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'M3'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/newtsdatasets/M3-micro-new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "        #features = pd.read_csv(\"/kaggle/input/ae-m3-unsupervised-attenlstm/Features_micro_32.csv\",sep=',', header=None) # CIF 2016 - Horizon 6\n",
    "\n",
    "        lag = 28\n",
    "        look_forward = 18\n",
    "        batch_size = 18\n",
    "        epochs = 25\n",
    "        learning_rate = 0.0001\n",
    "#         frequency = 12\n",
    "        frequency = None\n",
    "\n",
    "    elif dataset_name == 'm3-other':\n",
    "        dataset_path ='/kaggle/working'+'/'+ 'M3'\n",
    "        raw_data = pd.read_excel(\"/kaggle/input/newtsdatasets/m3-other-new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "        #features = pd.read_csv(\"/kaggle/input/ae-m3-unsupervised-attenlstm/Features_other_32.csv\",sep=',', header=None) # CIF 2016 - Horizon 6\n",
    "\n",
    "        lag = 28\n",
    "        look_forward = 18\n",
    "        batch_size = 2\n",
    "        epochs = 25\n",
    "        learning_rate = 0.0001\n",
    "        frequency = 12\n",
    "        # frequency = None\n",
    "\n",
    "    #-------------------------------------------------- Hospital ------------------------------------------------#\n",
    "    #-------------------------------------------------- Hospital ------------------------------------------------#\n",
    "\n",
    "    \n",
    "    #------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "    sample_overlap = look_forward - 1\n",
    "\n",
    "    raw_data = raw_data.to_numpy().astype('float64')\n",
    "    features = None # features.to_numpy().astype('float64')\n",
    "    dataset = []\n",
    "    seri_len=[]\n",
    "    for i in range(len(raw_data)):\n",
    "        dataset.append(raw_data[i][~np.isnan(raw_data[i])])\n",
    "        seri_len.append(len(raw_data[i][~np.isnan(raw_data[i])]))\n",
    "\n",
    "\n",
    "    print(dataset_name,np.min(seri_len),np.max(seri_len))\n",
    "            \n",
    "\n",
    "\n",
    "    return dataset, features, lag, look_forward, sample_overlap, learning_rate, dataset_path, suilin_smape, frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7d7004f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:14.633633Z",
     "iopub.status.busy": "2024-10-17T06:26:14.633340Z",
     "iopub.status.idle": "2024-10-17T06:26:14.654832Z",
     "shell.execute_reply": "2024-10-17T06:26:14.653927Z"
    },
    "papermill": {
     "duration": 0.047615,
     "end_time": "2024-10-17T06:26:14.656851",
     "exception": false,
     "start_time": "2024-10-17T06:26:14.609236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_dataset(dataset, look_forward ):\n",
    "    data_means = [];\n",
    "    for index in range(len(dataset)):\n",
    "    # Mean Noramlization\n",
    "        series_mean = np.mean(dataset[index][:len(dataset[index]) - look_forward]) # Train Mean: look_forward || Full Mean: Mean: look_forward = 0\n",
    "\n",
    "        if series_mean == 0:\n",
    "            series_mean = 0.001\n",
    "\n",
    "        data_means.append(series_mean)\n",
    "        dataset[index] = np.divide(dataset[index], series_mean)\n",
    "        if np.min(dataset[index][:len(dataset[index])])<=0:\n",
    "                    dataset[index] = np.log(dataset[index] + 1)\n",
    "        else:\n",
    "             dataset[index] = np.log(dataset[index])\n",
    "        # Log Transformation\n",
    "#         dataset[index] = np.log(dataset[index] + 1)\n",
    "       \n",
    "\n",
    "\n",
    "    return dataset, np.array(data_means)\n",
    "\n",
    "def rescale_data_to_main_value(data, means, dataset_seasonal = []):\n",
    "    for index in range(len(data)):\n",
    "        if len(dataset_seasonal) != 0:\n",
    "            data[index] = data[index] + dataset_seasonal[index]\n",
    "        # Revert Log Transformation\n",
    "        data[index] = np.e ** data[index]\n",
    "#         data[index] = data[index]\n",
    "\n",
    "        # Revert Mean Normalization\n",
    "        data[index] = means[index] * data[index]\n",
    "    \n",
    "\n",
    "    return data\n",
    "\n",
    "def normalize_feature_vectors(features):\n",
    "    #------------------- Z-score ----------------------#\n",
    "#     means = features.mean(0)\n",
    "#     stds = features.std(0)\n",
    "\n",
    "#     for i in range(len(features)):\n",
    "#         features[i] = (features[i] - means) / stds\n",
    "\n",
    "    #--------------------Min - Max---------------------#\n",
    "    minimum = features.min(0)\n",
    "    maximum = features.max(0)\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        features[i] = (features[i] - minimum) / (maximum - minimum)\n",
    "\n",
    "\n",
    "    return features\n",
    "\n",
    "\"\"\"![root_mean_square_deviation.svg](attachment:root_mean_square_deviation.svg)\"\"\"\n",
    "\n",
    "#RMSE\n",
    "def root_mean_squared_error(actual, forecast, method = 'single_value'):\n",
    "    # Methods = single_value | per_series\n",
    "    if method == 'single_value':\n",
    "        #Flatten To One Vector\n",
    "        actual = actual.flatten()\n",
    "        forecast = forecast.flatten()\n",
    "\n",
    "        return np.sqrt(np.mean(np.square(actual - forecast)))\n",
    "    elif method == 'per_series':\n",
    "        rmses = []\n",
    "        for i in range(len(actual)):\n",
    "            rmses.append(np.sqrt(np.mean(np.square(actual[i] - forecast[i]))))\n",
    "\n",
    "        return rmses\n",
    "\n",
    "\"\"\"![YIy33.png](attachment:YIy33.png)\"\"\"\n",
    "\n",
    "#SMAPE\n",
    "def single_point_smape(actual, forecast, suilin_smape = False):\n",
    "    if suilin_smape == True:\n",
    "        epsilon = 0.1\n",
    "\n",
    "        return (np.sum(2 * np.abs(forecast - actual) / max((np.abs(actual) + np.abs(forecast))+ epsilon, 0.5 + epsilon)))\n",
    "    else:\n",
    "        return (np.sum(2 * np.abs(forecast - actual) / (np.abs(actual) + np.abs(forecast))))\n",
    "\n",
    "def smape(actual, forecast, method = 'single_value', suilin_smape = False):\n",
    "    # Methods = single_value | per_series\n",
    "    if method == 'single_value':\n",
    "        #Flatten To One Vector\n",
    "        actual = actual.flatten()\n",
    "        forecast = forecast.flatten()\n",
    "        sum_smape = 0\n",
    "        for i in range(len(actual)):\n",
    "            sum_smape += single_point_smape(actual[i], forecast[i], suilin_smape)\n",
    "        return 100 * sum_smape / len(actual)\n",
    "\n",
    "    elif method == 'per_series':\n",
    "        smapes = []\n",
    "        for i in range(len(actual)):\n",
    "            sum_smape = 0\n",
    "            for j in range(len(actual[i])):\n",
    "                sum_smape += single_point_smape(actual[i,j], forecast[i,j], suilin_smape)\n",
    "            smapes.append(100 * sum_smape / len(actual[i]))\n",
    "        return np.array(smapes)\n",
    "\n",
    "# Create Samples from DataSet\n",
    "def create_dataset(sample, look_back, look_forward, sample_overlap, dataset_seasonal):\n",
    "    if(sample_overlap >= look_forward or sample_overlap < 0): sample_overlap = look_forward - 1\n",
    "    if(look_forward == 1): sample_overlap = 0\n",
    "\n",
    "    dataX, dataY, dataY_seasonal = [], [], []\n",
    "    dataX_means, dataY_means = [], []\n",
    "    for i in range(0, len(sample) - look_back - look_forward+1, look_forward - sample_overlap):\n",
    "        dataX.append(sample[i:(i+look_back), 0])\n",
    "        dataY.append(sample[(i + look_back):(i + look_back + look_forward), 0])\n",
    "\n",
    "        dataY_seasonal.append(dataset_seasonal[(i + look_back):(i + look_back + look_forward)])\n",
    "\n",
    "\n",
    "    return np.array(dataX), np.array(dataY), np.array(dataY_seasonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74f0c2ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:14.704042Z",
     "iopub.status.busy": "2024-10-17T06:26:14.703748Z",
     "iopub.status.idle": "2024-10-17T06:26:14.715806Z",
     "shell.execute_reply": "2024-10-17T06:26:14.714981Z"
    },
    "papermill": {
     "duration": 0.037896,
     "end_time": "2024-10-17T06:26:14.717788",
     "exception": false,
     "start_time": "2024-10-17T06:26:14.679892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset3_arima(sample, look_back, look_forward, sample_overlap, dataset_seasonal,dataset_name):\n",
    "    if(sample_overlap >= look_forward or sample_overlap < 0): sample_overlap = look_forward - 1\n",
    "    print('dddd')\n",
    "    if(look_forward == 1): sample_overlap = 0\n",
    "    # print(\"sample.shape\",sample.shape)\n",
    "    sample_trn=sample[0:len(sample)-look_forward]\n",
    "    frequency=12\n",
    "    if dataset_name=='tourism':\n",
    "        frequency=4\n",
    "    if dataset_name=='cif-6':\n",
    "        frequency=12\n",
    "#     print(\"freq\",frequency)\n",
    "    #         fit1 = pm.auto_arima(sample_trn,trace=True,error_action=\"ignore\",stepwise=False,seasonal=True,m=frequency,information_criterion='aic')#,stepwise=True,information_criterion='aic')\n",
    "\n",
    "    if len(sample_trn) > frequency*2:\n",
    "        \n",
    "        fit1 =pm.auto_arima(sample_trn, seasonal=True, m=12,maxiter=5,n_jobs=-1)\n",
    "        aug_seri, conf_int = fit1.predict(n_periods=look_forward, return_conf_int=True)\n",
    "        aug_seri = aug_seri.reshape(1,-1)\n",
    "           \n",
    "           \n",
    "    elif len(sample_trn)<frequency*2 and len(sample_trn)>frequency :\n",
    "        frequency = int(frequency/2)\n",
    "        fit1 = pm.auto_arima(sample_trn, seasonal=True, m=12,maxiter=5)\n",
    "\n",
    "        aug_seri, conf_int = fit1.predict(n_periods=look_forward, return_conf_int=True)\n",
    "        aug_seri = aug_seri.reshape(1,-1)\n",
    "    else:\n",
    "        fit1 = pm.auto_arima(sample_trn, seasonal=True, m=12,maxiter=5)\n",
    "        aug_seri, conf_int = fit1.predict(n_periods=look_forward, return_conf_int=True)\n",
    "        aug_seri = aug_seri.reshape(1,-1)\n",
    "\n",
    "\n",
    "#     fit1 = ExponentialSmoothing(endog=pd.Series(sample_trn.flatten()), seasonal_periods=12, trend='add',\n",
    "#                                 seasonal='add').fit()\n",
    "#     aug_seri = fit1.forecast(steps=look_forward).values.reshape(1,-1)\n",
    "    # print(aug_seri)\n",
    "    from_train = sample_trn[-(look_back+look_forward-1):]\n",
    "    frm_train_aug=np.concatenate([from_train.reshape(-1,1),aug_seri.reshape(-1,1)])\n",
    "    frm_train_aug=frm_train_aug.flatten()\n",
    "    # print(frm_train_aug)\n",
    "    # print(from_train[:,0])\n",
    "\n",
    "\n",
    "    aug_trainX,aug_trainY=[],[]\n",
    "    for i in range(0, len(frm_train_aug) - look_back - look_forward+1, look_forward - sample_overlap):\n",
    "        aug_trainX.append(frm_train_aug[i:(i+look_back)])\n",
    "        aug_trainY.append(frm_train_aug[(i + look_back):(i + look_back + look_forward)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # dataY_seasonal.append(dataset_seasonal[(i + look_back):(i + look_back + look_forward)])\n",
    "\n",
    "    return np.array(aug_trainX), np.array(aug_trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c4ec478",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:14.764700Z",
     "iopub.status.busy": "2024-10-17T06:26:14.764450Z",
     "iopub.status.idle": "2024-10-17T06:26:14.777867Z",
     "shell.execute_reply": "2024-10-17T06:26:14.776944Z"
    },
    "papermill": {
     "duration": 0.039324,
     "end_time": "2024-10-17T06:26:14.780094",
     "exception": false,
     "start_time": "2024-10-17T06:26:14.740770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset2(sample, look_back, look_forward, sample_overlap, dataset_seasonal,dataset_name):\n",
    "    if(sample_overlap >= look_forward or sample_overlap < 0): sample_overlap = look_forward - 1\n",
    "    if(look_forward == 1): sample_overlap = 0\n",
    "    # print(\"sample.shape\",sample.shape)\n",
    "    sample_trn=sample[0:len(sample)-look_forward]\n",
    "    frequency=12\n",
    "    if dataset_name=='tourism':\n",
    "        frequency=4\n",
    "    if dataset_name=='cif-6':\n",
    "        frequency=12\n",
    "#     print(\"freq\",frequency)\n",
    "    \n",
    "    if len(sample_trn) > frequency*2:\n",
    "        fit1 = ExponentialSmoothing(endog=pd.Series(sample_trn.flatten()), seasonal_periods=frequency, trend='add',\n",
    "                            seasonal='add').fit()\n",
    "        aug_seri = fit1.forecast(steps=look_forward).values.reshape(1,-1)\n",
    "           \n",
    "           \n",
    "    elif len(sample_trn)<frequency*2 and len(sample_trn)>frequency :\n",
    "        frequency = int(frequency/2)\n",
    "        fit1 = ExponentialSmoothing(endog=pd.Series(sample_trn.flatten()), seasonal_periods=frequency, trend='add',\n",
    "                            seasonal='add').fit()\n",
    "        aug_seri = fit1.forecast(steps=look_forward).values.reshape(1,-1)\n",
    "    else:\n",
    "        fit1 = ExponentialSmoothing(endog=pd.Series(sample_trn.flatten())).fit()\n",
    "\n",
    "        aug_seri = fit1.forecast(steps=look_forward).values.reshape(1,-1)\n",
    "\n",
    "\n",
    "#     fit1 = ExponentialSmoothing(endog=pd.Series(sample_trn.flatten()), seasonal_periods=12, trend='add',\n",
    "#                                 seasonal='add').fit()\n",
    "#     aug_seri = fit1.forecast(steps=look_forward).values.reshape(1,-1)\n",
    "    # print(aug_seri)\n",
    "    from_train = sample_trn[-(look_back+look_forward-1):]\n",
    "    frm_train_aug=np.concatenate([from_train.reshape(-1,1),aug_seri.reshape(-1,1)])\n",
    "    frm_train_aug=frm_train_aug.flatten()\n",
    "    # print(frm_train_aug)\n",
    "    # print(from_train[:,0])\n",
    "\n",
    "\n",
    "    aug_trainX,aug_trainY=[],[]\n",
    "    for i in range(0, len(frm_train_aug) - look_back - look_forward+1, look_forward - sample_overlap):\n",
    "        aug_trainX.append(frm_train_aug[i:(i+look_back)])\n",
    "        aug_trainY.append(frm_train_aug[(i + look_back):(i + look_back + look_forward)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # dataY_seasonal.append(dataset_seasonal[(i + look_back):(i + look_back + look_forward)])\n",
    "\n",
    "    return np.array(aug_trainX), np.array(aug_trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4eccd2c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:14.833496Z",
     "iopub.status.busy": "2024-10-17T06:26:14.833173Z",
     "iopub.status.idle": "2024-10-17T06:26:14.841773Z",
     "shell.execute_reply": "2024-10-17T06:26:14.840859Z"
    },
    "papermill": {
     "duration": 0.037116,
     "end_time": "2024-10-17T06:26:14.843772",
     "exception": false,
     "start_time": "2024-10-17T06:26:14.806656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stl_decomposition2(dataset, frequency,look):\n",
    "    seasonal = []\n",
    "    trend = []\n",
    "    for index in range(len(dataset)):\n",
    "        if frequency != None:\n",
    "            stl = STL(dataset[index][:len(dataset[index]) - look], frequency, \"periodic\")\n",
    "            stl_tot = STL(dataset[index], frequency, \"periodic\")\n",
    "\n",
    "            seasonal.append(np.concatenate([stl.seasonal,stl_tot.seasonal[-look:]]))\n",
    "            trend.append(np.concatenate([stl.trend,stl_tot.trend[-look:]]))\n",
    "            # deseason_part=dataset[index][:len(dataset[index]) - look] - stl.seasonal\n",
    "            # d=np.concatenate([deseason_part,dataset[index][-look:]])\n",
    "            dataset[index]=dataset[index]-np.concatenate([stl.seasonal,stl_tot.seasonal[-look:]])\n",
    "        else:\n",
    "            seasonal.append(np.zeros((dataset[index].shape)))\n",
    "            trend.append(np.zeros((dataset[index].shape)))\n",
    "\n",
    "    return dataset, np.array(seasonal), np.array(trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ac6257d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:14.892320Z",
     "iopub.status.busy": "2024-10-17T06:26:14.892078Z",
     "iopub.status.idle": "2024-10-17T06:26:14.899418Z",
     "shell.execute_reply": "2024-10-17T06:26:14.898525Z"
    },
    "papermill": {
     "duration": 0.032859,
     "end_time": "2024-10-17T06:26:14.901259",
     "exception": false,
     "start_time": "2024-10-17T06:26:14.868400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def single_point_mase(actual, forecast, insample, frequency) :\n",
    "    if frequency==None:\n",
    "        frequency=12\n",
    "    # print(\"HHHHHHHHHHHHHHHHHHHHHHHHHHHH\")\n",
    "    return np.mean(np.abs(forecast - actual)) / np.mean(np.abs(insample[:-frequency] - insample[frequency:]))\n",
    "\n",
    "\n",
    "def mase(actual, forecast, insample,frequency):\n",
    "    if frequency==None:\n",
    "        frequency=12\n",
    "    print(\"shapes\",actual.shape,forecast.shape,insample.shape)\n",
    "    # print(insample)\n",
    "    MASE = []\n",
    "    for i in range(len(actual)):\n",
    "        sum_MASE= 0\n",
    "        for j in range(len(actual[i])):\n",
    "            sum_MASE += single_point_mase(actual[i, j], forecast[i, j],insample[i][:-len(actual[i])], frequency)\n",
    "        MASE.append(sum_MASE / len(actual[i]))\n",
    "    return np.array(MASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59495af2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:14.948885Z",
     "iopub.status.busy": "2024-10-17T06:26:14.948560Z",
     "iopub.status.idle": "2024-10-17T06:26:14.954859Z",
     "shell.execute_reply": "2024-10-17T06:26:14.954039Z"
    },
    "papermill": {
     "duration": 0.03245,
     "end_time": "2024-10-17T06:26:14.956757",
     "exception": false,
     "start_time": "2024-10-17T06:26:14.924307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mase_val(actual, forecast, insample,frequency):\n",
    "    if frequency==None:\n",
    "        frequency=12\n",
    "    print(\"shapes\",actual.shape,forecast.shape,insample.shape)\n",
    "    # print(insample)\n",
    "    MASE = []\n",
    "    for i in range(len(actual)):\n",
    "        sum_MASE= 0\n",
    "        for j in range(len(actual[i])):\n",
    "            sum_MASE += single_point_mase(actual[i, j], forecast[i, j],insample[i][:-2*len(actual[i])], frequency)\n",
    "        MASE.append(sum_MASE / len(actual[i]))\n",
    "    return np.array(MASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "247c9cb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:15.004061Z",
     "iopub.status.busy": "2024-10-17T06:26:15.003787Z",
     "iopub.status.idle": "2024-10-17T06:26:15.027150Z",
     "shell.execute_reply": "2024-10-17T06:26:15.026269Z"
    },
    "papermill": {
     "duration": 0.049576,
     "end_time": "2024-10-17T06:26:15.029251",
     "exception": false,
     "start_time": "2024-10-17T06:26:14.979675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_sample(look_forward,sample_seasonal,dataX, dataY, data_mean, dataY_seasonal,frequency):\n",
    "    test_size=1\n",
    "    val_size=1\n",
    "    \n",
    "    # print(frequency)\n",
    "    # print(sample_seasonal)\n",
    "    # test_size=int(len(dataX) * testsize)\n",
    "    # val_size=int((len(dataX) - test_size) * valsize)\n",
    "#     print(dataY)\n",
    "    train_size0=(len(dataX)-test_size)-look_forward+1\n",
    "    train_size=(len(dataX)-test_size)\n",
    "#     print(\"train_size\",train_size)\n",
    "#     trainX, testX = dataX[0:train_size0,:], dataX[train_size:,:]\n",
    "#     trainY, testY = dataY[0:train_size0,:], dataY[train_size:,:]\n",
    "\n",
    "\n",
    "    trainX, testX = dataX[0:train_size0-val_size,:], dataX[train_size:,:]  #note -valsize is added now\n",
    "    trainY, testY = dataY[0:train_size0-val_size,:], dataY[train_size:,:]\n",
    "\n",
    "    valX, valY = dataX[train_size0-val_size:train_size0,:],dataY[train_size0-val_size:train_size0, :]\n",
    "  \n",
    "    #     print(\"valY\",valY)\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0],1, trainX.shape[1]))\n",
    "    valX = np.reshape(valX, (valX.shape[0],1, valX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0],1, testX.shape[1]))\n",
    "\n",
    "    val_means = np.full(len(valY), data_mean)\n",
    "    test_means = np.full(len(testY), data_mean)\n",
    "\n",
    "    val_seasonal = dataY_seasonal[train_size0-val_size:train_size0, :]\n",
    "    # print(\"len val\",len(val_seasonal))\n",
    "    # print(\"val\",val_seasonal)\n",
    "\n",
    "    train=dataY_seasonal[:train_size0, :]\n",
    "    train=train.reshape(-1,1)\n",
    "#     train2=train[:len(train):len(valY[0])]\n",
    "    # print(train2.reshape(1,-1))\n",
    "    # print(len(train2))\n",
    "\n",
    "    test2 = []\n",
    "\n",
    "\n",
    "    # modl = pm.auto_arima(train2, trace=False,seasonal=True, stepwise=False, information_criterion='aic',)\n",
    "    # preds = modl.predict(n_periods=len(valY[0]))\n",
    "    # print(\"xx\",train2.shape)\n",
    "    sample_size = len(sample_seasonal.flatten()) - look_forward\n",
    "\n",
    "    train3=sample_seasonal[:sample_size].flatten()\n",
    "#     print(\"len,frequency\",len(train3.flatten()),frequency)\n",
    "    if frequency!=None:\n",
    "        if len(train3.flatten()) > frequency*2:\n",
    "            sp = frequency\n",
    "           \n",
    "            fit1 = ExponentialSmoothing(endog=pd.Series(train3.flatten()), seasonal_periods=sp,trend='add', seasonal='add').fit()\n",
    "\n",
    "        elif len(train3.flatten())<frequency*2 and len(train3.flatten())>frequency :\n",
    "            sp = int(frequency/2)\n",
    "            fit1 = ExponentialSmoothing(endog=pd.Series(train3.flatten()), seasonal_periods=sp, trend='add',\n",
    "                                        seasonal='add').fit()\n",
    "        else:\n",
    "            fit1 = ExponentialSmoothing(endog=pd.Series(train3.flatten())).fit()\n",
    "\n",
    "        preds2 = fit1.forecast(steps=look_forward).values.reshape(1,-1)\n",
    "\n",
    "        # print(\"preds\",preds2,type(preds2))\n",
    "    else:\n",
    "        preds2=np.zeros(look_forward)\n",
    "#     for i in range(0,len(val_seasonal[0])):\n",
    "#         test2.append(val_seasonal[0][len(val_seasonal[0])-1])\n",
    "\n",
    "    # print(\"datay_seasonal\",dataY_seasonal)\n",
    "\n",
    "    test_seasonal_y = dataY_seasonal[train_size:,:]\n",
    "    # print(\"test\",test_seasonal_y)\n",
    "    # print(\"train2\",train2.flatten())\n",
    "    # np.savetxt('train.csv', train2.flatten(), delimiter=', ')\n",
    "\n",
    "    return np.array(trainX),np.array(valX),np.array(testX),np.array(trainY),np.array(valY),np.array(testY), test_means, val_means, val_seasonal,test_seasonal_y, preds2\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess Data For Sampling\n",
    "def all_pre_process(all_dataset, lag, look_forward, sample_overlap, data_means, dataset_seasonal,frequency):\n",
    "    look_back = lag\n",
    "\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "\n",
    "\n",
    "    valX = []\n",
    "    valY = []\n",
    "\n",
    "    testX = []\n",
    "    testY = []\n",
    "\n",
    "    all_test_means = []\n",
    "    all_val_means = []\n",
    "\n",
    "    all_test_seasonals = []\n",
    "    all_test_seasonals2 = []\n",
    "    all_val_seasonals = []\n",
    "\n",
    "    for index in range(len(all_dataset)):\n",
    "        sample = np.array(all_dataset[index])\n",
    "        sample = sample.reshape(sample.shape[0], 1)\n",
    "        # sample_seasonal=np.array(dataset_seasonal[index])\n",
    "\n",
    "        dataX_s, dataY_s, dataY_seasonal = create_dataset(sample, look_back, look_forward, sample_overlap, dataset_seasonal[index])\n",
    "\n",
    "    \n",
    "        temp_trainX, temp_valX, temp_testX, temp_trainY, temp_valY, temp_testY, test_means, val_means, val_seasonal, test_seasonal,test2 = create_sample(look_forward,dataset_seasonal[index],dataX_s,dataY_s,data_means[index], dataY_seasonal,frequency)\n",
    "\n",
    "        trainX = trainX + temp_trainX.tolist()\n",
    "        trainY = trainY + temp_trainY.tolist()\n",
    "        #for adding augmented data\n",
    "#         trainX = trainX + aug_trainX.tolist()\n",
    "#         trainY = trainY + aug_trainY.tolist()\n",
    "\n",
    "        valX = valX + temp_valX.tolist()\n",
    "        valY = valY + temp_valY.tolist()\n",
    "\n",
    "        testX = testX + temp_testX.tolist()\n",
    "        testY = testY + temp_testY.tolist()\n",
    "\n",
    "        all_test_means = all_test_means + test_means.tolist()\n",
    "        all_val_means = all_val_means + val_means.tolist()\n",
    "        all_test_seasonals = all_test_seasonals +  test_seasonal.tolist()\n",
    "        all_test_seasonals2 = all_test_seasonals2 +test2.tolist()  # test_seasonal.tolist() #\"NOTE\"\n",
    "        all_val_seasonals = all_val_seasonals + val_seasonal.tolist()\n",
    "\n",
    "\n",
    "    return np.array(trainX), np.array(valX), np.array(testX), np.array(trainY), np.array(valY), np.array(testY), np.array(all_test_means), np.array(all_val_means), np.array(all_val_seasonals), np.array(all_test_seasonals),np.array(all_test_seasonals2)\n",
    "\n",
    "def save_prediction_result(data, dataset_name = 'cif-6', dataset_path = ''):\n",
    "    if dataset_name == '':\n",
    "        filename = dataset_name + '-results.csv'\n",
    "    else:\n",
    "        filename = dataset_path + '/' + dataset_name + '-results.csv'\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, sep=',',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b0280e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:15.077459Z",
     "iopub.status.busy": "2024-10-17T06:26:15.076759Z",
     "iopub.status.idle": "2024-10-17T06:26:15.088891Z",
     "shell.execute_reply": "2024-10-17T06:26:15.087966Z"
    },
    "papermill": {
     "duration": 0.038368,
     "end_time": "2024-10-17T06:26:15.090932",
     "exception": false,
     "start_time": "2024-10-17T06:26:15.052564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def all_pre_process_aug(all_dataset, lag, look_forward, sample_overlap, data_means, dataset_seasonal,frequency,dataset_name):\n",
    "    look_back = lag\n",
    "\n",
    "    trainX = []\n",
    "    trainY = []\n",
    " \n",
    "\n",
    "    valX = []\n",
    "    valY = []\n",
    "\n",
    "    testX = []\n",
    "    testY = []\n",
    "\n",
    "    all_test_means = []\n",
    "    all_val_means = []\n",
    "\n",
    "    all_test_seasonals = []\n",
    "    all_test_seasonals2 = []\n",
    "    all_val_seasonals = []\n",
    "\n",
    "    for index in range(len(all_dataset)):\n",
    "        sample = np.array(all_dataset[index])\n",
    "        sample = sample.reshape(sample.shape[0], 1)\n",
    "        # sample_seasonal=np.array(dataset_seasonal[index])\n",
    "\n",
    "        dataX_s, dataY_s, dataY_seasonal = create_dataset(sample, look_back, look_forward, sample_overlap, dataset_seasonal[index])\n",
    "        aug_trainX,aug_trainY = create_dataset2(sample, look_back, look_forward, sample_overlap, dataset_seasonal[index],dataset_name)\n",
    "#         aug_trainX,aug_trainY = create_dataset3_arima(sample, look_back, look_forward, sample_overlap, dataset_seasonal[index],dataset_name)\n",
    "\n",
    "        aug_trainX= np.reshape(aug_trainX, (aug_trainX.shape[0],1, aug_trainX.shape[1]))\n",
    "    \n",
    "        temp_trainX, temp_valX, temp_testX, temp_trainY, temp_valY, temp_testY, test_means, val_means, val_seasonal, test_seasonal,test2 = create_sample(look_forward,dataset_seasonal[index],dataX_s,dataY_s,data_means[index], dataY_seasonal,frequency)\n",
    "        # print(\"temp_trainX.shape\",temp_trainX.shape)\n",
    "        trainX = trainX + temp_trainX.tolist()\n",
    "        trainY = trainY + temp_trainY.tolist()\n",
    "        #for adding augmented data\n",
    "        trainX = trainX + aug_trainX.tolist()\n",
    "        trainY = trainY + aug_trainY.tolist()\n",
    "\n",
    "        valX = valX + temp_valX.tolist()\n",
    "        valY = valY + temp_valY.tolist()\n",
    "\n",
    "        testX = testX + temp_testX.tolist()\n",
    "        testY = testY + temp_testY.tolist()\n",
    "\n",
    "        all_test_means = all_test_means + test_means.tolist()\n",
    "        all_val_means = all_val_means + val_means.tolist()\n",
    "        all_test_seasonals = all_test_seasonals +  test_seasonal.tolist()\n",
    "        all_test_seasonals2 = all_test_seasonals2 +test2.tolist()  # test_seasonal.tolist() #\"NOTE\"\n",
    "        all_val_seasonals = all_val_seasonals + val_seasonal.tolist()\n",
    "\n",
    "\n",
    "    return np.array(trainX), np.array(valX), np.array(testX), np.array(trainY), np.array(valY), np.array(testY), np.array(all_test_means), np.array(all_val_means), np.array(all_val_seasonals), np.array(all_test_seasonals),np.array(all_test_seasonals2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1dd86fdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:15.138889Z",
     "iopub.status.busy": "2024-10-17T06:26:15.138168Z",
     "iopub.status.idle": "2024-10-17T06:26:15.152767Z",
     "shell.execute_reply": "2024-10-17T06:26:15.151892Z"
    },
    "papermill": {
     "duration": 0.040611,
     "end_time": "2024-10-17T06:26:15.154567",
     "exception": false,
     "start_time": "2024-10-17T06:26:15.113956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_sample_sep_val(look_forward,sample_seasonal,dataX, dataY, data_mean, dataY_seasonal,frequency):\n",
    "    test_size=1\n",
    "    val_size=1\n",
    "    # print(frequency)\n",
    "    # print(sample_seasonal)\n",
    "    # test_size=int(len(dataX) * testsize)\n",
    "    # val_size=int((len(dataX) - test_size) * valsize)\n",
    "#     print(\"LENNNN\",len(dataX))\n",
    "    train_size=(len(dataX)-test_size-val_size)\n",
    "#     print(\"trainsize, train+val\",train_size,train_size +val_size)\n",
    "\n",
    "    trainX, testX = dataX[0:train_size,:], dataX[train_size+val_size:,:]\n",
    "    trainY, testY = dataY[0:train_size,:], dataY[train_size+val_size:,:]\n",
    "\n",
    "    valX, valY = dataX[train_size:train_size +val_size,:],dataY[train_size:train_size+val_size, :]\n",
    "#     print(\"LENNNN\",len(valX),len(valY),valX.shape,valY.shape)\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0],1, trainX.shape[1]))\n",
    "    valX = np.reshape(valX, (valX.shape[0],1, valX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0],1, testX.shape[1]))\n",
    "\n",
    "    val_means = np.full(len(valY), data_mean)\n",
    "    test_means = np.full(len(testY), data_mean)\n",
    "\n",
    "    val_seasonal = dataY_seasonal[train_size:train_size+val_size, :]\n",
    "    # print(\"len val\",len(val_seasonal))\n",
    "#     print(\"valseasonal \",val_seasonal.shape)\n",
    "\n",
    "#     train=dataY_seasonal[:train_size, :]\n",
    "#     train=train.reshape(-1,1)\n",
    "   \n",
    "    # print(train2.reshape(1,-1))\n",
    "    # print(len(train2))\n",
    "\n",
    "#     test2 = []\n",
    "\n",
    "\n",
    "    # modl = pm.auto_arima(train2, trace=False,seasonal=True, stepwise=False, information_criterion='aic',)\n",
    "    # preds = modl.predict(n_periods=len(valY[0]))\n",
    "    # print(\"xx\",train2.shape)\n",
    "    sample_size = len(sample_seasonal.flatten()) - look_forward\n",
    "\n",
    "    train3=sample_seasonal[:sample_size].flatten()\n",
    "\n",
    "    if frequency!=None:\n",
    "        if len(train3.flatten()) > frequency*2:\n",
    "            sp = frequency\n",
    "            fit1 = ExponentialSmoothing(endog=pd.Series(train3.flatten()), seasonal_periods=sp,trend='add', seasonal='add').fit()\n",
    "\n",
    "        elif len(train3.flatten())<frequency*2 and len(train3.flatten())>frequency :\n",
    "            sp = int(frequency/2)\n",
    "            fit1 = ExponentialSmoothing(endog=pd.Series(train3.flatten()), seasonal_periods=sp, trend='add',\n",
    "                                        seasonal='add').fit()\n",
    "        else:\n",
    "            fit1 = ExponentialSmoothing(endog=pd.Series(train3.flatten())).fit()\n",
    "\n",
    "        preds2 = fit1.forecast(steps=look_forward).values.reshape(1,-1)\n",
    "\n",
    "        # print(\"preds\",preds2,type(preds2))\n",
    "    else:\n",
    "        preds2=np.zeros(look_forward)\n",
    "#     for i in range(0,len(val_seasonal[0])):\n",
    "#         test2.append(val_seasonal[0][len(val_seasonal[0])-1])\n",
    "\n",
    "    # print(\"datay_seasonal\",dataY_seasonal)\n",
    "\n",
    "    test_seasonal_y = dataY_seasonal[train_size+val_size:,:]\n",
    "    \n",
    "#     print(\"test_seasonal\",test_seasonal_y.shape)\n",
    "    # print(\"train2\",train2.flatten())\n",
    "    # np.savetxt('train.csv', train2.flatten(), delimiter=', ')\n",
    "\n",
    "    return np.array(trainX),np.array(valX),np.array(testX),np.array(trainY),np.array(valY),np.array(testY), test_means, val_means, val_seasonal,test_seasonal_y, preds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "767ff354",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:15.202024Z",
     "iopub.status.busy": "2024-10-17T06:26:15.201509Z",
     "iopub.status.idle": "2024-10-17T06:26:15.212214Z",
     "shell.execute_reply": "2024-10-17T06:26:15.211334Z"
    },
    "papermill": {
     "duration": 0.036573,
     "end_time": "2024-10-17T06:26:15.214120",
     "exception": false,
     "start_time": "2024-10-17T06:26:15.177547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess Data For Sampling\n",
    "def all_pre_process(all_dataset, lag, look_forward, sample_overlap, data_means, dataset_seasonal,frequency):\n",
    "    look_back = lag\n",
    "\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "\n",
    "    valX = []\n",
    "    valY = []\n",
    "\n",
    "    testX = []\n",
    "    testY = []\n",
    "\n",
    "    all_test_means = []\n",
    "    all_val_means = []\n",
    "\n",
    "    all_test_seasonals = []\n",
    "    all_test_seasonals2 = []\n",
    "    all_val_seasonals = []\n",
    "\n",
    "    for index in range(len(all_dataset)):\n",
    "        sample = np.array(all_dataset[index])\n",
    "        sample = sample.reshape(sample.shape[0], 1)\n",
    "        # sample_seasonal=np.array(dataset_seasonal[index])\n",
    "\n",
    "        dataX_s, dataY_s, dataY_seasonal = create_dataset(sample, look_back, look_forward, sample_overlap, dataset_seasonal[index])\n",
    "        temp_trainX, temp_valX, temp_testX, temp_trainY, temp_valY, temp_testY, test_means, val_means, val_seasonal, test_seasonal,test2 = create_sample(look_forward,dataset_seasonal[index],dataX_s,dataY_s,data_means[index], dataY_seasonal,frequency)\n",
    "\n",
    "        trainX = trainX + temp_trainX.tolist()\n",
    "        trainY = trainY + temp_trainY.tolist()\n",
    "\n",
    "        valX = valX + temp_valX.tolist()\n",
    "        valY = valY + temp_valY.tolist()\n",
    "\n",
    "        testX = testX + temp_testX.tolist()\n",
    "        testY = testY + temp_testY.tolist()\n",
    "\n",
    "        all_test_means = all_test_means + test_means.tolist()\n",
    "        all_val_means = all_val_means + val_means.tolist()\n",
    "        all_test_seasonals = all_test_seasonals +  test_seasonal.tolist()\n",
    "        all_test_seasonals2 = all_test_seasonals2 +test2.tolist()  # test_seasonal.tolist() #\"NOTE\"\n",
    "        all_val_seasonals = all_val_seasonals + val_seasonal.tolist()\n",
    "\n",
    "\n",
    "    return np.array(trainX), np.array(valX), np.array(testX), np.array(trainY), np.array(valY), np.array(testY), np.array(all_test_means), np.array(all_val_means), np.array(all_val_seasonals), np.array(all_test_seasonals),np.array(all_test_seasonals2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8af2585",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:15.261863Z",
     "iopub.status.busy": "2024-10-17T06:26:15.261546Z",
     "iopub.status.idle": "2024-10-17T06:26:15.265610Z",
     "shell.execute_reply": "2024-10-17T06:26:15.264681Z"
    },
    "papermill": {
     "duration": 0.030481,
     "end_time": "2024-10-17T06:26:15.267463",
     "exception": false,
     "start_time": "2024-10-17T06:26:15.236982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tcn import TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9bedda2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:15.315432Z",
     "iopub.status.busy": "2024-10-17T06:26:15.315138Z",
     "iopub.status.idle": "2024-10-17T06:26:15.327695Z",
     "shell.execute_reply": "2024-10-17T06:26:15.326953Z"
    },
    "papermill": {
     "duration": 0.039067,
     "end_time": "2024-10-17T06:26:15.329671",
     "exception": false,
     "start_time": "2024-10-17T06:26:15.290604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def autoencoder(latent_dim=16, series_len=None):\n",
    "\n",
    "    # Input\n",
    "    x = Input((None, series_len),name='input')\n",
    "    #masked_seq = layers.Masking(mask_value=0.0)(encoder_inputs)\n",
    "    # Encoder\n",
    "    encoded = LSTM(256, return_sequences=True, name=\"encoded_lstm1\")(x)\n",
    "    encoded = LSTM(128, return_sequences=True, name=\"encoded_lstm2\")(encoded)\n",
    "    attention = Attention(name=\"encoded_atten\")([encoded, encoded, encoded])  # Apply self-attention to the encoder outputs\n",
    "    merged = layers.Concatenate(axis=-1)([encoded, attention])        \n",
    "    encoded = LSTM(64, name=\"encoded_lstm3\")(merged)\n",
    "    encoded = Dense(latent_dim, name=\"encoded_dense\")(encoded)\n",
    "\n",
    "    # Decoder\n",
    "    decoded = RepeatVector(1)(encoded)\n",
    "    decoded = LSTM(64, return_sequences=True, name=\"decoded_lstm1\")(decoded)\n",
    "    decoded = LSTM(128, return_sequences=True, name=\"decoded_lstm2\")(decoded)\n",
    "    decoded = LSTM(256, return_sequences=True, name=\"decoded_lstm3\")(decoded)\n",
    "    attention = Attention(name=\"decoded_atten\")([decoded, decoded, decoded])  # Apply self-attention to the decoder outputs\n",
    "    decoded = layers.Concatenate(axis=-1, name=\"decoded_concat\")([decoded, attention])\n",
    "    decoder_outputs = TimeDistributed(Dense(series_len), name=\"decoded_timeD\")(decoded)\n",
    "    \n",
    "    # AE model\n",
    "    autoencoder = Model(inputs=x, outputs=decoder_outputs, name='AE')\n",
    "\n",
    "    # Encoder model\n",
    "    encoder = Model(inputs=x, outputs=encoded, name='encoder')\n",
    "\n",
    "    # Create input for decoder model\n",
    "    encoded_input =Input(shape=(latent_dim,))\n",
    "    # Internal layers in decoder\n",
    "    decoded = RepeatVector(1)(encoded_input)\n",
    "    decoded = autoencoder.get_layer('decoded_lstm1')(decoded)\n",
    "    decoded = autoencoder.get_layer('decoded_lstm2')(decoded)\n",
    "    decoded = autoencoder.get_layer('decoded_lstm3')(decoded)\n",
    "    attention = autoencoder.get_layer('decoded_atten')([decoded, decoded, decoded])\n",
    "    decoded = autoencoder.get_layer('decoded_concat')([decoded, attention])\n",
    "    decoder_output = autoencoder.get_layer('decoded_timeD')(decoded)\n",
    "    # Decoder model\n",
    "    decoder = Model(inputs=encoded_input, outputs=decoder_output, name='decoder')\n",
    "\n",
    "    return autoencoder, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd52bcd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:15.378790Z",
     "iopub.status.busy": "2024-10-17T06:26:15.378451Z",
     "iopub.status.idle": "2024-10-17T06:26:15.387695Z",
     "shell.execute_reply": "2024-10-17T06:26:15.386731Z"
    },
    "papermill": {
     "duration": 0.036368,
     "end_time": "2024-10-17T06:26:15.389866",
     "exception": false,
     "start_time": "2024-10-17T06:26:15.353498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Prediction_Model(lag, look_forward, learning_rate, dense_neuron):\n",
    "    input_layer = layers.Input(shape = (1, lag,), name = \"Input-Layer\")\n",
    "    multi_head_attention_layer = TCN(return_sequences=True,dilations=[1, 2, 4, 8])(input_layer)\n",
    "    # multi_head_attention_layer=TCN(return_sequences=True)(input_layer)\n",
    "    # dot=keras.layers.Dot(axes=1)([multi_head_attention_layer, multi_head_attention_layer])\n",
    "    # flatten_layer1 = keras.layers.Flatten(name=\"Flatten-Layer\")(multi_head_attention_layer)\n",
    "    conv = keras.layers.Conv1D(64,\n",
    "                              strides=2,\n",
    "                              kernel_size=4,\n",
    "                              activation=None,\n",
    "                              padding=\"same\",)(multi_head_attention_layer)#multi_head_attention_layer\n",
    "    conv2 = keras.layers.Conv1D(16,\n",
    "                              strides=2,\n",
    "                              kernel_size=4,\n",
    "                              activation=None,\n",
    "                              padding=\"same\",)(conv)\n",
    "    flatten_layer2=keras.layers.Flatten(name=\"Flatten-Layer2\")(conv2)\n",
    "    # concat=keras.layers.concatenate([flatten_layer1,flatten_layer2])\n",
    "\n",
    "    dense_layer1 = Dense(\n",
    "                look_forward,\n",
    "                activation = 'linear',\n",
    "                name = \"Fully-Connected-Layer\")(flatten_layer2)\n",
    "\n",
    "    dense_layer2 = Dense(\n",
    "                look_forward,\n",
    "                activation = None,\n",
    "                name = \"Output-Layer\")(dense_layer1)\n",
    "\n",
    "\n",
    "\n",
    "                # Create Model\n",
    "    model = Model(inputs = [input_layer], outputs = dense_layer2)\n",
    "\n",
    "            # Optimizer\n",
    "    opt=tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "            # Compile Model\n",
    "    model.compile(loss=\"mse\",optimizer=opt,metrics=[\"mse\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "552571a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:15.439061Z",
     "iopub.status.busy": "2024-10-17T06:26:15.437960Z",
     "iopub.status.idle": "2024-10-17T06:26:15.450733Z",
     "shell.execute_reply": "2024-10-17T06:26:15.450029Z"
    },
    "papermill": {
     "duration": 0.03959,
     "end_time": "2024-10-17T06:26:15.452688",
     "exception": false,
     "start_time": "2024-10-17T06:26:15.413098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eucl(x, y):\n",
    "    \"\"\"\n",
    "    Euclidean distance between two multivariate time series given as arrays of shape (timesteps, dim)\n",
    "    \"\"\"\n",
    "    d = np.sqrt(np.sum(np.square(x - y), axis=0))\n",
    "    return np.sum(d)\n",
    "\n",
    "\n",
    "def cid(x, y):\n",
    "    \"\"\"\n",
    "    Complexity-Invariant Distance (CID) between two multivariate time series given as arrays of shape (timesteps, dim)\n",
    "    Reference: Batista, Wang & Keogh (2011). A Complexity-Invariant Distance Measure for Time Series. https://doi.org/10.1137/1.9781611972818.60\n",
    "    \"\"\"\n",
    "    assert(len(x.shape) == 2 and x.shape == y.shape)  # time series must have same length and dimensionality\n",
    "    ce_x = np.sqrt(np.sum(np.square(np.diff(x, axis=0)), axis=0) + 1e-9)\n",
    "    ce_y = np.sqrt(np.sum(np.square(np.diff(y, axis=0)), axis=0) + 1e-9)\n",
    "    d = np.sqrt(np.sum(np.square(x - y), axis=0)) * np.divide(np.maximum(ce_x, ce_y), np.minimum(ce_x, ce_y))\n",
    "    return np.sum(d)\n",
    "\n",
    "\n",
    "def cor(x, y):\n",
    "    \"\"\"\n",
    "    Correlation-based distance (COR) between two multivariate time series given as arrays of shape (timesteps, dim)\n",
    "    \"\"\"\n",
    "    scaler = TimeSeriesScalerMeanVariance()\n",
    "    x_norm = scaler.fit_transform(x)\n",
    "    y_norm = scaler.fit_transform(y)\n",
    "    pcc = np.mean(x_norm * y_norm)  # Pearson correlation coefficients\n",
    "    d = np.sqrt(2.0 * (1.0 - pcc + 1e-9))  # correlation-based similarities\n",
    "    return np.sum(d)\n",
    "\n",
    "\n",
    "def acf(x, y):\n",
    "    \"\"\"\n",
    "    Autocorrelation-based distance (ACF) between two multivariate time series given as arrays of shape (timesteps, dim)\n",
    "    Computes a linearly weighted euclidean distance between the autocorrelation coefficients of the input time series.\n",
    "    Reference: Galeano & Pena (2000). Multivariate Analysis in Vector Time Series.\n",
    "    \"\"\"\n",
    "    assert (len(x.shape) == 2 and x.shape == y.shape)  # time series must have same length and dimensionality\n",
    "    x_acf = np.apply_along_axis(lambda z: stattools.acf(z, nlags=z.shape[0]), 0, x)\n",
    "    y_acf = np.apply_along_axis(lambda z: stattools.acf(z, nlags=z.shape[0]), 0, y)\n",
    "    weights = np.linspace(1.0, 0.0, x.shape[0])\n",
    "    d = np.sqrt(np.sum(np.expand_dims(weights, axis=1) * np.square(x_acf - y_acf), axis=0))\n",
    "    return np.sum(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92b8cc07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:15.501681Z",
     "iopub.status.busy": "2024-10-17T06:26:15.501364Z",
     "iopub.status.idle": "2024-10-17T06:26:15.513493Z",
     "shell.execute_reply": "2024-10-17T06:26:15.512600Z"
    },
    "papermill": {
     "duration": 0.038883,
     "end_time": "2024-10-17T06:26:15.515545",
     "exception": false,
     "start_time": "2024-10-17T06:26:15.476662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cluster_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate unsupervised clustering accuracy. Requires scikit-learn installed\n",
    "\n",
    "    # Arguments\n",
    "        y_true: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
    "    return w[row_ind, col_ind].sum() * 1.0 / y_pred.size\n",
    "\n",
    "\n",
    "def cluster_purity(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering purity\n",
    "\n",
    "    # Arguments\n",
    "        y_true: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "\n",
    "    # Return\n",
    "        purity, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    label_mapping = w.argmax(axis=1)\n",
    "    y_pred_voted = y_pred.copy()\n",
    "    for i in range(y_pred.size):\n",
    "        y_pred_voted[i] = label_mapping[y_pred[i]]\n",
    "    return metrics.accuracy_score(y_pred_voted, y_true)\n",
    "\n",
    "\n",
    "def roc_auc(y_true, q_pred, n_classes):\n",
    "    \"\"\"\n",
    "    Calculate area under ROC curve (ROC AUC)\n",
    "    WARNING: DO NOT USE, MAY CONTAIN ERRORS\n",
    "    TODO: CHECK IT!\n",
    "\n",
    "    # Arguments\n",
    "        y_true: true labels, numpy.array with shape `(n_samples,)`\n",
    "        q_pred: predicted probabilities, numpy.array with shape `(n_samples,)`\n",
    "\n",
    "    # Return\n",
    "        ROC AUC score, in [0,1]\n",
    "    \"\"\"\n",
    "    if n_classes == 2:  # binary ROC AUC\n",
    "        auc = max(metrics.roc_auc_score(y_true, q_pred[:, 1]), metrics.roc_auc_score(y_true, q_pred[:, 0]))\n",
    "    else:  # micro-averaged ROC AUC (multiclass)\n",
    "        fpr, tpr, _ = metrics.roc_curve(label_binarize(y_true, classes=np.unique(y_true)).ravel(), q_pred.ravel())\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0508857a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:15.562907Z",
     "iopub.status.busy": "2024-10-17T06:26:15.562645Z",
     "iopub.status.idle": "2024-10-17T06:26:15.566544Z",
     "shell.execute_reply": "2024-10-17T06:26:15.565587Z"
    },
    "papermill": {
     "duration": 0.03017,
     "end_time": "2024-10-17T06:26:15.568595",
     "exception": false,
     "start_time": "2024-10-17T06:26:15.538425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# self.latent_shape = 16 dim  batch epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd7f160f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:15.618854Z",
     "iopub.status.busy": "2024-10-17T06:26:15.618549Z",
     "iopub.status.idle": "2024-10-17T06:26:15.640085Z",
     "shell.execute_reply": "2024-10-17T06:26:15.639319Z"
    },
    "papermill": {
     "duration": 0.048402,
     "end_time": "2024-10-17T06:26:15.641961",
     "exception": false,
     "start_time": "2024-10-17T06:26:15.593559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TSClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, timesteps, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "        dist_metric: distance metric between sequences used in similarity kernel ('eucl', 'cir', 'cor' or 'acf').\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(n_samples, timesteps, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, dist_metric='eucl', **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(TSClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.dist_metric = dist_metric\n",
    "        self.initial_weights = weights\n",
    "        self.clusters = None\n",
    "        self.built = False\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_shape[1]))\n",
    "        self.clusters = self.add_weight(shape=(self.n_clusters, input_shape[1]), initializer='glorot_uniform', name='cluster_centers')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    '''def call(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Student t-distribution kernel, probability of assigning encoded sequence i to cluster k.\n",
    "            q_{ik} = (1 + dist(z_i, m_k)^2)^{-1} / normalization.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: encoded input sequences, shape=(n_samples, timesteps, n_features)\n",
    "        Return:\n",
    "            q: soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        if self.dist_metric == 'eucl':\n",
    "            distance = K.sum(K.sqrt(K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=1)), axis=-1)\n",
    "        elif self.dist_metric == 'cid':\n",
    "            ce_x = K.sqrt(K.sum(K.square(inputs[:, 1:, :] - inputs[:, :-1, :]), axis=1))  # shape (n_samples, n_features)\n",
    "            ce_w = K.sqrt(K.sum(K.square(self.clusters[:, 1:, :] - self.clusters[:, :-1, :]), axis=1))  # shape (n_clusters, n_features)\n",
    "            ce = K.maximum(K.expand_dims(ce_x, axis=1), ce_w) / K.minimum(K.expand_dims(ce_x, axis=1), ce_w)  # shape (n_samples, n_clusters, n_features)\n",
    "            ed = K.sqrt(K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2))  # shape (n_samples, n_clusters, n_features)\n",
    "            distance = K.sum(ed * ce, axis=-1)  # shape (n_samples, n_clusters)\n",
    "        elif self.dist_metric == 'cor':\n",
    "            inputs_norm = (inputs - K.expand_dims(K.mean(inputs, axis=1), axis=1)) / K.expand_dims(K.std(inputs, axis=1), axis=1)  # shape (n_samples, timesteps, n_features)\n",
    "            clusters_norm = (self.clusters - K.expand_dims(K.mean(self.clusters, axis=1), axis=1)) / K.expand_dims(K.std(self.clusters, axis=1), axis=1)  # shape (n_clusters, timesteps, n_features)\n",
    "            pcc = K.mean(K.expand_dims(inputs_norm, axis=1) * clusters_norm, axis=2)  # Pearson correlation coefficients\n",
    "            distance = K.sum(K.sqrt(2.0 * (1.0 - pcc)), axis=-1)  # correlation-based similarities, shape (n_samples, n_clusters)\n",
    "        elif self.dist_metric == 'acf':\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise ValueError('Available distances are eucl, cid, cor and acf!')\n",
    "        q = 1.0 / (1.0 + K.square(distance) / self.alpha)\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q'''\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Student t-distribution kernel, probability of assigning encoded sequence i to cluster k.\n",
    "            q_{ik} = (1 + dist(z_i, m_k)^2)^{-1} / normalization.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: encoded input sequences, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        inputs_expanded = K.expand_dims(inputs, axis=1)  # shape=(n_samples, 1, n_features)\n",
    "        clusters_expanded = K.expand_dims(self.clusters, axis=0)  # shape=(1, n_clusters, n_features)\n",
    "\n",
    "        if self.dist_metric == 'eucl':\n",
    "            distance = K.sqrt(K.sum(K.square(inputs_expanded - clusters_expanded), axis=-1))\n",
    "        elif self.dist_metric == 'cid':\n",
    "            ce_x = K.sqrt(K.sum(K.square(inputs_expanded - K.expand_dims(inputs[:, :-1], axis=2)), axis=-1))  # shape (n_samples, timesteps)\n",
    "            ce_w = K.sqrt(K.sum(K.square(clusters_expanded - K.expand_dims(self.clusters[:, :-1], axis=1)), axis=-1))  # shape (n_clusters, timesteps)\n",
    "            ce = K.maximum(K.expand_dims(ce_x, axis=1), ce_w) / K.minimum(K.expand_dims(ce_x, axis=1), ce_w)  # shape (n_samples, n_clusters, timesteps)\n",
    "            ed = K.sqrt(K.sum(K.square(inputs_expanded - clusters_expanded), axis=-1))  # shape (n_samples, n_clusters)\n",
    "            distance = K.sum(ed * ce, axis=-1)  # shape (n_samples, n_clusters)\n",
    "        elif self.dist_metric == 'cor':\n",
    "            inputs_norm = (inputs - K.mean(inputs, axis=1, keepdims=True)) / K.std(inputs, axis=1, keepdims=True)  # shape (n_samples, n_features)\n",
    "            clusters_norm = (self.clusters - K.mean(self.clusters, axis=1, keepdims=True)) / K.std(self.clusters, axis=1, keepdims=True)  # shape (n_clusters, n_features)\n",
    "            pcc = K.mean(K.expand_dims(inputs_norm, axis=1) * K.expand_dims(clusters_norm, axis=0), axis=-1)  # Pearson correlation coefficients\n",
    "            distance = K.sqrt(2.0 * (1.0 - pcc))  # correlation-based similarities, shape (n_samples, n_clusters)\n",
    "        elif self.dist_metric == 'acf':\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise ValueError('Available distances are eucl, cid, cor and acf!')\n",
    "\n",
    "        q = 1.0 / (1.0 + K.square(distance) / self.alpha)\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 3\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters, 'dist_metric': self.dist_metric}\n",
    "        base_config = super(TSClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3f606ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:15.689698Z",
     "iopub.status.busy": "2024-10-17T06:26:15.689427Z",
     "iopub.status.idle": "2024-10-17T06:26:15.749464Z",
     "shell.execute_reply": "2024-10-17T06:26:15.748716Z"
    },
    "papermill": {
     "duration": 0.086291,
     "end_time": "2024-10-17T06:26:15.751380",
     "exception": false,
     "start_time": "2024-10-17T06:26:15.665089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DTC:\n",
    "    def __init__(self, n_clusters, input_dim, timesteps,max_dim=None,dataset_name=None,\n",
    "                 alpha=1.0, dist_metric='eucl', cluster_init='kmeans', heatmap=False):\n",
    "        self.n_clusters = 2\n",
    "        self.input_dim = None\n",
    "        self.timesteps = 791\n",
    "        self.max_dim=max_dim\n",
    "        self.dataset_name=dataset_name\n",
    "        self.latent_shape = 16\n",
    "        self.alpha = alpha\n",
    "        self.dist_metric = dist_metric\n",
    "        self.cluster_init = cluster_init\n",
    "        self.heatmap = heatmap\n",
    "        self.pretrained = False\n",
    "        self.alpha2 = 0.8\n",
    "        self.learning_rate = None\n",
    "        self.optimizer = keras.optimizers.Adam()\n",
    "        self.model = self.autoencoder = self.encoder = self.decoder = self.predmodel =  None\n",
    "        \n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Create DTC model\n",
    "        \"\"\"\n",
    "        # Create AE models\n",
    "        self.autoencoder, self.encoder, self.decoder = autoencoder(series_len=self.max_dim)\n",
    "        clustering_layer = TSClusteringLayer(self.n_clusters,\n",
    "                                             alpha=self.alpha,\n",
    "                                             dist_metric=self.dist_metric,\n",
    "                                             name='TSClustering')(self.encoder.output)\n",
    "        self.predmodel\n",
    "        # Create DTC model\n",
    "        self.model = Model(inputs=[self.autoencoder.input],\n",
    "                               outputs=[self.autoencoder.output, clustering_layer])\n",
    "\n",
    "    @property\n",
    "    def cluster_centers_(self):\n",
    "        \"\"\"\n",
    "        Returns cluster centers\n",
    "        \"\"\"\n",
    "        return self.model.get_layer(name='TSClustering').get_weights()[0]\n",
    "    \n",
    "    def reconstruction_loss(self, data, reconstruction):\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.mean_squared_error(data, reconstruction), axis=(0, 1)\n",
    "                )\n",
    "            )\n",
    "        return reconstruction_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def weighted_kld(loss_weight):\n",
    "        \"\"\"\n",
    "        Custom KL-divergence loss with a variable weight parameter\n",
    "        \"\"\"\n",
    "        def loss(y_true, y_pred):\n",
    "            return loss_weight * kullback_leibler_divergence(y_true, y_pred)\n",
    "        return loss\n",
    "    \n",
    "    def loss_kld(self, y_true, y_pred):\n",
    "        return kullback_leibler_divergence(y_true, y_pred)\n",
    "    \n",
    "    def on_epoch_end(self, epoch):\n",
    "        \"\"\"\n",
    "        Update heatmap loss weight on epoch end\n",
    "        \"\"\"\n",
    "        if epoch > self.finetune_heatmap_at_epoch:\n",
    "            K.set_value(self.heatmap_loss_weight, self.final_heatmap_loss_weight)\n",
    "\n",
    "    def compile(self, gamma, optimizer, initial_heatmap_loss_weight=None, final_heatmap_loss_weight=None):\n",
    "        \"\"\"\n",
    "        Compile DTC model\n",
    "\n",
    "        # Arguments\n",
    "            gamma: coefficient of TS clustering loss\n",
    "            optimizer: optimization algorithm\n",
    "            initial_heatmap_loss_weight (optional): initial weight of heatmap loss vs clustering loss\n",
    "            final_heatmap_loss_weight (optional): final weight of heatmap loss vs clustering loss (heatmap finetuning)\n",
    "        \"\"\"\n",
    "        if self.heatmap:\n",
    "            self.initial_heatmap_loss_weight = initial_heatmap_loss_weight\n",
    "            self.final_heatmap_loss_weight = final_heatmap_loss_weight\n",
    "            self.heatmap_loss_weight = K.variable(self.initial_heatmap_loss_weight)\n",
    "            self.model.compile(loss=['mse', DTC.weighted_kld(1.0 - self.heatmap_loss_weight), DTC.weighted_kld(self.heatmap_loss_weight)],\n",
    "                               loss_weights=[1.0, gamma, gamma],\n",
    "                               optimizer=optimizer)\n",
    "        else:\n",
    "            self.model.compile(loss=['mse', 'kld'],\n",
    "                               loss_weights=[1.0, gamma],\n",
    "                               optimizer=optimizer)\n",
    "\n",
    "    def load_weights(self, weights_path):\n",
    "        \"\"\"\n",
    "        Load pre-trained weights of DTC model\n",
    "\n",
    "        # Arguments\n",
    "            weight_path: path to weights file (.h5)\n",
    "        \"\"\"\n",
    "        self.model.load_weights(weights_path)\n",
    "        self.pretrained = True\n",
    "\n",
    "    def load_ae_weights(self, ae_weights_path):\n",
    "        \"\"\"\n",
    "        Load pre-trained weights of AE\n",
    "\n",
    "        # Arguments\n",
    "            ae_weight_path: path to weights file (.h5)\n",
    "        \"\"\"\n",
    "        self.autoencoder.load_weights(ae_weights_path)\n",
    "        self.pretrained = True\n",
    "\n",
    "    def dist(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Compute distance between two multivariate time series using chosen distance metric\n",
    "\n",
    "        # Arguments\n",
    "            x1: first input (np array)\n",
    "            x2: second input (np array)\n",
    "        # Return\n",
    "            distance\n",
    "        \"\"\"\n",
    "        if self.dist_metric == 'eucl':\n",
    "            return tsdistances.eucl(x1, x2)\n",
    "        elif self.dist_metric == 'cid':\n",
    "            return tsdistances.cid(x1, x2)\n",
    "        elif self.dist_metric == 'cor':\n",
    "            return tsdistances.cor(x1, x2)\n",
    "        elif self.dist_metric == 'acf':\n",
    "            return tsdistances.acf(x1, x2)\n",
    "        else:\n",
    "            raise ValueError('Available distances are eucl, cid, cor and acf!')\n",
    "\n",
    "    def init_cluster_weights(self, X):\n",
    "        \"\"\"\n",
    "        Initialize with complete-linkage hierarchical clustering or k-means.\n",
    "\n",
    "        # Arguments\n",
    "            X: numpy array containing training set or batch\n",
    "        \"\"\"\n",
    "        assert(self.cluster_init in ['hierarchical', 'kmeans'])\n",
    "        print('Initializing cluster...')\n",
    "\n",
    "        features = self.encode(X)\n",
    "\n",
    "        if self.cluster_init == 'hierarchical':\n",
    "            if self.dist_metric == 'eucl':  # use AgglomerativeClustering off-the-shelf\n",
    "                hc = AgglomerativeClustering(n_clusters=self.n_clusters,\n",
    "                                             affinity='euclidean',\n",
    "                                             linkage='complete').fit(features.reshape(features.shape[0], -1))\n",
    "            else:  # compute distance matrix using dist\n",
    "                d = np.zeros((features.shape[0], features.shape[0]))\n",
    "                for i in range(features.shape[0]):\n",
    "                    for j in range(i):\n",
    "                        d[i, j] = d[j, i] = self.dist(features[i], features[j])\n",
    "                hc = AgglomerativeClustering(n_clusters=self.n_clusters,\n",
    "                                             affinity='precomputed',\n",
    "                                             linkage='complete').fit(d)\n",
    "            # compute centroid\n",
    "            cluster_centers = np.array([features[hc.labels_ == c].mean(axis=0) for c in range(self.n_clusters)])\n",
    "        elif self.cluster_init == 'kmeans':\n",
    "            # fit k-means on flattened features\n",
    "            km = KMeans(n_clusters=self.n_clusters, n_init=10).fit(features.reshape(features.shape[0], -1))\n",
    "            cluster_centers = km.cluster_centers_.reshape(self.n_clusters, features.shape[1])\n",
    "\n",
    "        self.model.get_layer(name='TSClustering').set_weights([cluster_centers])\n",
    "        print('Done!')\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encoding function. Extract latent features from hidden layer\n",
    "\n",
    "        # Arguments\n",
    "            x: data point\n",
    "        # Return\n",
    "            encoded (latent) data point\n",
    "        \"\"\"\n",
    "        return self.encoder.predict(x)\n",
    "\n",
    "    def decode(self, x):\n",
    "        \"\"\"\n",
    "        Decoding function. Decodes encoded sequence from latent space.\n",
    "\n",
    "        # Arguments\n",
    "            x: encoded (latent) data point\n",
    "        # Return\n",
    "            decoded data point\n",
    "        \"\"\"\n",
    "        return self.decoder.predict(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict cluster assignment.\n",
    "\n",
    "        \"\"\"\n",
    "        q = self.model.predict(x, verbose=0)[1]\n",
    "        return q.argmax(axis=1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def target_distribution(q):  # target distribution p which enhances the discrimination of soft label q\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def predict_heatmap(self, x):\n",
    "        \"\"\"\n",
    "        Produces TS clustering heatmap from input sequence.\n",
    "\n",
    "        # Arguments\n",
    "            x: data point\n",
    "        # Return\n",
    "            heatmap\n",
    "        \"\"\"\n",
    "        return self.heatmap_model.predict(x, verbose=0)\n",
    "\n",
    "    def pretrain(self, X,\n",
    "                 optimizer='adam',\n",
    "                 epochs=10,\n",
    "                 batch_size=64,\n",
    "                 save_dir='results/tmp',\n",
    "                 verbose=1):\n",
    "        \"\"\"\n",
    "        Pre-train the autoencoder using only MSE reconstruction loss\n",
    "        Saves weights in h5 format.\n",
    "\n",
    "        # Arguments\n",
    "            X: training set\n",
    "            optimizer: optimization algorithm\n",
    "            epochs: number of pre-training epochs\n",
    "            batch_size: training batch size\n",
    "            save_dir: path to existing directory where weights will be saved\n",
    "        \"\"\"\n",
    "        print('Pretraining...')\n",
    "        self.autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        # Begin pretraining\n",
    "        #t0 = time()\n",
    "        self.autoencoder.fit(X, X, batch_size=batch_size, epochs=epochs)\n",
    "        #print('Pretraining time: ', time() - t0)\n",
    "        self.autoencoder.save_weights('{}/ae_weights-epoch{}.h5'.format(save_dir, epochs))\n",
    "        print('Pretrained weights are saved to {}/ae_weights-epoch{}.h5'.format(save_dir, epochs))\n",
    "        self.pretrained = True\n",
    "        \n",
    "    def mutate_clustering(self, clustering, mutation_rate=0.3):\n",
    "        new_clustering = clustering.copy()\n",
    "        n_samples = len(clustering)\n",
    "        for i in range(n_samples):\n",
    "            if np.random.rand() < mutation_rate:\n",
    "                new_clustering[i] = 1 - new_clustering[i]\n",
    "        return new_clustering\n",
    "\n",
    "    def crossover_clustering(self, clustering1, clustering2, crossover_rate=0.5):\n",
    "        n_samples = len(clustering1)\n",
    "        new_clustering = clustering1.copy()\n",
    "        for i in range(n_samples):\n",
    "            if np.random.rand() < crossover_rate:\n",
    "                new_clustering[i] = clustering2[i]\n",
    "        return new_clustering\n",
    "    \n",
    "    \n",
    "    def save_metrics(self, metrics, file_path='metrics.csv'):\n",
    "        \"\"\"\n",
    "        Save training metrics to a CSV file.\n",
    "\n",
    "        # Arguments\n",
    "            metrics: List of dictionaries containing metrics for each epoch\n",
    "            file_path: Path to the CSV file where metrics will be saved\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(metrics)\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f'Metrics saved to {file_path}')\n",
    "    \n",
    "    def fit(self, X_train, y_train=None, X_val=None, y_val=None,\n",
    "        epochs=10,\n",
    "        eval_epochs=10,\n",
    "        save_epochs=10,\n",
    "        batch_size=64,\n",
    "        tol=0.001,\n",
    "        patience=5,\n",
    "        finetune_heatmap_at_epoch=8,\n",
    "        save_dir='results/tmp',\n",
    "        mutation_rate=0.01,\n",
    "        crossover_rate=0.5,\n",
    "        num_iterations=5):\n",
    "        \n",
    "    \n",
    "            if not self.pretrained:\n",
    "                print('Autoencoder was not pre-trained!')\n",
    "\n",
    "            calculations_method = 'per_series' # single_value | per_series\n",
    "\n",
    "            y_pred_last = None\n",
    "            patience_cnt = 0\n",
    "            best = []\n",
    "            print('Training for {} epochs.\\nEvaluating every {} and saving model every {} epochs.'.format(epochs, eval_epochs, save_epochs))\n",
    "            best_val_error = float('inf')\n",
    "            # Metrics storage\n",
    "            metrics = []\n",
    "            for epoch in range(epochs):\n",
    "                dataset, features, lag, look_forward, sample_overlap, learning_rate, dataset_path, suilin_smape, frequency = get_dataset_params(self.dataset_name)\n",
    "                dataset, data_means = normalize_dataset(dataset, look_forward=look_forward)\n",
    "                dataset, seasonal, trend = stl_decomposition2(dataset, frequency, look_forward)\n",
    "                dataset = np.array(dataset)\n",
    "\n",
    "                # Initial clustering\n",
    "                q = self.model.predict(X_train)[1]\n",
    "                p = DTC.target_distribution(q)\n",
    "                p_pred = p.argmax(axis=1)\n",
    "                best_clustering = p_pred.copy()\n",
    "                new_clustering = best_clustering\n",
    "                for _ in range(num_iterations):  # Number of mutation and crossover iterations\n",
    "                    testing_RMSE = np.array([])\n",
    "                    validation_RMSE = np.array([])\n",
    "                    testing_SMAPE = np.array([])\n",
    "                    validation_SMAPE = np.array([])\n",
    "                    validation_loss_update = []\n",
    "                    for cluster_label in range(self.n_clusters):\n",
    "                        idx = [x for x in range(len(p_pred)) if p_pred[x] == cluster_label]\n",
    "                        cluster_dataset = np.array(dataset)[idx]\n",
    "                        cluster_dataset_means = data_means[idx]\n",
    "                        cluster_dataset_seasonal = seasonal[idx]\n",
    "                        self.predmodel = Prediction_Model(lag, look_forward, learning_rate,look_forward)\n",
    "\n",
    "                        look_back = lag\n",
    "                        calculations_method = 'per_series' \n",
    "\n",
    "                        trainX, valX, testX, trainY, valY, testY, test_means, val_means, val_seasonal, test_seasonal, test_seasonal2 = all_pre_process_aug(cluster_dataset, lag, \n",
    "                        look_forward, sample_overlap, cluster_dataset_means, cluster_dataset_seasonal, frequency, dataset_name = self.dataset_name)\n",
    "\n",
    "                        history = self.predmodel.fit([trainX], trainY, validation_data=([valX, valY]),\n",
    "                                                     epochs=5,\n",
    "                                                     verbose=0,\n",
    "                                                     batch_size=40).history\n",
    "\n",
    "                        val_prediction_results = self.predmodel.predict([valX], batch_size=16, verbose=0)\n",
    "                        val_RMSE = root_mean_squared_error(valY, val_prediction_results, calculations_method)\n",
    "                        val_SMAPE = smape(valY, val_prediction_results, calculations_method, suilin_smape)\n",
    "\n",
    "                        test_prediction_results = self.predmodel.predict([testX], batch_size=16, verbose=0)\n",
    "                        test_RMSE = root_mean_squared_error(testY, test_prediction_results, calculations_method)\n",
    "                        test_SMAPE = smape(testY, test_prediction_results, calculations_method, suilin_smape)\n",
    "\n",
    "                        validation_loss_update.extend(val_RMSE)\n",
    "\n",
    "                        rescaled_valY = rescale_data_to_main_value(valY, val_means, val_seasonal)\n",
    "                        rescaled_val_prediction_results = rescale_data_to_main_value(val_prediction_results, val_means, val_seasonal)\n",
    "                        val_SMAPE = smape(rescaled_valY, rescaled_val_prediction_results, calculations_method, suilin_smape)\n",
    "                        val_RMSE = root_mean_squared_error(rescaled_valY, rescaled_val_prediction_results, calculations_method)\n",
    "\n",
    "                        rescaled_testY = rescale_data_to_main_value(testY, test_means, test_seasonal)\n",
    "                        rescaled_test_prediction_results = rescale_data_to_main_value(test_prediction_results, test_means, test_seasonal2)\n",
    "                        test_SMAPE = smape(rescaled_testY, rescaled_test_prediction_results, calculations_method)\n",
    "                        test_RMSE = root_mean_squared_error(rescaled_testY, rescaled_test_prediction_results, calculations_method)\n",
    "                        \n",
    "                        validation_RMSE = np.concatenate((validation_RMSE, val_RMSE))\n",
    "                        testing_RMSE = np.concatenate((testing_RMSE, test_RMSE))\n",
    "                        validation_SMAPE = np.concatenate((validation_SMAPE, val_SMAPE))\n",
    "                        testing_SMAPE = np.concatenate((testing_SMAPE, test_SMAPE))\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                    validation_loss_update = np.array(validation_loss_update).astype(np.float32)\n",
    "\n",
    "                    total_val_error = np.mean(validation_loss_update)\n",
    "                    if np.mean(validation_SMAPE) < best_val_error:\n",
    "                        best_val_error = np.mean(validation_SMAPE)\n",
    "                        best_clustering = new_clustering\n",
    "                        best.append(best_clustering)\n",
    "                        print(\"list: \", best)\n",
    "                        print(\"Best Clusters: \", best_clustering)\n",
    "                        print(\"Validation SMAPE: \", np.mean(validation_SMAPE))\n",
    "                        print(\"Test SMAPE: \", np.mean(testing_SMAPE))\n",
    "                        print(\"Validation RMSE: \", np.mean(validation_RMSE))\n",
    "                        print(\"Test RMSE: \", np.mean(testing_RMSE))\n",
    "                        \n",
    "                        # Record metrics for this epoch\n",
    "                        metrics.append({\n",
    "                            'Epoch': epoch + 1,\n",
    "                            'Validation SMAPE': np.mean(validation_SMAPE),\n",
    "                            'Test SMAPE': np.mean(testing_SMAPE),\n",
    "                            'Validation RMSE': np.mean(validation_RMSE),\n",
    "                            'Test RMSE': np.mean(testing_RMSE),\n",
    "                        })\n",
    "                        # Custom training step\n",
    "                        with tf.GradientTape() as tape:\n",
    "                            outputs = self.model(X_train, training=True)\n",
    "                            reconstruction_output, clustering_output = outputs\n",
    "\n",
    "                            reconstruction_loss = self.reconstruction_loss(X_train, reconstruction_output)\n",
    "                            clustering_loss = self.loss_kld(p, clustering_output)\n",
    "\n",
    "                            validation_loss_update = np.array(validation_loss_update).astype(np.float32)\n",
    "                            total_loss = ((1 - self.alpha2) * (tf.reduce_sum(reconstruction_loss) + tf.reduce_sum(clustering_loss))) + (self.alpha2 * tf.reduce_sum(validation_loss_update))\n",
    "                            total_loss = tf.reduce_sum(total_loss)\n",
    "                            print('Total Loss',total_loss)\n",
    "                            print('reconstruction_loss',tf.reduce_sum(reconstruction_loss))\n",
    "                            print('clustering loss', tf.reduce_sum(clustering_loss))\n",
    "                            print('Validation Loss',tf.reduce_sum(validation_loss_update))\n",
    "\n",
    "                        grads = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "                        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "                    new_clustering = self.mutate_clustering(best_clustering)\n",
    "                    new_clustering = self.crossover_clustering(new_clustering, p_pred)\n",
    "                    p_pred = new_clustering\n",
    "                    print(\"Modified Clustering: \", p_pred)\n",
    "                    \n",
    "                print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "                print(\"Validation SMAPE: \", np.mean(validation_SMAPE))\n",
    "                print(\"Test SMAPE: \", np.mean(testing_SMAPE))\n",
    "                print(\"Validation RMSE: \", np.mean(validation_RMSE))\n",
    "                print(\"Test RMSE: \", np.mean(testing_SMAPE))\n",
    "                print(\"Total Loss: \", total_loss)\n",
    "            self.save_metrics(metrics, file_path='metrics.csv')\n",
    "            np.save('BestCluster.npy', np.array(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4ed908b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:15.799389Z",
     "iopub.status.busy": "2024-10-17T06:26:15.798835Z",
     "iopub.status.idle": "2024-10-17T06:26:15.892167Z",
     "shell.execute_reply": "2024-10-17T06:26:15.891279Z"
    },
    "papermill": {
     "duration": 0.119231,
     "end_time": "2024-10-17T06:26:15.894234",
     "exception": false,
     "start_time": "2024-10-17T06:26:15.775003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Series</th>\n",
       "      <th>N</th>\n",
       "      <th>NF</th>\n",
       "      <th>Category</th>\n",
       "      <th>Starting Year</th>\n",
       "      <th>Starting Month</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>...</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N1402</td>\n",
       "      <td>68</td>\n",
       "      <td>18</td>\n",
       "      <td>MICRO</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N1403</td>\n",
       "      <td>68</td>\n",
       "      <td>18</td>\n",
       "      <td>MICRO</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>1920.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N1404</td>\n",
       "      <td>68</td>\n",
       "      <td>18</td>\n",
       "      <td>MICRO</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>4860.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N1405</td>\n",
       "      <td>68</td>\n",
       "      <td>18</td>\n",
       "      <td>MICRO</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>180.0</td>\n",
       "      <td>940.0</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N1406</td>\n",
       "      <td>68</td>\n",
       "      <td>18</td>\n",
       "      <td>MICRO</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1550.0</td>\n",
       "      <td>4450.0</td>\n",
       "      <td>3050.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Series   N  NF      Category  Starting Year  Starting Month       1       2  \\\n",
       "0  N1402  68  18  MICRO                  1990               1  2640.0  2640.0   \n",
       "1  N1403  68  18  MICRO                  1990               1  1680.0  1920.0   \n",
       "2  N1404  68  18  MICRO                  1990               1  1140.0   720.0   \n",
       "3  N1405  68  18  MICRO                  1990               1   180.0   940.0   \n",
       "4  N1406  68  18  MICRO                  1990               1  2000.0  1550.0   \n",
       "\n",
       "        3       4  ...  135  136  137  138  139  140  141  142  143  144  \n",
       "0  2160.0  4200.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1   120.0  1080.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "2  4860.0  1200.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "3  2040.0   800.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "4  4450.0  3050.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 150 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3 = pd.read_csv(\"/kaggle/input/m3monthdataset/M3Month.csv\")\n",
    "m3.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6df180e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:15.945535Z",
     "iopub.status.busy": "2024-10-17T06:26:15.945209Z",
     "iopub.status.idle": "2024-10-17T06:26:15.960817Z",
     "shell.execute_reply": "2024-10-17T06:26:15.959813Z"
    },
    "papermill": {
     "duration": 0.043996,
     "end_time": "2024-10-17T06:26:15.963491",
     "exception": false,
     "start_time": "2024-10-17T06:26:15.919495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MICRO       \n",
      "INDUSTRY    \n",
      "MACRO       \n",
      "FINANCE     \n",
      "DEMOGRAPHIC \n",
      "OTHER       \n"
     ]
    }
   ],
   "source": [
    "class_dataframes = {}  # A dictionary to store DataFrames for each class\n",
    "\n",
    "# Iterate over unique class labels in the 'category' column\n",
    "for class_label in m3['Category'].unique():\n",
    "    print(class_label)\n",
    "    # Filter the original DataFrame for rows with the current class label\n",
    "    class_df = m3[m3['Category'] == class_label]\n",
    "    class_label = class_label.replace(\" \", \"\")\n",
    "    # Store the class-specific DataFrame in the dictionary with the class label as the key\n",
    "    class_dataframes[class_label] = class_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e079e07c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:16.014904Z",
     "iopub.status.busy": "2024-10-17T06:26:16.014617Z",
     "iopub.status.idle": "2024-10-17T06:26:16.020214Z",
     "shell.execute_reply": "2024-10-17T06:26:16.019334Z"
    },
    "papermill": {
     "duration": 0.033997,
     "end_time": "2024-10-17T06:26:16.022194",
     "exception": false,
     "start_time": "2024-10-17T06:26:15.988197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    ts_train = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        temp=np.array(list(data.iloc[i][6:].dropna()))\n",
    "        temp = temp[:-18]\n",
    "        temp=temp.reshape(1,len(temp),1)\n",
    "        temp2 = TimeSeriesScalerMeanVariance().fit_transform(temp)\n",
    "        ts_train.append(temp2.reshape(-1,1))\n",
    "\n",
    "    return ts_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "388f243a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:16.070107Z",
     "iopub.status.busy": "2024-10-17T06:26:16.069810Z",
     "iopub.status.idle": "2024-10-17T06:26:17.858996Z",
     "shell.execute_reply": "2024-10-17T06:26:17.858132Z"
    },
    "papermill": {
     "duration": 1.81575,
     "end_time": "2024-10-17T06:26:17.861345",
     "exception": false,
     "start_time": "2024-10-17T06:26:16.045595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_MICRO = preprocessing(class_dataframes['MICRO'])\n",
    "dataset_INDUSTRY = preprocessing(class_dataframes['INDUSTRY'])\n",
    "dataset_MACRO = preprocessing(class_dataframes['MACRO'])\n",
    "dataset_FINANCE = preprocessing(class_dataframes['FINANCE'])\n",
    "dataset_DEMOGRAPHIC = preprocessing(class_dataframes['DEMOGRAPHIC'])\n",
    "dataset_OTHER = preprocessing(class_dataframes['OTHER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a52a54d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:17.909990Z",
     "iopub.status.busy": "2024-10-17T06:26:17.909685Z",
     "iopub.status.idle": "2024-10-17T06:26:17.915731Z",
     "shell.execute_reply": "2024-10-17T06:26:17.914862Z"
    },
    "papermill": {
     "duration": 0.032354,
     "end_time": "2024-10-17T06:26:17.917590",
     "exception": false,
     "start_time": "2024-10-17T06:26:17.885236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_seq_length_MICRO = max(len(seq) for seq in dataset_MICRO)\n",
    "max_seq_length_INDUSTRY = max(len(seq) for seq in dataset_INDUSTRY)\n",
    "max_seq_length_MACRO = max(len(seq) for seq in dataset_MACRO)\n",
    "max_seq_length_FINANCE = max(len(seq) for seq in dataset_FINANCE)\n",
    "max_seq_length_DEMOGRAPHIC = max(len(seq) for seq in dataset_DEMOGRAPHIC)\n",
    "max_seq_length_OTHER = max(len(seq) for seq in dataset_OTHER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfa0c737",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:17.966453Z",
     "iopub.status.busy": "2024-10-17T06:26:17.965725Z",
     "iopub.status.idle": "2024-10-17T06:26:17.979802Z",
     "shell.execute_reply": "2024-10-17T06:26:17.978881Z"
    },
    "papermill": {
     "duration": 0.04074,
     "end_time": "2024-10-17T06:26:17.981802",
     "exception": false,
     "start_time": "2024-10-17T06:26:17.941062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "padded_sequences_MICRO = tf.keras.preprocessing.sequence.pad_sequences(dataset_MICRO, maxlen=max_seq_length_MICRO, padding='post', dtype='float32')\n",
    "padded_sequences_INDUSTRY = tf.keras.preprocessing.sequence.pad_sequences(dataset_INDUSTRY, maxlen=max_seq_length_INDUSTRY, padding='post', dtype='float32')\n",
    "padded_sequences_MACRO = tf.keras.preprocessing.sequence.pad_sequences(dataset_MACRO, maxlen=max_seq_length_MACRO, padding='post', dtype='float32')\n",
    "padded_sequences_FINANCE = tf.keras.preprocessing.sequence.pad_sequences(dataset_FINANCE, maxlen=max_seq_length_FINANCE, padding='post', dtype='float32')\n",
    "padded_sequences_DEMOGRAPHIC = tf.keras.preprocessing.sequence.pad_sequences(dataset_DEMOGRAPHIC, maxlen=max_seq_length_DEMOGRAPHIC, padding='post', dtype='float32')\n",
    "padded_sequences_OTHER = tf.keras.preprocessing.sequence.pad_sequences(dataset_OTHER, maxlen=max_seq_length_OTHER, padding='post', dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec449f92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:18.030961Z",
     "iopub.status.busy": "2024-10-17T06:26:18.030693Z",
     "iopub.status.idle": "2024-10-17T06:26:18.037287Z",
     "shell.execute_reply": "2024-10-17T06:26:18.036474Z"
    },
    "papermill": {
     "duration": 0.033572,
     "end_time": "2024-10-17T06:26:18.039312",
     "exception": false,
     "start_time": "2024-10-17T06:26:18.005740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reshaped_array_MICRO = padded_sequences_MICRO.reshape(padded_sequences_MICRO.shape[0], 1, padded_sequences_MICRO.shape[1])\n",
    "reshaped_array_INDUSTRY = padded_sequences_INDUSTRY.reshape(padded_sequences_INDUSTRY.shape[0], 1, padded_sequences_INDUSTRY.shape[1])\n",
    "reshaped_array_MACRO = padded_sequences_MACRO.reshape(padded_sequences_MACRO.shape[0], 1, padded_sequences_MACRO.shape[1])\n",
    "reshaped_array_FINANCE = padded_sequences_FINANCE.reshape(padded_sequences_FINANCE.shape[0], 1, padded_sequences_FINANCE.shape[1])\n",
    "reshaped_array_DEMOGRAPHIC = padded_sequences_DEMOGRAPHIC.reshape(padded_sequences_DEMOGRAPHIC.shape[0], 1, padded_sequences_DEMOGRAPHIC.shape[1])\n",
    "reshaped_array_OTHER = padded_sequences_OTHER.reshape(padded_sequences_OTHER.shape[0], 1, padded_sequences_OTHER.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "925394b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:18.087650Z",
     "iopub.status.busy": "2024-10-17T06:26:18.087375Z",
     "iopub.status.idle": "2024-10-17T06:26:18.092566Z",
     "shell.execute_reply": "2024-10-17T06:26:18.091773Z"
    },
    "papermill": {
     "duration": 0.03209,
     "end_time": "2024-10-17T06:26:18.094760",
     "exception": false,
     "start_time": "2024-10-17T06:26:18.062670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(474, 1, 108)\n",
      "(334, 1, 126)\n",
      "(312, 1, 126)\n",
      "(145, 1, 126)\n",
      "(111, 1, 120)\n",
      "(52, 1, 102)\n"
     ]
    }
   ],
   "source": [
    "print(reshaped_array_MICRO.shape)\n",
    "print(reshaped_array_INDUSTRY.shape)\n",
    "print(reshaped_array_MACRO.shape)\n",
    "print(reshaped_array_FINANCE.shape)\n",
    "print(reshaped_array_DEMOGRAPHIC.shape)\n",
    "print(reshaped_array_OTHER.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40e39362",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:18.142660Z",
     "iopub.status.busy": "2024-10-17T06:26:18.142367Z",
     "iopub.status.idle": "2024-10-17T06:26:18.147818Z",
     "shell.execute_reply": "2024-10-17T06:26:18.147112Z"
    },
    "papermill": {
     "duration": 0.03164,
     "end_time": "2024-10-17T06:26:18.149784",
     "exception": false,
     "start_time": "2024-10-17T06:26:18.118144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocessing_tourism_hospital(data):\n",
    "    ts_train = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        temp=np.array(list(data.iloc[i][:].dropna()))\n",
    "#         temp = temp[:-18]\n",
    "        temp=temp.reshape(1,len(temp),1)\n",
    "        temp2 = TimeSeriesScalerMeanVariance().fit_transform(temp)\n",
    "        ts_train.append(temp2.reshape(-1,1))\n",
    "\n",
    "    return ts_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9e3a9b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:18.197479Z",
     "iopub.status.busy": "2024-10-17T06:26:18.197202Z",
     "iopub.status.idle": "2024-10-17T06:26:19.713172Z",
     "shell.execute_reply": "2024-10-17T06:26:19.712082Z"
    },
    "papermill": {
     "duration": 1.542177,
     "end_time": "2024-10-17T06:26:19.715280",
     "exception": false,
     "start_time": "2024-10-17T06:26:18.173103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(427, 1, 122)\n"
     ]
    }
   ],
   "source": [
    "dataset_tourism = pd.read_excel(\"/kaggle/input/newtsdatasets/Tourism-new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "\n",
    "dataset_tourism=dataset_tourism.iloc[:,:-8]\n",
    "dataset_tourism = preprocessing_tourism_hospital(dataset_tourism)\n",
    "max_seq_length_tourism= max(len(seq) for seq in dataset_tourism)\n",
    "padded_sequences_tourism = tf.keras.preprocessing.sequence.pad_sequences(dataset_tourism, maxlen=max_seq_length_tourism, padding='post', dtype='float32')\n",
    "reshaped_array_tourism = padded_sequences_tourism.reshape(padded_sequences_tourism.shape[0], 1, padded_sequences_tourism.shape[1])\n",
    "print(reshaped_array_tourism.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ee3e240",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:19.766468Z",
     "iopub.status.busy": "2024-10-17T06:26:19.765045Z",
     "iopub.status.idle": "2024-10-17T06:26:21.658508Z",
     "shell.execute_reply": "2024-10-17T06:26:21.657450Z"
    },
    "papermill": {
     "duration": 1.921379,
     "end_time": "2024-10-17T06:26:21.660813",
     "exception": false,
     "start_time": "2024-10-17T06:26:19.739434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(767, 1, 72)\n"
     ]
    }
   ],
   "source": [
    "dataset_hospital = pd.read_excel(\"/kaggle/input/newtsdatasets/Hospital_new.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "\n",
    "dataset_hospital=dataset_hospital.iloc[:,:-12]\n",
    "dataset_hospital = preprocessing_tourism_hospital(dataset_hospital)\n",
    "max_seq_length_hospital= max(len(seq) for seq in dataset_hospital)\n",
    "padded_sequences_hospital = tf.keras.preprocessing.sequence.pad_sequences(dataset_hospital, maxlen=max_seq_length_hospital, padding='post', dtype='float32')\n",
    "reshaped_array_hospital= padded_sequences_hospital.reshape(padded_sequences_hospital.shape[0], 1, padded_sequences_hospital.shape[1])\n",
    "print(reshaped_array_hospital.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dcb44a7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:21.710539Z",
     "iopub.status.busy": "2024-10-17T06:26:21.710227Z",
     "iopub.status.idle": "2024-10-17T06:26:21.906560Z",
     "shell.execute_reply": "2024-10-17T06:26:21.905480Z"
    },
    "papermill": {
     "duration": 0.223753,
     "end_time": "2024-10-17T06:26:21.908607",
     "exception": false,
     "start_time": "2024-10-17T06:26:21.684854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 1, 108)\n"
     ]
    }
   ],
   "source": [
    "dataset_cif = pd.read_excel(\"/kaggle/input/cifnewdataset/12.xlsx\",header=None) # Kaggle - Wikipedia - Horizon 59\n",
    "\n",
    "dataset_cif=dataset_cif.iloc[:,:-12]\n",
    "dataset_cif = preprocessing_tourism_hospital(dataset_cif)\n",
    "max_seq_length_cif= max(len(seq) for seq in dataset_cif)\n",
    "padded_sequences_cif = tf.keras.preprocessing.sequence.pad_sequences(dataset_cif, maxlen=max_seq_length_cif, padding='post', dtype='float32')\n",
    "reshaped_array_cif= padded_sequences_cif.reshape(padded_sequences_cif.shape[0], 1, padded_sequences_cif.shape[1])\n",
    "print(reshaped_array_cif.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a53cb5f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:21.958320Z",
     "iopub.status.busy": "2024-10-17T06:26:21.957697Z",
     "iopub.status.idle": "2024-10-17T06:26:21.961894Z",
     "shell.execute_reply": "2024-10-17T06:26:21.961022Z"
    },
    "papermill": {
     "duration": 0.030938,
     "end_time": "2024-10-17T06:26:21.963778",
     "exception": false,
     "start_time": "2024-10-17T06:26:21.932840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name='m3-industry'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "313599c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:22.012067Z",
     "iopub.status.busy": "2024-10-17T06:26:22.011759Z",
     "iopub.status.idle": "2024-10-17T06:26:22.019135Z",
     "shell.execute_reply": "2024-10-17T06:26:22.018214Z"
    },
    "papermill": {
     "duration": 0.033956,
     "end_time": "2024-10-17T06:26:22.021103",
     "exception": false,
     "start_time": "2024-10-17T06:26:21.987147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset_name=='cif-12':\n",
    "    padded_seq=padded_sequences_cif\n",
    "    reshaped_array=reshaped_array_cif\n",
    "    max_dim=max_seq_length_cif\n",
    "elif dataset_name=='hospital':\n",
    "    padded_seq=padded_sequences_hospital\n",
    "    reshaped_array=reshaped_array_hospital\n",
    "    max_dim=max_seq_length_hospital\n",
    "elif dataset_name=='tourism':\n",
    "    padded_seq=padded_sequences_tourism\n",
    "    reshaped_array=reshaped_array_tourism\n",
    "    max_dim=max_seq_length_tourism\n",
    "elif dataset_name=='m3-demo':\n",
    "    padded_seq=padded_sequences_DEMOGRAPHIC\n",
    "    reshaped_array=reshaped_array_DEMOGRAPHIC\n",
    "    max_dim=max_seq_length_DEMOGRAPHIC\n",
    "elif dataset_name=='m3-finance':\n",
    "    padded_seq=padded_sequences_FINANCE\n",
    "    reshaped_array=reshaped_array_FINANCE\n",
    "    max_dim=max_seq_length_FINANCE\n",
    "elif dataset_name=='m3-industry':\n",
    "    padded_seq=padded_sequences_INDUSTRY\n",
    "    reshaped_array=reshaped_array_INDUSTRY\n",
    "    max_dim=max_seq_length_INDUSTRY\n",
    "\n",
    "elif dataset_name=='m3-macro':\n",
    "    padded_seq=padded_sequences_MACRO\n",
    "    reshaped_array=reshaped_array_MACRO\n",
    "    max_dim=max_seq_length_MACRO\n",
    "elif dataset_name=='m3-micro':\n",
    "    padded_seq=padded_sequences_MICRO\n",
    "    reshaped_array=reshaped_array_MICRO\n",
    "    max_dim=max_seq_length_MICRO\n",
    "else:\n",
    "    padded_seq=padded_sequences_OTHER\n",
    "    reshaped_array=reshaped_array_OTHER\n",
    "    max_dim=max_seq_length_OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5292c8e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:22.069619Z",
     "iopub.status.busy": "2024-10-17T06:26:22.069305Z",
     "iopub.status.idle": "2024-10-17T06:26:22.078385Z",
     "shell.execute_reply": "2024-10-17T06:26:22.077695Z"
    },
    "papermill": {
     "duration": 0.035558,
     "end_time": "2024-10-17T06:26:22.080389",
     "exception": false,
     "start_time": "2024-10-17T06:26:22.044831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtc = DTC(n_clusters=2,input_dim=reshaped_array.shape[-1], timesteps=padded_seq.shape[1], max_dim=max_dim,dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "afbdf850",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:22.128466Z",
     "iopub.status.busy": "2024-10-17T06:26:22.128184Z",
     "iopub.status.idle": "2024-10-17T06:26:22.131828Z",
     "shell.execute_reply": "2024-10-17T06:26:22.131092Z"
    },
    "papermill": {
     "duration": 0.029775,
     "end_time": "2024-10-17T06:26:22.133720",
     "exception": false,
     "start_time": "2024-10-17T06:26:22.103945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "# dtc = DTC(n_clusters=2,input_dim=reshaped_array_DEMOGRAPHIC.shape[-1], timesteps=reshaped_array_DEMOGRAPHIC.shape[1], max_dim=max_seq_length_DEMOGRAPHIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f5b5e9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:22.188733Z",
     "iopub.status.busy": "2024-10-17T06:26:22.185901Z",
     "iopub.status.idle": "2024-10-17T06:26:26.042444Z",
     "shell.execute_reply": "2024-10-17T06:26:26.041319Z"
    },
    "papermill": {
     "duration": 3.894522,
     "end_time": "2024-10-17T06:26:26.051763",
     "exception": false,
     "start_time": "2024-10-17T06:26:22.157241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, None, 126)]  0           []                               \n",
      "                                                                                                  \n",
      " encoded_lstm1 (LSTM)           (None, None, 256)    392192      ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " encoded_lstm2 (LSTM)           (None, None, 128)    197120      ['encoded_lstm1[0][0]']          \n",
      "                                                                                                  \n",
      " encoded_atten (Attention)      (None, None, 128)    0           ['encoded_lstm2[0][0]',          \n",
      "                                                                  'encoded_lstm2[0][0]',          \n",
      "                                                                  'encoded_lstm2[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, None, 256)    0           ['encoded_lstm2[0][0]',          \n",
      "                                                                  'encoded_atten[0][0]']          \n",
      "                                                                                                  \n",
      " encoded_lstm3 (LSTM)           (None, 64)           82176       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " encoded_dense (Dense)          (None, 16)           1040        ['encoded_lstm3[0][0]']          \n",
      "                                                                                                  \n",
      " repeat_vector (RepeatVector)   (None, 1, 16)        0           ['encoded_dense[0][0]']          \n",
      "                                                                                                  \n",
      " decoded_lstm1 (LSTM)           (None, 1, 64)        20736       ['repeat_vector[0][0]']          \n",
      "                                                                                                  \n",
      " decoded_lstm2 (LSTM)           (None, 1, 128)       98816       ['decoded_lstm1[0][0]']          \n",
      "                                                                                                  \n",
      " decoded_lstm3 (LSTM)           (None, 1, 256)       394240      ['decoded_lstm2[0][0]']          \n",
      "                                                                                                  \n",
      " decoded_atten (Attention)      (None, 1, 256)       0           ['decoded_lstm3[0][0]',          \n",
      "                                                                  'decoded_lstm3[0][0]',          \n",
      "                                                                  'decoded_lstm3[0][0]']          \n",
      "                                                                                                  \n",
      " decoded_concat (Concatenate)   (None, 1, 512)       0           ['decoded_lstm3[0][0]',          \n",
      "                                                                  'decoded_atten[0][0]']          \n",
      "                                                                                                  \n",
      " decoded_timeD (TimeDistributed  (None, 1, 126)      64638       ['decoded_concat[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " TSClustering (TSClusteringLaye  (None, 2)           32          ['encoded_dense[0][0]']          \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,250,990\n",
      "Trainable params: 1,250,990\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = 'adam'\n",
    "dtc.initialize()\n",
    "dtc.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1992b40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:26.108197Z",
     "iopub.status.busy": "2024-10-17T06:26:26.107801Z",
     "iopub.status.idle": "2024-10-17T06:26:26.126301Z",
     "shell.execute_reply": "2024-10-17T06:26:26.125409Z"
    },
    "papermill": {
     "duration": 0.04864,
     "end_time": "2024-10-17T06:26:26.128231",
     "exception": false,
     "start_time": "2024-10-17T06:26:26.079591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtc.compile(gamma=1.0, optimizer=optimizer, initial_heatmap_loss_weight=0.1,\n",
    "                final_heatmap_loss_weight=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5919fd05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:26:26.185169Z",
     "iopub.status.busy": "2024-10-17T06:26:26.184456Z",
     "iopub.status.idle": "2024-10-17T06:30:26.986085Z",
     "shell.execute_reply": "2024-10-17T06:30:26.984925Z"
    },
    "papermill": {
     "duration": 240.832111,
     "end_time": "2024-10-17T06:30:26.988262",
     "exception": false,
     "start_time": "2024-10-17T06:26:26.156151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining...\n",
      "Epoch 1/50\n",
      "34/34 [==============================] - 5s 141ms/step - loss: 0.9373\n",
      "Epoch 2/50\n",
      "34/34 [==============================] - 5s 143ms/step - loss: 0.7145\n",
      "Epoch 3/50\n",
      "34/34 [==============================] - 5s 142ms/step - loss: 0.6579\n",
      "Epoch 4/50\n",
      "34/34 [==============================] - 5s 141ms/step - loss: 0.6470\n",
      "Epoch 5/50\n",
      "34/34 [==============================] - 5s 141ms/step - loss: 0.6416\n",
      "Epoch 6/50\n",
      "34/34 [==============================] - 5s 142ms/step - loss: 0.6371\n",
      "Epoch 7/50\n",
      "34/34 [==============================] - 5s 144ms/step - loss: 0.6358\n",
      "Epoch 8/50\n",
      "34/34 [==============================] - 5s 143ms/step - loss: 0.6339\n",
      "Epoch 9/50\n",
      "34/34 [==============================] - 5s 140ms/step - loss: 0.6313\n",
      "Epoch 10/50\n",
      "34/34 [==============================] - 5s 138ms/step - loss: 0.6291\n",
      "Epoch 11/50\n",
      "34/34 [==============================] - 5s 140ms/step - loss: 0.6241\n",
      "Epoch 12/50\n",
      "34/34 [==============================] - 5s 140ms/step - loss: 0.6172\n",
      "Epoch 13/50\n",
      "34/34 [==============================] - 5s 140ms/step - loss: 0.5925\n",
      "Epoch 14/50\n",
      "34/34 [==============================] - 5s 140ms/step - loss: 0.5543\n",
      "Epoch 15/50\n",
      "34/34 [==============================] - 5s 142ms/step - loss: 0.5332\n",
      "Epoch 16/50\n",
      "34/34 [==============================] - 5s 139ms/step - loss: 0.5236\n",
      "Epoch 17/50\n",
      "34/34 [==============================] - 5s 141ms/step - loss: 0.5188\n",
      "Epoch 18/50\n",
      "34/34 [==============================] - 5s 144ms/step - loss: 0.5154\n",
      "Epoch 19/50\n",
      "34/34 [==============================] - 5s 139ms/step - loss: 0.5150\n",
      "Epoch 20/50\n",
      "34/34 [==============================] - 5s 143ms/step - loss: 0.5121\n",
      "Epoch 21/50\n",
      "34/34 [==============================] - 5s 145ms/step - loss: 0.5102\n",
      "Epoch 22/50\n",
      "34/34 [==============================] - 5s 142ms/step - loss: 0.5091\n",
      "Epoch 23/50\n",
      "34/34 [==============================] - 5s 139ms/step - loss: 0.5102\n",
      "Epoch 24/50\n",
      "34/34 [==============================] - 5s 136ms/step - loss: 0.5082\n",
      "Epoch 25/50\n",
      "34/34 [==============================] - 5s 141ms/step - loss: 0.5079\n",
      "Epoch 26/50\n",
      "34/34 [==============================] - 5s 139ms/step - loss: 0.5057\n",
      "Epoch 27/50\n",
      "34/34 [==============================] - 5s 144ms/step - loss: 0.5039\n",
      "Epoch 28/50\n",
      "34/34 [==============================] - 5s 142ms/step - loss: 0.5022\n",
      "Epoch 29/50\n",
      "34/34 [==============================] - 5s 143ms/step - loss: 0.5011\n",
      "Epoch 30/50\n",
      "34/34 [==============================] - 5s 141ms/step - loss: 0.5001\n",
      "Epoch 31/50\n",
      "34/34 [==============================] - 5s 140ms/step - loss: 0.4990\n",
      "Epoch 32/50\n",
      "34/34 [==============================] - 5s 138ms/step - loss: 0.4969\n",
      "Epoch 33/50\n",
      "34/34 [==============================] - 5s 140ms/step - loss: 0.4963\n",
      "Epoch 34/50\n",
      "34/34 [==============================] - 5s 142ms/step - loss: 0.4933\n",
      "Epoch 35/50\n",
      "34/34 [==============================] - 5s 142ms/step - loss: 0.4918\n",
      "Epoch 36/50\n",
      "34/34 [==============================] - 5s 140ms/step - loss: 0.4907\n",
      "Epoch 37/50\n",
      "34/34 [==============================] - 5s 139ms/step - loss: 0.4888\n",
      "Epoch 38/50\n",
      "34/34 [==============================] - 5s 141ms/step - loss: 0.4879\n",
      "Epoch 39/50\n",
      "34/34 [==============================] - 5s 142ms/step - loss: 0.4894\n",
      "Epoch 40/50\n",
      "34/34 [==============================] - 5s 140ms/step - loss: 0.4867\n",
      "Epoch 41/50\n",
      "34/34 [==============================] - 5s 142ms/step - loss: 0.4862\n",
      "Epoch 42/50\n",
      "34/34 [==============================] - 5s 142ms/step - loss: 0.4837\n",
      "Epoch 43/50\n",
      "34/34 [==============================] - 5s 146ms/step - loss: 0.4793\n",
      "Epoch 44/50\n",
      "34/34 [==============================] - 5s 141ms/step - loss: 0.4766\n",
      "Epoch 45/50\n",
      "34/34 [==============================] - 5s 142ms/step - loss: 0.4741\n",
      "Epoch 46/50\n",
      "34/34 [==============================] - 5s 140ms/step - loss: 0.4701\n",
      "Epoch 47/50\n",
      "34/34 [==============================] - 5s 142ms/step - loss: 0.4661\n",
      "Epoch 48/50\n",
      "34/34 [==============================] - 5s 141ms/step - loss: 0.4618\n",
      "Epoch 49/50\n",
      "34/34 [==============================] - 5s 137ms/step - loss: 0.4589\n",
      "Epoch 50/50\n",
      "34/34 [==============================] - 5s 139ms/step - loss: 0.4579\n",
      "Pretrained weights are saved to /kaggle/working//ae_weights-epoch50.h5\n"
     ]
    }
   ],
   "source": [
    "dtc.pretrain(X=reshaped_array, optimizer=optimizer, epochs=50, batch_size=10,save_dir=\"/kaggle/working/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6248ec91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:30:27.315742Z",
     "iopub.status.busy": "2024-10-17T06:30:27.314864Z",
     "iopub.status.idle": "2024-10-17T06:30:27.319271Z",
     "shell.execute_reply": "2024-10-17T06:30:27.318330Z"
    },
    "papermill": {
     "duration": 0.16971,
     "end_time": "2024-10-17T06:30:27.321219",
     "exception": false,
     "start_time": "2024-10-17T06:30:27.151509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca592535",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:30:27.650282Z",
     "iopub.status.busy": "2024-10-17T06:30:27.649642Z",
     "iopub.status.idle": "2024-10-17T14:52:31.169280Z",
     "shell.execute_reply": "2024-10-17T14:52:31.168267Z"
    },
    "papermill": {
     "duration": 30123.861697,
     "end_time": "2024-10-17T14:52:31.346715",
     "exception": false,
     "start_time": "2024-10-17T06:30:27.485018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10 epochs.\n",
      "Evaluating every 10 and saving model every 10 epochs.\n",
      "m3-industry 96 144\n",
      "11/11 [==============================] - 1s 44ms/step\n",
      "list:  [array([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "       0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
      "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
      "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,\n",
      "       1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
      "       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
      "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
      "       0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0])]\n",
      "Best Clusters:  [0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1\n",
      " 1 0 0 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 1\n",
      " 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0 0 1 0 0\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 0\n",
      " 0]\n",
      "Validation SMAPE:  11.167501371523743\n",
      "Test SMAPE:  13.04008242821686\n",
      "Validation RMSE:  658.6818007576753\n",
      "Test RMSE:  844.2561398844216\n",
      "Total Loss tf.Tensor(66.45755, shape=(), dtype=float32)\n",
      "reconstruction_loss tf.Tensor(150.29523, shape=(), dtype=float32)\n",
      "clustering loss tf.Tensor(0.4711286, shape=(), dtype=float32)\n",
      "Validation Loss tf.Tensor(45.380344, shape=(), dtype=float32)\n",
      "Modified Clustering:  [0 0 1 1 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 1 1 1 1 0\n",
      " 1 1 0 1 1 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1\n",
      " 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 1\n",
      " 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 1\n",
      " 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0\n",
      " 1 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0\n",
      " 0]\n",
      "Modified Clustering:  [0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0\n",
      " 1 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 0 0 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0\n",
      " 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 1\n",
      " 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1\n",
      " 1 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1\n",
      " 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 1\n",
      " 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0\n",
      " 0]\n",
      "Modified Clustering:  [0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0\n",
      " 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1\n",
      " 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 1\n",
      " 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0\n",
      " 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1\n",
      " 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0\n",
      " 1 1 1 1 1 0 1 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1\n",
      " 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1\n",
      " 1 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1\n",
      " 0]\n",
      "Modified Clustering:  [0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0\n",
      " 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1\n",
      " 0 0 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 0 0\n",
      " 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0\n",
      " 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 0 0 1 1 1 0\n",
      " 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1\n",
      " 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1\n",
      " 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1\n",
      " 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 0\n",
      " 0]\n",
      "Modified Clustering:  [1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1\n",
      " 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1\n",
      " 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0\n",
      " 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1\n",
      " 0 1 0 0 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0\n",
      " 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1\n",
      " 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 1 0 1 1 1\n",
      " 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 0\n",
      " 0]\n",
      "Epoch: 1/10\n",
      "Validation SMAPE:  11.624943559699995\n",
      "Test SMAPE:  13.50562507553203\n",
      "Validation RMSE:  676.1694343949044\n",
      "Test RMSE:  13.50562507553203\n",
      "Total Loss:  tf.Tensor(66.45755, shape=(), dtype=float32)\n",
      "m3-industry 96 144\n",
      "11/11 [==============================] - 1s 44ms/step\n",
      "list:  [array([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "       0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
      "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
      "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,\n",
      "       1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
      "       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
      "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
      "       0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0]), array([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "       0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
      "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
      "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,\n",
      "       1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
      "       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
      "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
      "       0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0])]\n",
      "Best Clusters:  [0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1\n",
      " 1 0 0 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 1\n",
      " 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0 0 1 0 0\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 0\n",
      " 0]\n",
      "Validation SMAPE:  11.02797205209295\n",
      "Test SMAPE:  12.74880650404517\n",
      "Validation RMSE:  646.9146914135548\n",
      "Test RMSE:  814.9222161163563\n",
      "Total Loss tf.Tensor(66.29281, shape=(), dtype=float32)\n",
      "reconstruction_loss tf.Tensor(150.65387, shape=(), dtype=float32)\n",
      "clustering loss tf.Tensor(0.50811255, shape=(), dtype=float32)\n",
      "Validation Loss tf.Tensor(45.075512, shape=(), dtype=float32)\n",
      "Modified Clustering:  [1 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1\n",
      " 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0\n",
      " 1 0 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0\n",
      " 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0\n",
      " 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1\n",
      " 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 1\n",
      " 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 0\n",
      " 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0\n",
      " 0]\n",
      "Modified Clustering:  [0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1\n",
      " 1 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0\n",
      " 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0\n",
      " 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0\n",
      " 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0\n",
      " 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0\n",
      " 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 1\n",
      " 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 1 0\n",
      " 1 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0\n",
      " 0]\n",
      "Modified Clustering:  [1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1\n",
      " 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0\n",
      " 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1\n",
      " 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 0\n",
      " 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0\n",
      " 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1\n",
      " 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0\n",
      " 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 1\n",
      " 0]\n",
      "Modified Clustering:  [1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1\n",
      " 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0\n",
      " 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 1 1\n",
      " 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1\n",
      " 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0\n",
      " 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1\n",
      " 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0\n",
      " 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 1 1\n",
      " 0]\n",
      "Modified Clustering:  [0 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0\n",
      " 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0\n",
      " 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1\n",
      " 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 1 1\n",
      " 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0\n",
      " 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0\n",
      " 1 1 1 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0\n",
      " 0]\n",
      "Epoch: 2/10\n",
      "Validation SMAPE:  11.950718720535468\n",
      "Test SMAPE:  13.401945028744699\n",
      "Validation RMSE:  687.5867378127381\n",
      "Test RMSE:  13.401945028744699\n",
      "Total Loss:  tf.Tensor(66.29281, shape=(), dtype=float32)\n",
      "m3-industry 96 144\n",
      "11/11 [==============================] - 0s 42ms/step\n",
      "Modified Clustering:  [0 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1\n",
      " 1 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1\n",
      " 0 0 0 0 0 1 1 1 1 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1\n",
      " 1 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0\n",
      " 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1\n",
      " 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1\n",
      " 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1\n",
      " 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0\n",
      " 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0\n",
      " 0]\n",
      "Modified Clustering:  [0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1\n",
      " 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 1 0 0 1\n",
      " 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1\n",
      " 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0\n",
      " 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1\n",
      " 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 1\n",
      " 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 0 1\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0\n",
      " 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 0 0 1 0\n",
      " 0]\n",
      "Modified Clustering:  [0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1\n",
      " 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1\n",
      " 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1\n",
      " 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 0 0\n",
      " 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 1\n",
      " 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0\n",
      " 1 1 1 0 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0\n",
      " 0]\n",
      "Modified Clustering:  [0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1\n",
      " 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0\n",
      " 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1\n",
      " 1 1 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1\n",
      " 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1\n",
      " 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 1 0 0 0 0\n",
      " 0 1 0 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1\n",
      " 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
      " 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 1 0\n",
      " 0]\n",
      "Modified Clustering:  [0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 0 1 1 1\n",
      " 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0\n",
      " 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0\n",
      " 1 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 1\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
      " 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 0\n",
      " 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 1\n",
      " 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 1 0 0 0 0 0\n",
      " 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0 1 0\n",
      " 0]\n",
      "Epoch: 3/10\n",
      "Validation SMAPE:  11.748436759016435\n",
      "Test SMAPE:  13.028800356458827\n",
      "Validation RMSE:  683.6952242808338\n",
      "Test RMSE:  13.028800356458827\n",
      "Total Loss:  tf.Tensor(66.29281, shape=(), dtype=float32)\n",
      "m3-industry 96 144\n",
      "11/11 [==============================] - 0s 43ms/step\n",
      "Modified Clustering:  [0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1\n",
      " 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0\n",
      " 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1\n",
      " 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1\n",
      " 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 0 1\n",
      " 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 0 1 0 1 0 1 1 0\n",
      " 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 0\n",
      " 0]\n",
      "Modified Clustering:  [0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1\n",
      " 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1\n",
      " 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0\n",
      " 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1\n",
      " 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1\n",
      " 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1\n",
      " 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1\n",
      " 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1 0\n",
      " 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1\n",
      " 0]\n",
      "Modified Clustering:  [0 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1\n",
      " 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1\n",
      " 0 0 0 1 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0\n",
      " 1 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1\n",
      " 1 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1\n",
      " 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 1\n",
      " 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 0 0\n",
      " 0 1 1 1 1 0 0 0 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1\n",
      " 0]\n",
      "Modified Clustering:  [0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 1 1\n",
      " 1 0 0 1 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0\n",
      " 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0\n",
      " 1 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1\n",
      " 1 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1\n",
      " 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1\n",
      " 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0\n",
      " 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1\n",
      " 0]\n",
      "Modified Clustering:  [0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1\n",
      " 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 0 0 1 1\n",
      " 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0\n",
      " 1 0 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0\n",
      " 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1\n",
      " 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1\n",
      " 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0\n",
      " 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1\n",
      " 0]\n",
      "Epoch: 4/10\n",
      "Validation SMAPE:  11.701657683257219\n",
      "Test SMAPE:  13.083323928126543\n",
      "Validation RMSE:  678.4582827902265\n",
      "Test RMSE:  13.083323928126543\n",
      "Total Loss:  tf.Tensor(66.29281, shape=(), dtype=float32)\n",
      "m3-industry 96 144\n",
      "11/11 [==============================] - 0s 43ms/step\n",
      "Modified Clustering:  [0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1\n",
      " 1 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 1\n",
      " 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1\n",
      " 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1\n",
      " 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0\n",
      " 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 0\n",
      " 0]\n",
      "Modified Clustering:  [0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1\n",
      " 1 0 0 1 1 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 0\n",
      " 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1\n",
      " 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 1\n",
      " 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 0 0 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1\n",
      " 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1\n",
      " 1 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 0 1 1\n",
      " 0]\n",
      "Modified Clustering:  [0 0 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 1\n",
      " 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1\n",
      " 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 1\n",
      " 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 1 1\n",
      " 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0\n",
      " 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0\n",
      " 1 1 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 0 1 1\n",
      " 0]\n",
      "Modified Clustering:  [0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0\n",
      " 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1\n",
      " 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
      " 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1\n",
      " 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0\n",
      " 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1\n",
      " 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1\n",
      " 0]\n",
      "Modified Clustering:  [0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 0 1 1 0 0\n",
      " 1 0 1 0 1 1 0 1 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0\n",
      " 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0\n",
      " 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1\n",
      " 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0\n",
      " 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0\n",
      " 0 0 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0\n",
      " 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0\n",
      " 1]\n",
      "Epoch: 5/10\n",
      "Validation SMAPE:  11.720183605160473\n",
      "Test SMAPE:  13.370418997147558\n",
      "Validation RMSE:  685.4014621379506\n",
      "Test RMSE:  13.370418997147558\n",
      "Total Loss:  tf.Tensor(66.29281, shape=(), dtype=float32)\n",
      "m3-industry 96 144\n",
      "11/11 [==============================] - 1s 46ms/step\n",
      "Modified Clustering:  [0 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 1 1\n",
      " 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0\n",
      " 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 1 1 0 1\n",
      " 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1\n",
      " 1 1 1 1 0 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1\n",
      " 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0\n",
      " 0]\n",
      "Modified Clustering:  [0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1 1\n",
      " 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0\n",
      " 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1\n",
      " 1 0 0 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0\n",
      " 1]\n",
      "Modified Clustering:  [0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 1\n",
      " 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 1\n",
      " 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 1\n",
      " 1 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 1\n",
      " 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 1 0\n",
      " 1]\n",
      "Modified Clustering:  [0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1\n",
      " 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0\n",
      " 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0\n",
      " 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1\n",
      " 1 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1\n",
      " 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1\n",
      " 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0 1\n",
      " 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0\n",
      " 1]\n",
      "Modified Clustering:  [0 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1\n",
      " 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0\n",
      " 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1\n",
      " 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0\n",
      " 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0\n",
      " 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1\n",
      " 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0\n",
      " 1]\n",
      "Epoch: 6/10\n",
      "Validation SMAPE:  11.81249625116941\n",
      "Test SMAPE:  13.545344320537257\n",
      "Validation RMSE:  682.9546111553752\n",
      "Test RMSE:  13.545344320537257\n",
      "Total Loss:  tf.Tensor(66.29281, shape=(), dtype=float32)\n",
      "m3-industry 96 144\n",
      "11/11 [==============================] - 0s 43ms/step\n",
      "Modified Clustering:  [0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 1 1 0\n",
      " 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0\n",
      " 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0\n",
      " 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1\n",
      " 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1\n",
      " 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1\n",
      " 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0\n",
      " 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0\n",
      " 0]\n",
      "Modified Clustering:  [0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0\n",
      " 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0\n",
      " 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0\n",
      " 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0\n",
      " 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1\n",
      " 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1\n",
      " 0 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1\n",
      " 0 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0\n",
      " 1 0 1 1 1 1 1 1 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1\n",
      " 0]\n",
      "Modified Clustering:  [0 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 1 0\n",
      " 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1\n",
      " 1 1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1\n",
      " 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1 1 1 1 0 0 0\n",
      " 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0\n",
      " 1]\n",
      "Modified Clustering:  [0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 1 0\n",
      " 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0\n",
      " 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1\n",
      " 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1\n",
      " 1 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1\n",
      " 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1\n",
      " 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 0\n",
      " 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0\n",
      " 1]\n",
      "Modified Clustering:  [0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0\n",
      " 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0\n",
      " 0 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0\n",
      " 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1\n",
      " 0 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1\n",
      " 0 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 0\n",
      " 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0\n",
      " 1]\n",
      "Epoch: 7/10\n",
      "Validation SMAPE:  11.966416835496874\n",
      "Test SMAPE:  13.502584027895766\n",
      "Validation RMSE:  688.9863776724159\n",
      "Test RMSE:  13.502584027895766\n",
      "Total Loss:  tf.Tensor(66.29281, shape=(), dtype=float32)\n",
      "m3-industry 96 144\n",
      "11/11 [==============================] - 0s 42ms/step\n",
      "Modified Clustering:  [0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 1\n",
      " 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0\n",
      " 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1\n",
      " 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1\n",
      " 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0 0 1 0 0\n",
      " 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1\n",
      " 0]\n",
      "Modified Clustering:  [0 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0\n",
      " 0 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0\n",
      " 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 1 1 0\n",
      " 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1\n",
      " 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1\n",
      " 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1\n",
      " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 0 0 0\n",
      " 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1\n",
      " 0]\n",
      "Modified Clustering:  [1 0 1 0 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1\n",
      " 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1\n",
      " 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1\n",
      " 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1\n",
      " 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0\n",
      " 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0\n",
      " 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 1\n",
      " 0]\n",
      "Modified Clustering:  [1 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 0\n",
      " 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0\n",
      " 0 0 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1\n",
      " 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1\n",
      " 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 0 1 1\n",
      " 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0\n",
      " 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1\n",
      " 0]\n",
      "Modified Clustering:  [0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0\n",
      " 1 0 0 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 0 1\n",
      " 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 0\n",
      " 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0\n",
      " 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1\n",
      " 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 1\n",
      " 0]\n",
      "Epoch: 8/10\n",
      "Validation SMAPE:  11.62360922674362\n",
      "Test SMAPE:  13.209558404903857\n",
      "Validation RMSE:  672.9617488917057\n",
      "Test RMSE:  13.209558404903857\n",
      "Total Loss:  tf.Tensor(66.29281, shape=(), dtype=float32)\n",
      "m3-industry 96 144\n",
      "11/11 [==============================] - 1s 46ms/step\n",
      "Modified Clustering:  [1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1\n",
      " 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1\n",
      " 0 1 0 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0\n",
      " 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0\n",
      " 1 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0\n",
      " 1 0 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0\n",
      " 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0\n",
      " 0]\n",
      "Modified Clustering:  [1 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1\n",
      " 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1\n",
      " 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1\n",
      " 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0\n",
      " 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1\n",
      " 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 0\n",
      " 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 0 1 0 1 0\n",
      " 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0\n",
      " 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 0 0\n",
      " 0]\n",
      "Modified Clustering:  [0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0\n",
      " 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0\n",
      " 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1\n",
      " 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0\n",
      " 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1\n",
      " 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0\n",
      " 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1\n",
      " 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 0 0\n",
      " 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 1 1 0 0 0 0 1 0 1\n",
      " 0]\n",
      "Modified Clustering:  [0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 0\n",
      " 0 0 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0\n",
      " 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 0 0\n",
      " 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1\n",
      " 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0\n",
      " 1 1 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1\n",
      " 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1\n",
      " 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 1\n",
      " 0]\n",
      "Modified Clustering:  [0 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0\n",
      " 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1\n",
      " 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0\n",
      " 0 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1\n",
      " 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1\n",
      " 1 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0\n",
      " 0]\n",
      "Epoch: 9/10\n",
      "Validation SMAPE:  11.88814464333119\n",
      "Test SMAPE:  13.358047179349816\n",
      "Validation RMSE:  679.9226264814578\n",
      "Test RMSE:  13.358047179349816\n",
      "Total Loss:  tf.Tensor(66.29281, shape=(), dtype=float32)\n",
      "m3-industry 96 144\n",
      "11/11 [==============================] - 0s 43ms/step\n",
      "Modified Clustering:  [0 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
      " 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1\n",
      " 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1\n",
      " 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1\n",
      " 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1\n",
      " 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 0 1 1 0\n",
      " 0]\n",
      "Modified Clustering:  [0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1\n",
      " 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1\n",
      " 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1\n",
      " 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0\n",
      " 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1\n",
      " 0 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 1 0\n",
      " 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0\n",
      " 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0\n",
      " 0]\n",
      "Modified Clustering:  [0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1\n",
      " 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0\n",
      " 0 1 0 0 1 1 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1\n",
      " 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 0\n",
      " 1 1 0 0 1 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 1 1 0 1 1 1 1 1 1\n",
      " 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0\n",
      " 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 0 1 0\n",
      " 0]\n",
      "Modified Clustering:  [1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 1 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1\n",
      " 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0\n",
      " 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1\n",
      " 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 0\n",
      " 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0\n",
      " 0]\n",
      "Modified Clustering:  [0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 1\n",
      " 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0\n",
      " 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1\n",
      " 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0\n",
      " 1 0 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1\n",
      " 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 1 1 1 0 1 1\n",
      " 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0\n",
      " 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 1 0 0\n",
      " 0]\n",
      "Epoch: 10/10\n",
      "Validation SMAPE:  11.750430795770018\n",
      "Test SMAPE:  13.47209076135416\n",
      "Validation RMSE:  675.9304464730967\n",
      "Test RMSE:  13.47209076135416\n",
      "Total Loss:  tf.Tensor(66.29281, shape=(), dtype=float32)\n",
      "Metrics saved to metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "dtc.fit(reshaped_array, y_train=None, X_val=None, y_val=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9147265d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T14:52:31.697509Z",
     "iopub.status.busy": "2024-10-17T14:52:31.697148Z",
     "iopub.status.idle": "2024-10-17T14:52:31.701372Z",
     "shell.execute_reply": "2024-10-17T14:52:31.700515Z"
    },
    "papermill": {
     "duration": 0.182434,
     "end_time": "2024-10-17T14:52:31.703287",
     "exception": false,
     "start_time": "2024-10-17T14:52:31.520853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Evaluate\n",
    "# print('Performance (TRAIN)')\n",
    "# results = {}\n",
    "# q = dtc.model.predict(reshaped_array)[1]\n",
    "# y_pred = q.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b7613bdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T14:52:32.054441Z",
     "iopub.status.busy": "2024-10-17T14:52:32.053475Z",
     "iopub.status.idle": "2024-10-17T14:52:32.057962Z",
     "shell.execute_reply": "2024-10-17T14:52:32.057228Z"
    },
    "papermill": {
     "duration": 0.182317,
     "end_time": "2024-10-17T14:52:32.059926",
     "exception": false,
     "start_time": "2024-10-17T14:52:31.877609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# np.unique(y_pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba1917a",
   "metadata": {
    "papermill": {
     "duration": 0.196096,
     "end_time": "2024-10-17T14:52:32.436816",
     "exception": false,
     "start_time": "2024-10-17T14:52:32.240720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca74670",
   "metadata": {
    "papermill": {
     "duration": 0.172968,
     "end_time": "2024-10-17T14:52:32.784765",
     "exception": false,
     "start_time": "2024-10-17T14:52:32.611797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0897bb08",
   "metadata": {
    "papermill": {
     "duration": 0.185872,
     "end_time": "2024-10-17T14:52:33.144435",
     "exception": false,
     "start_time": "2024-10-17T14:52:32.958563",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a1694e",
   "metadata": {
    "papermill": {
     "duration": 0.176207,
     "end_time": "2024-10-17T14:52:33.493628",
     "exception": false,
     "start_time": "2024-10-17T14:52:33.317421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a6e9ea",
   "metadata": {
    "papermill": {
     "duration": 0.173848,
     "end_time": "2024-10-17T14:52:33.841682",
     "exception": false,
     "start_time": "2024-10-17T14:52:33.667834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d215159",
   "metadata": {
    "papermill": {
     "duration": 0.199596,
     "end_time": "2024-10-17T14:52:34.215652",
     "exception": false,
     "start_time": "2024-10-17T14:52:34.016056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30b0b29",
   "metadata": {
    "papermill": {
     "duration": 0.173507,
     "end_time": "2024-10-17T14:52:34.569918",
     "exception": false,
     "start_time": "2024-10-17T14:52:34.396411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeca880",
   "metadata": {
    "papermill": {
     "duration": 0.173221,
     "end_time": "2024-10-17T14:52:34.918863",
     "exception": false,
     "start_time": "2024-10-17T14:52:34.745642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b86aaf",
   "metadata": {
    "papermill": {
     "duration": 0.173322,
     "end_time": "2024-10-17T14:52:35.265859",
     "exception": false,
     "start_time": "2024-10-17T14:52:35.092537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3743351,
     "sourceId": 6479400,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3792584,
     "sourceId": 6564428,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4520850,
     "sourceId": 7735771,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4624107,
     "sourceId": 7889633,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4454423,
     "sourceId": 9326749,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 146550165,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 146550185,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 166856298,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30493.146787,
   "end_time": "2024-10-17T14:52:38.220408",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-17T06:24:25.073621",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
